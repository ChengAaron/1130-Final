{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project 1130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *AirBnb DataSet Problem*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals (3 min)\n",
    "\n",
    "To understand and work with the AirBnb datasets to solve our business problem \n",
    "\n",
    "## Introduction (5 min)\n",
    "\n",
    "**Business Context.** We are a company looking to expand our ventures based off tourism.\n",
    "\n",
    "**Business Problem.** The main Task is  **wrangle datasets related to AirBnb and create a critera to figure out the areas to build in**.\n",
    "\n",
    "**Analytical Context.** Text data is highly unstructured, and often requires pre-processing before we can gather any business insights from it. We will be leveraging tools from **natural language processing (NLP)** in order to help us process this data and generate new features that can be used for analytics or model building.\n",
    "\n",
    "The case will proceed as follows: \n",
    "1. We will extract the files based off what they are\n",
    "2. We will clean the data \n",
    "3. Create bar graph and determine average age in each major zipcode \n",
    "4. Same as above but with economic demos \n",
    "5. Average Airbnb review per zip code /posyiieve airbnb reviews if they're text\n",
    "6. Average size of Airbnb per zip code \n",
    "7. Average cost of Airbnb per zip code \n",
    "8. Airbnb date availability\n",
    "9. Using all these graphs and our critera we will decide where to build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk # imports the natural language toolkit\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import string\n",
    "import zipfile\n",
    "import plotly\n",
    "import tarfile\n",
    "import gzip\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE DATASETS\n",
    "demo = pd.read_csv('demographics.csv')\n",
    "econ = pd.read_csv('econ_state.csv')\n",
    "listings = pd.read_csv('listings.csv')\n",
    "\n",
    "realestate = pd.read_csv('real_estate.csv.gz', compression='gzip')\n",
    "\n",
    "venues = pd.read_csv('venues.csv.gz', compression='gzip')\n",
    "\n",
    "zf = zipfile.ZipFile('calendar.csv.zip') \n",
    "calendar = pd.read_csv(zf.open('calendar.csv'))\n",
    "ziplook = pd.read_csv('zipcodesForLookup.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>5_years_or_less</th>\n",
       "      <th>5-9_years</th>\n",
       "      <th>10-14_years</th>\n",
       "      <th>15-19_years</th>\n",
       "      <th>20-24_years</th>\n",
       "      <th>25-34_years</th>\n",
       "      <th>35-44_years</th>\n",
       "      <th>45-54_years</th>\n",
       "      <th>55-59_years</th>\n",
       "      <th>...</th>\n",
       "      <th>$10,000-$14,999</th>\n",
       "      <th>$15,000-$24,999</th>\n",
       "      <th>$25,000-$34,999</th>\n",
       "      <th>$35,000-$49,999</th>\n",
       "      <th>$50,000-$64,999</th>\n",
       "      <th>$65,000-$74,999</th>\n",
       "      <th>$75,000-$99,999</th>\n",
       "      <th>$100,000_or_more</th>\n",
       "      <th>median_household_income</th>\n",
       "      <th>mean_household_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601</td>\n",
       "      <td>17982</td>\n",
       "      <td>1006</td>\n",
       "      <td>1080</td>\n",
       "      <td>1342</td>\n",
       "      <td>1352</td>\n",
       "      <td>1321</td>\n",
       "      <td>2253</td>\n",
       "      <td>2149</td>\n",
       "      <td>2434</td>\n",
       "      <td>...</td>\n",
       "      <td>48.1</td>\n",
       "      <td>12</td>\n",
       "      <td>12.8</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>16.3</td>\n",
       "      <td>10816</td>\n",
       "      <td>20349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>40260</td>\n",
       "      <td>2006</td>\n",
       "      <td>2440</td>\n",
       "      <td>2421</td>\n",
       "      <td>2953</td>\n",
       "      <td>2865</td>\n",
       "      <td>5124</td>\n",
       "      <td>5139</td>\n",
       "      <td>5947</td>\n",
       "      <td>...</td>\n",
       "      <td>31.4</td>\n",
       "      <td>16.3</td>\n",
       "      <td>17.9</td>\n",
       "      <td>12.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>16079</td>\n",
       "      <td>23282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>603</td>\n",
       "      <td>52408</td>\n",
       "      <td>2664</td>\n",
       "      <td>3177</td>\n",
       "      <td>3351</td>\n",
       "      <td>3685</td>\n",
       "      <td>3585</td>\n",
       "      <td>6473</td>\n",
       "      <td>6775</td>\n",
       "      <td>6678</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>14.9</td>\n",
       "      <td>17.5</td>\n",
       "      <td>11.7</td>\n",
       "      <td>10.8</td>\n",
       "      <td>8.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>21.9</td>\n",
       "      <td>16804</td>\n",
       "      <td>26820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>606</td>\n",
       "      <td>6331</td>\n",
       "      <td>347</td>\n",
       "      <td>331</td>\n",
       "      <td>461</td>\n",
       "      <td>474</td>\n",
       "      <td>469</td>\n",
       "      <td>707</td>\n",
       "      <td>933</td>\n",
       "      <td>776</td>\n",
       "      <td>...</td>\n",
       "      <td>45.3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>20</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>12512</td>\n",
       "      <td>15730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>610</td>\n",
       "      <td>28328</td>\n",
       "      <td>1438</td>\n",
       "      <td>1490</td>\n",
       "      <td>2044</td>\n",
       "      <td>2122</td>\n",
       "      <td>1985</td>\n",
       "      <td>3358</td>\n",
       "      <td>3778</td>\n",
       "      <td>3858</td>\n",
       "      <td>...</td>\n",
       "      <td>26.9</td>\n",
       "      <td>14.8</td>\n",
       "      <td>23.7</td>\n",
       "      <td>15.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>18.4</td>\n",
       "      <td>17475</td>\n",
       "      <td>23360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   zipcode  5_years_or_less  5-9_years  10-14_years  15-19_years  20-24_years  \\\n",
       "0      601            17982       1006         1080         1342         1352   \n",
       "1      602            40260       2006         2440         2421         2953   \n",
       "2      603            52408       2664         3177         3351         3685   \n",
       "3      606             6331        347          331          461          474   \n",
       "4      610            28328       1438         1490         2044         2122   \n",
       "\n",
       "   25-34_years  35-44_years  45-54_years  55-59_years  ...  $10,000-$14,999  \\\n",
       "0         1321         2253         2149         2434  ...             48.1   \n",
       "1         2865         5124         5139         5947  ...             31.4   \n",
       "2         3585         6473         6775         6678  ...               31   \n",
       "3          469          707          933          776  ...             45.3   \n",
       "4         1985         3358         3778         3858  ...             26.9   \n",
       "\n",
       "   $15,000-$24,999  $25,000-$34,999  $35,000-$49,999  $50,000-$64,999  \\\n",
       "0               12             12.8              8.6              8.7   \n",
       "1             16.3             17.9             12.2             10.6   \n",
       "2             14.9             17.5             11.7             10.8   \n",
       "3             10.2               20             11.7               11   \n",
       "4             14.8             23.7             15.2              9.3   \n",
       "\n",
       "   $65,000-$74,999 $75,000-$99,999 $100,000_or_more median_household_income  \\\n",
       "0              6.2             1.4             16.3                   10816   \n",
       "1              7.7             2.9             21.2                   16079   \n",
       "2              8.7             2.4             21.9                   16804   \n",
       "3              1.8               0             12.8                   12512   \n",
       "4              7.5             1.6             18.4                   17475   \n",
       "\n",
       "  mean_household_income  \n",
       "0                 20349  \n",
       "1                 23282  \n",
       "2                 26820  \n",
       "3                 15730  \n",
       "4                 23360  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>2005Q1_gdp</th>\n",
       "      <th>2005Q2_gdp</th>\n",
       "      <th>2005Q3_gdp</th>\n",
       "      <th>2005Q4_gdp</th>\n",
       "      <th>2006Q1_gdp</th>\n",
       "      <th>2006Q2_gdp</th>\n",
       "      <th>2006Q3_gdp</th>\n",
       "      <th>2006Q4_gdp</th>\n",
       "      <th>2007Q1_gdp</th>\n",
       "      <th>...</th>\n",
       "      <th>2016/03_ur</th>\n",
       "      <th>2016/04_ur</th>\n",
       "      <th>2016/05_ur</th>\n",
       "      <th>2016/06_ur</th>\n",
       "      <th>2016/07_ur</th>\n",
       "      <th>2016/08_ur</th>\n",
       "      <th>2016/09_ur</th>\n",
       "      <th>2016/10_ur</th>\n",
       "      <th>2016/11_ur</th>\n",
       "      <th>2016/12_ur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>153332</td>\n",
       "      <td>155940</td>\n",
       "      <td>157437</td>\n",
       "      <td>160293</td>\n",
       "      <td>161934</td>\n",
       "      <td>163974</td>\n",
       "      <td>165470</td>\n",
       "      <td>166495</td>\n",
       "      <td>166821</td>\n",
       "      <td>...</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>37517</td>\n",
       "      <td>38907</td>\n",
       "      <td>40691</td>\n",
       "      <td>43138</td>\n",
       "      <td>42872</td>\n",
       "      <td>44653</td>\n",
       "      <td>45349</td>\n",
       "      <td>45840</td>\n",
       "      <td>46658</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>218206</td>\n",
       "      <td>224496</td>\n",
       "      <td>231629</td>\n",
       "      <td>235099</td>\n",
       "      <td>241787</td>\n",
       "      <td>244659</td>\n",
       "      <td>250886</td>\n",
       "      <td>256505</td>\n",
       "      <td>258078</td>\n",
       "      <td>...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>88446</td>\n",
       "      <td>89264</td>\n",
       "      <td>90515</td>\n",
       "      <td>93050</td>\n",
       "      <td>93413</td>\n",
       "      <td>95259</td>\n",
       "      <td>95481</td>\n",
       "      <td>95203</td>\n",
       "      <td>94289</td>\n",
       "      <td>...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>1722091</td>\n",
       "      <td>1747827</td>\n",
       "      <td>1787427</td>\n",
       "      <td>1809426</td>\n",
       "      <td>1857944</td>\n",
       "      <td>1865835</td>\n",
       "      <td>1886549</td>\n",
       "      <td>1907754</td>\n",
       "      <td>1915172</td>\n",
       "      <td>...</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 519 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  2005Q1_gdp  2005Q2_gdp  2005Q3_gdp  2005Q4_gdp  2006Q1_gdp  \\\n",
       "0    AL      153332      155940      157437      160293      161934   \n",
       "1    AK       37517       38907       40691       43138       42872   \n",
       "2    AZ      218206      224496      231629      235099      241787   \n",
       "3    AR       88446       89264       90515       93050       93413   \n",
       "4    CA     1722091     1747827     1787427     1809426     1857944   \n",
       "\n",
       "   2006Q2_gdp  2006Q3_gdp  2006Q4_gdp  2007Q1_gdp  ...  2016/03_ur  \\\n",
       "0      163974      165470      166495      166821  ...         6.6   \n",
       "1       44653       45349       45840       46658  ...         5.9   \n",
       "2      244659      250886      256505      258078  ...         4.1   \n",
       "3       95259       95481       95203       94289  ...         5.5   \n",
       "4     1865835     1886549     1907754     1915172  ...         5.6   \n",
       "\n",
       "   2016/04_ur  2016/05_ur  2016/06_ur  2016/07_ur  2016/08_ur  2016/09_ur  \\\n",
       "0         6.6         6.6         6.7         6.7         6.7         6.6   \n",
       "1         5.8         5.8         5.8         5.8         5.9         6.0   \n",
       "2         4.1         4.1         4.1         4.0         4.0         4.0   \n",
       "3         5.4         5.3         5.3         5.2         5.1         5.1   \n",
       "4         5.5         5.5         5.5         5.4         5.4         5.3   \n",
       "\n",
       "   2016/10_ur  2016/11_ur  2016/12_ur  \n",
       "0         6.6         6.6         6.6  \n",
       "1         6.1         6.2         6.3  \n",
       "2         4.0         4.0         3.9  \n",
       "3         5.0         5.0         5.0  \n",
       "4         5.3         5.3         5.2  \n",
       "\n",
       "[5 rows x 519 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "econ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>metro</th>\n",
       "      <th>county</th>\n",
       "      <th>size_rank</th>\n",
       "      <th>1996-04</th>\n",
       "      <th>1996-05</th>\n",
       "      <th>1996-06</th>\n",
       "      <th>...</th>\n",
       "      <th>2016-09</th>\n",
       "      <th>2016-10</th>\n",
       "      <th>2016-11</th>\n",
       "      <th>2016-12</th>\n",
       "      <th>2017-01</th>\n",
       "      <th>2017-02</th>\n",
       "      <th>2017-03</th>\n",
       "      <th>2017-04</th>\n",
       "      <th>2017-05</th>\n",
       "      <th>2017-06</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10025</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1137500</td>\n",
       "      <td>1137700</td>\n",
       "      <td>1152700</td>\n",
       "      <td>1156000.0</td>\n",
       "      <td>1140200</td>\n",
       "      <td>1130000</td>\n",
       "      <td>1131900</td>\n",
       "      <td>1149600</td>\n",
       "      <td>1198400</td>\n",
       "      <td>1247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>60657</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>2</td>\n",
       "      <td>146700.0</td>\n",
       "      <td>146500.0</td>\n",
       "      <td>146300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>318200</td>\n",
       "      <td>318100</td>\n",
       "      <td>318800</td>\n",
       "      <td>320200.0</td>\n",
       "      <td>320800</td>\n",
       "      <td>322000</td>\n",
       "      <td>323800</td>\n",
       "      <td>326100</td>\n",
       "      <td>327800</td>\n",
       "      <td>329100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>60614</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>3</td>\n",
       "      <td>198000.0</td>\n",
       "      <td>195500.0</td>\n",
       "      <td>194200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>401900</td>\n",
       "      <td>406000</td>\n",
       "      <td>414100</td>\n",
       "      <td>417800.0</td>\n",
       "      <td>417400</td>\n",
       "      <td>418400</td>\n",
       "      <td>414100</td>\n",
       "      <td>404100</td>\n",
       "      <td>406400</td>\n",
       "      <td>415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>79936</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>TX</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>4</td>\n",
       "      <td>70800.0</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>113800</td>\n",
       "      <td>113900</td>\n",
       "      <td>114100</td>\n",
       "      <td>114500.0</td>\n",
       "      <td>114900</td>\n",
       "      <td>115000</td>\n",
       "      <td>114700</td>\n",
       "      <td>114700</td>\n",
       "      <td>114800</td>\n",
       "      <td>114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>60640</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>5</td>\n",
       "      <td>102300.0</td>\n",
       "      <td>101300.0</td>\n",
       "      <td>100700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>198800</td>\n",
       "      <td>199200</td>\n",
       "      <td>200100</td>\n",
       "      <td>201500.0</td>\n",
       "      <td>203000</td>\n",
       "      <td>205100</td>\n",
       "      <td>206700</td>\n",
       "      <td>206500</td>\n",
       "      <td>206200</td>\n",
       "      <td>206700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29106</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>12720</td>\n",
       "      <td>Bethel</td>\n",
       "      <td>NY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sullivan</td>\n",
       "      <td>15907</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1185</td>\n",
       "      <td>1181</td>\n",
       "      <td>1174</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>1156</td>\n",
       "      <td>1162</td>\n",
       "      <td>1175</td>\n",
       "      <td>1194</td>\n",
       "      <td>1202</td>\n",
       "      <td>1206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29107</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>1338</td>\n",
       "      <td>Shelburne Falls</td>\n",
       "      <td>MA</td>\n",
       "      <td>Greenfield Town</td>\n",
       "      <td>Franklin</td>\n",
       "      <td>15908</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1769</td>\n",
       "      <td>1772</td>\n",
       "      <td>1775</td>\n",
       "      <td>1780.0</td>\n",
       "      <td>1786</td>\n",
       "      <td>1790</td>\n",
       "      <td>1797</td>\n",
       "      <td>1798</td>\n",
       "      <td>1796</td>\n",
       "      <td>1795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29108</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>21405</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>MD</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>Anne Arundel</td>\n",
       "      <td>15909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>4048</td>\n",
       "      <td>4005</td>\n",
       "      <td>3888</td>\n",
       "      <td>3729.0</td>\n",
       "      <td>3563</td>\n",
       "      <td>3493</td>\n",
       "      <td>3474</td>\n",
       "      <td>3496</td>\n",
       "      <td>3488</td>\n",
       "      <td>3471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29109</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>85220</td>\n",
       "      <td>Apache Junction</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Pinal</td>\n",
       "      <td>15910</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1007</td>\n",
       "      <td>1018</td>\n",
       "      <td>1035</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>1061</td>\n",
       "      <td>1060</td>\n",
       "      <td>1058</td>\n",
       "      <td>1065</td>\n",
       "      <td>1074</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29110</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>89595</td>\n",
       "      <td>Reno</td>\n",
       "      <td>NV</td>\n",
       "      <td>Reno</td>\n",
       "      <td>Washoe</td>\n",
       "      <td>15911</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>872</td>\n",
       "      <td>877</td>\n",
       "      <td>871</td>\n",
       "      <td>863.0</td>\n",
       "      <td>843</td>\n",
       "      <td>826</td>\n",
       "      <td>816</td>\n",
       "      <td>820</td>\n",
       "      <td>838</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29111 rows × 262 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type  zipcode             city state            metro        county  \\\n",
       "0      ZHVI    10025         New York    NY         New York      New York   \n",
       "1      ZHVI    60657          Chicago    IL          Chicago          Cook   \n",
       "2      ZHVI    60614          Chicago    IL          Chicago          Cook   \n",
       "3      ZHVI    79936          El Paso    TX          El Paso       El Paso   \n",
       "4      ZHVI    60640          Chicago    IL          Chicago          Cook   \n",
       "...     ...      ...              ...   ...              ...           ...   \n",
       "29106   ZRI    12720           Bethel    NY              NaN      Sullivan   \n",
       "29107   ZRI     1338  Shelburne Falls    MA  Greenfield Town      Franklin   \n",
       "29108   ZRI    21405        Annapolis    MD        Baltimore  Anne Arundel   \n",
       "29109   ZRI    85220  Apache Junction    AZ          Phoenix         Pinal   \n",
       "29110   ZRI    89595             Reno    NV             Reno        Washoe   \n",
       "\n",
       "       size_rank   1996-04   1996-05   1996-06  ...  2016-09  2016-10  \\\n",
       "0              1       NaN       NaN       NaN  ...  1137500  1137700   \n",
       "1              2  146700.0  146500.0  146300.0  ...   318200   318100   \n",
       "2              3  198000.0  195500.0  194200.0  ...   401900   406000   \n",
       "3              4   70800.0   71000.0   71000.0  ...   113800   113900   \n",
       "4              5  102300.0  101300.0  100700.0  ...   198800   199200   \n",
       "...          ...       ...       ...       ...  ...      ...      ...   \n",
       "29106      15907       NaN       NaN       NaN  ...     1185     1181   \n",
       "29107      15908       NaN       NaN       NaN  ...     1769     1772   \n",
       "29108      15909       NaN       NaN       NaN  ...     4048     4005   \n",
       "29109      15910       NaN       NaN       NaN  ...     1007     1018   \n",
       "29110      15911       NaN       NaN       NaN  ...      872      877   \n",
       "\n",
       "       2016-11    2016-12  2017-01  2017-02  2017-03  2017-04  2017-05  \\\n",
       "0      1152700  1156000.0  1140200  1130000  1131900  1149600  1198400   \n",
       "1       318800   320200.0   320800   322000   323800   326100   327800   \n",
       "2       414100   417800.0   417400   418400   414100   404100   406400   \n",
       "3       114100   114500.0   114900   115000   114700   114700   114800   \n",
       "4       200100   201500.0   203000   205100   206700   206500   206200   \n",
       "...        ...        ...      ...      ...      ...      ...      ...   \n",
       "29106     1174     1167.0     1156     1162     1175     1194     1202   \n",
       "29107     1775     1780.0     1786     1790     1797     1798     1796   \n",
       "29108     3888     3729.0     3563     3493     3474     3496     3488   \n",
       "29109     1035     1052.0     1061     1060     1058     1065     1074   \n",
       "29110      871      863.0      843      826      816      820      838   \n",
       "\n",
       "       2017-06  \n",
       "0      1247000  \n",
       "1       329100  \n",
       "2       415500  \n",
       "3       114700  \n",
       "4       206700  \n",
       "...        ...  \n",
       "29106     1206  \n",
       "29107     1795  \n",
       "29108     3471  \n",
       "29109     1080  \n",
       "29110      848  \n",
       "\n",
       "[29111 rows x 262 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realestate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "      <th>metro_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-05</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-03</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        date available  price metro_area\n",
       "0        2515  2018-03-05         t   69.0        NYC\n",
       "1        2515  2018-03-04         t   69.0        NYC\n",
       "2        2515  2018-03-03         t   69.0        NYC\n",
       "3        2515  2018-03-02         t   69.0        NYC\n",
       "4        2515  2018-03-01         t   69.0        NYC"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new york city</td>\n",
       "      <td>b1a0d113cb17d1d85f0e12700dd71f36bddedc54</td>\n",
       "      <td>40.601540</td>\n",
       "      <td>-73.729636</td>\n",
       "      <td>A Bacon Yacht Charter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new york city</td>\n",
       "      <td>8799bb8d8ac7c2e1933f92ac1ef0f69a5e0b37c1</td>\n",
       "      <td>40.608921</td>\n",
       "      <td>-73.728256</td>\n",
       "      <td>Mezzanote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new york city</td>\n",
       "      <td>f2390680116af4d62e0da6f0432d33e94b9cb0e6</td>\n",
       "      <td>40.616978</td>\n",
       "      <td>-73.730349</td>\n",
       "      <td>Prime Bistro</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['bar', 'restaurant', 'food', 'point_of_intere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new york city</td>\n",
       "      <td>e9294eb56025e8eb29b937dc0aca29fd3059b9ab</td>\n",
       "      <td>40.617318</td>\n",
       "      <td>-73.730637</td>\n",
       "      <td>Rita's Ice</td>\n",
       "      <td>4.6</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new york city</td>\n",
       "      <td>5c7b72c7bf9e7bc4f94baf09db1252088087f65d</td>\n",
       "      <td>40.618371</td>\n",
       "      <td>-73.728178</td>\n",
       "      <td>Cho-Sen Island</td>\n",
       "      <td>4.4</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            city                                        id   latitude  \\\n",
       "0  new york city  b1a0d113cb17d1d85f0e12700dd71f36bddedc54  40.601540   \n",
       "1  new york city  8799bb8d8ac7c2e1933f92ac1ef0f69a5e0b37c1  40.608921   \n",
       "2  new york city  f2390680116af4d62e0da6f0432d33e94b9cb0e6  40.616978   \n",
       "3  new york city  e9294eb56025e8eb29b937dc0aca29fd3059b9ab  40.617318   \n",
       "4  new york city  5c7b72c7bf9e7bc4f94baf09db1252088087f65d  40.618371   \n",
       "\n",
       "   longitude                   name  rating  \\\n",
       "0 -73.729636  A Bacon Yacht Charter     NaN   \n",
       "1 -73.728256              Mezzanote     NaN   \n",
       "2 -73.730349           Prime Bistro     4.0   \n",
       "3 -73.730637             Rita's Ice     4.6   \n",
       "4 -73.728178         Cho-Sen Island     4.4   \n",
       "\n",
       "                                               types  \n",
       "0  ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "1  ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "2  ['bar', 'restaurant', 'food', 'point_of_intere...  \n",
       "3  ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "4  ['restaurant', 'food', 'point_of_interest', 'e...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: (4 min)\n",
    "\n",
    "Clean the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new york city</td>\n",
       "      <td>f2390680116af4d62e0da6f0432d33e94b9cb0e6</td>\n",
       "      <td>40.616978</td>\n",
       "      <td>-73.730349</td>\n",
       "      <td>Prime Bistro</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['bar', 'restaurant', 'food', 'point_of_intere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new york city</td>\n",
       "      <td>e9294eb56025e8eb29b937dc0aca29fd3059b9ab</td>\n",
       "      <td>40.617318</td>\n",
       "      <td>-73.730637</td>\n",
       "      <td>Rita's Ice</td>\n",
       "      <td>4.6</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new york city</td>\n",
       "      <td>5c7b72c7bf9e7bc4f94baf09db1252088087f65d</td>\n",
       "      <td>40.618371</td>\n",
       "      <td>-73.728178</td>\n",
       "      <td>Cho-Sen Island</td>\n",
       "      <td>4.4</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>new york city</td>\n",
       "      <td>0b99220b44ee0d45d28f44e95d08da112f6e2ca7</td>\n",
       "      <td>40.618126</td>\n",
       "      <td>-73.728679</td>\n",
       "      <td>Sunflower Cafe- Lawrence</td>\n",
       "      <td>4.2</td>\n",
       "      <td>['cafe', 'restaurant', 'food', 'point_of_inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>new york city</td>\n",
       "      <td>97fdbf124eeb91d70d7c3f710ca52c997d618bb9</td>\n",
       "      <td>40.617592</td>\n",
       "      <td>-73.729390</td>\n",
       "      <td>Sushi Tokyo Lawrence</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130580</th>\n",
       "      <td>new york city</td>\n",
       "      <td>3a87c22d2097ff671c680532bcf99d5665c2f79d</td>\n",
       "      <td>40.767784</td>\n",
       "      <td>-73.974471</td>\n",
       "      <td>Victorian Gardens Amusement Park</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['amusement_park', 'point_of_interest', 'estab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130583</th>\n",
       "      <td>new york city</td>\n",
       "      <td>1399b4a298f5376d6639f7a7250f847b22aa7c81</td>\n",
       "      <td>40.590669</td>\n",
       "      <td>-73.994694</td>\n",
       "      <td>Adventurers Amusement Park</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['amusement_park', 'point_of_interest', 'estab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130584</th>\n",
       "      <td>new york city</td>\n",
       "      <td>ee5de6cef5e9acd0820f0f293b5788f06f027bab</td>\n",
       "      <td>40.694869</td>\n",
       "      <td>-74.001728</td>\n",
       "      <td>Brooklyn Bridge Park Pier 5</td>\n",
       "      <td>4.6</td>\n",
       "      <td>['amusement_park', 'premise', 'point_of_intere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130586</th>\n",
       "      <td>new york city</td>\n",
       "      <td>680c0019c6aec12c75a21dd8ba1568e78eb0c3b8</td>\n",
       "      <td>40.873391</td>\n",
       "      <td>-73.990393</td>\n",
       "      <td>Field Station: Dinosaurs</td>\n",
       "      <td>3.6</td>\n",
       "      <td>['amusement_park', 'point_of_interest', 'estab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130587</th>\n",
       "      <td>new york city</td>\n",
       "      <td>9a9197183b0dc768834ece2a7efcc1937a0d135d</td>\n",
       "      <td>40.706807</td>\n",
       "      <td>-74.018736</td>\n",
       "      <td>Oasis Park</td>\n",
       "      <td>4.8</td>\n",
       "      <td>['amusement_park', 'point_of_interest', 'estab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>69847 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 city                                        id   latitude  \\\n",
       "2       new york city  f2390680116af4d62e0da6f0432d33e94b9cb0e6  40.616978   \n",
       "3       new york city  e9294eb56025e8eb29b937dc0aca29fd3059b9ab  40.617318   \n",
       "4       new york city  5c7b72c7bf9e7bc4f94baf09db1252088087f65d  40.618371   \n",
       "5       new york city  0b99220b44ee0d45d28f44e95d08da112f6e2ca7  40.618126   \n",
       "6       new york city  97fdbf124eeb91d70d7c3f710ca52c997d618bb9  40.617592   \n",
       "...               ...                                       ...        ...   \n",
       "130580  new york city  3a87c22d2097ff671c680532bcf99d5665c2f79d  40.767784   \n",
       "130583  new york city  1399b4a298f5376d6639f7a7250f847b22aa7c81  40.590669   \n",
       "130584  new york city  ee5de6cef5e9acd0820f0f293b5788f06f027bab  40.694869   \n",
       "130586  new york city  680c0019c6aec12c75a21dd8ba1568e78eb0c3b8  40.873391   \n",
       "130587  new york city  9a9197183b0dc768834ece2a7efcc1937a0d135d  40.706807   \n",
       "\n",
       "        longitude                              name  rating  \\\n",
       "2      -73.730349                      Prime Bistro     4.0   \n",
       "3      -73.730637                        Rita's Ice     4.6   \n",
       "4      -73.728178                    Cho-Sen Island     4.4   \n",
       "5      -73.728679          Sunflower Cafe- Lawrence     4.2   \n",
       "6      -73.729390              Sushi Tokyo Lawrence     4.0   \n",
       "...           ...                               ...     ...   \n",
       "130580 -73.974471  Victorian Gardens Amusement Park     4.0   \n",
       "130583 -73.994694        Adventurers Amusement Park     4.0   \n",
       "130584 -74.001728       Brooklyn Bridge Park Pier 5     4.6   \n",
       "130586 -73.990393          Field Station: Dinosaurs     3.6   \n",
       "130587 -74.018736                        Oasis Park     4.8   \n",
       "\n",
       "                                                    types  \n",
       "2       ['bar', 'restaurant', 'food', 'point_of_intere...  \n",
       "3       ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "4       ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "5       ['cafe', 'restaurant', 'food', 'point_of_inter...  \n",
       "6       ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "...                                                   ...  \n",
       "130580  ['amusement_park', 'point_of_interest', 'estab...  \n",
       "130583  ['amusement_park', 'point_of_interest', 'estab...  \n",
       "130584  ['amusement_park', 'premise', 'point_of_intere...  \n",
       "130586  ['amusement_park', 'point_of_interest', 'estab...  \n",
       "130587  ['amusement_park', 'point_of_interest', 'estab...  \n",
       "\n",
       "[69847 rows x 7 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning venues, only get NYC drops NA values\n",
    "venuescl = venues[venues.city == 'new york city']\n",
    "venuescl[venuescl.columns[~venues.isnull().any()]]\n",
    "venuescl = venuescl.dropna(how='any',axis=0) \n",
    "venuescl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "      <th>metro_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-05</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-03</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14874793</th>\n",
       "      <td>18519989</td>\n",
       "      <td>2017-06-23</td>\n",
       "      <td>t</td>\n",
       "      <td>159.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14874794</th>\n",
       "      <td>18519989</td>\n",
       "      <td>2017-06-22</td>\n",
       "      <td>t</td>\n",
       "      <td>154.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14874795</th>\n",
       "      <td>18519989</td>\n",
       "      <td>2017-06-21</td>\n",
       "      <td>t</td>\n",
       "      <td>153.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14874796</th>\n",
       "      <td>18519989</td>\n",
       "      <td>2017-06-20</td>\n",
       "      <td>t</td>\n",
       "      <td>150.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14874797</th>\n",
       "      <td>18519989</td>\n",
       "      <td>2017-06-19</td>\n",
       "      <td>t</td>\n",
       "      <td>145.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5464228 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          listing_id        date available  price metro_area\n",
       "0               2515  2018-03-05         t   69.0        NYC\n",
       "1               2515  2018-03-04         t   69.0        NYC\n",
       "2               2515  2018-03-03         t   69.0        NYC\n",
       "3               2515  2018-03-02         t   69.0        NYC\n",
       "4               2515  2018-03-01         t   69.0        NYC\n",
       "...              ...         ...       ...    ...        ...\n",
       "14874793    18519989  2017-06-23         t  159.0        NYC\n",
       "14874794    18519989  2017-06-22         t  154.0        NYC\n",
       "14874795    18519989  2017-06-21         t  153.0        NYC\n",
       "14874796    18519989  2017-06-20         t  150.0        NYC\n",
       "14874797    18519989  2017-06-19         t  145.0        NYC\n",
       "\n",
       "[5464228 rows x 5 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning Calender, drops NAS and gets only avaliable airbnbs\n",
    "calendar_aval = calendar[calendar.available == 't']\n",
    "calendar_aval[calendar_aval.columns[~calendar_aval.isnull().any()]]\n",
    "calendar_aval = calendar_aval[calendar_aval.metro_area == 'NYC']\n",
    "calendar_aval[calendar_aval.columns[~calendar_aval.isnull().any()]]\n",
    "calendar_aval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>metro</th>\n",
       "      <th>county</th>\n",
       "      <th>size_rank</th>\n",
       "      <th>2016-08</th>\n",
       "      <th>2016-09</th>\n",
       "      <th>2016-10</th>\n",
       "      <th>2016-11</th>\n",
       "      <th>2016-12</th>\n",
       "      <th>2017-01</th>\n",
       "      <th>2017-02</th>\n",
       "      <th>2017-03</th>\n",
       "      <th>2017-04</th>\n",
       "      <th>2017-05</th>\n",
       "      <th>2017-06</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10025</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>1</td>\n",
       "      <td>1132500</td>\n",
       "      <td>1137500</td>\n",
       "      <td>1137700</td>\n",
       "      <td>1152700</td>\n",
       "      <td>1156000.0</td>\n",
       "      <td>1140200</td>\n",
       "      <td>1130000</td>\n",
       "      <td>1131900</td>\n",
       "      <td>1149600</td>\n",
       "      <td>1198400</td>\n",
       "      <td>1247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>11226</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>Kings</td>\n",
       "      <td>9</td>\n",
       "      <td>572800</td>\n",
       "      <td>583600</td>\n",
       "      <td>594800</td>\n",
       "      <td>605200</td>\n",
       "      <td>612100.0</td>\n",
       "      <td>612800</td>\n",
       "      <td>616900</td>\n",
       "      <td>628900</td>\n",
       "      <td>644200</td>\n",
       "      <td>653500</td>\n",
       "      <td>658700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10016</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>11</td>\n",
       "      <td>917000</td>\n",
       "      <td>934800</td>\n",
       "      <td>946000</td>\n",
       "      <td>949200</td>\n",
       "      <td>950000.0</td>\n",
       "      <td>951800</td>\n",
       "      <td>960100</td>\n",
       "      <td>972200</td>\n",
       "      <td>986900</td>\n",
       "      <td>1021200</td>\n",
       "      <td>1061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10128</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>18</td>\n",
       "      <td>1077000</td>\n",
       "      <td>1074900</td>\n",
       "      <td>1076400</td>\n",
       "      <td>1090300</td>\n",
       "      <td>1111300.0</td>\n",
       "      <td>1133100</td>\n",
       "      <td>1154700</td>\n",
       "      <td>1189800</td>\n",
       "      <td>1241000</td>\n",
       "      <td>1288200</td>\n",
       "      <td>1318300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10462</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>24</td>\n",
       "      <td>118200</td>\n",
       "      <td>120600</td>\n",
       "      <td>121600</td>\n",
       "      <td>121500</td>\n",
       "      <td>121000.0</td>\n",
       "      <td>121100</td>\n",
       "      <td>123200</td>\n",
       "      <td>126200</td>\n",
       "      <td>128700</td>\n",
       "      <td>131900</td>\n",
       "      <td>135400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23669</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>11363</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>Queens</td>\n",
       "      <td>10470</td>\n",
       "      <td>2522</td>\n",
       "      <td>2547</td>\n",
       "      <td>2571</td>\n",
       "      <td>2608</td>\n",
       "      <td>2627.0</td>\n",
       "      <td>2657</td>\n",
       "      <td>2680</td>\n",
       "      <td>2697</td>\n",
       "      <td>2697</td>\n",
       "      <td>2707</td>\n",
       "      <td>2739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23952</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>10006</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>10753</td>\n",
       "      <td>3505</td>\n",
       "      <td>3481</td>\n",
       "      <td>3456</td>\n",
       "      <td>3439</td>\n",
       "      <td>3410.0</td>\n",
       "      <td>3386</td>\n",
       "      <td>3361</td>\n",
       "      <td>3331</td>\n",
       "      <td>3301</td>\n",
       "      <td>3284</td>\n",
       "      <td>3289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24257</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>10004</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>11058</td>\n",
       "      <td>4918</td>\n",
       "      <td>4871</td>\n",
       "      <td>4836</td>\n",
       "      <td>4806</td>\n",
       "      <td>4774.0</td>\n",
       "      <td>4761</td>\n",
       "      <td>4738</td>\n",
       "      <td>4740</td>\n",
       "      <td>4749</td>\n",
       "      <td>4781</td>\n",
       "      <td>4770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24892</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>10464</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>11693</td>\n",
       "      <td>1903</td>\n",
       "      <td>1898</td>\n",
       "      <td>1877</td>\n",
       "      <td>1858</td>\n",
       "      <td>1843.0</td>\n",
       "      <td>1853</td>\n",
       "      <td>1885</td>\n",
       "      <td>1915</td>\n",
       "      <td>1927</td>\n",
       "      <td>1938</td>\n",
       "      <td>1946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25197</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>11005</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>Queens</td>\n",
       "      <td>11998</td>\n",
       "      <td>2150</td>\n",
       "      <td>2133</td>\n",
       "      <td>2130</td>\n",
       "      <td>2145</td>\n",
       "      <td>2157.0</td>\n",
       "      <td>2160</td>\n",
       "      <td>2155</td>\n",
       "      <td>2157</td>\n",
       "      <td>2163</td>\n",
       "      <td>2160</td>\n",
       "      <td>2145</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type  zipcode      city state     metro    county  size_rank  2016-08  \\\n",
       "0      ZHVI    10025  New York    NY  New York  New York          1  1132500   \n",
       "8      ZHVI    11226  New York    NY  New York     Kings          9   572800   \n",
       "10     ZHVI    10016  New York    NY  New York  New York         11   917000   \n",
       "17     ZHVI    10128  New York    NY  New York  New York         18  1077000   \n",
       "23     ZHVI    10462  New York    NY  New York     Bronx         24   118200   \n",
       "...     ...      ...       ...   ...       ...       ...        ...      ...   \n",
       "23669   ZRI    11363  New York    NY  New York    Queens      10470     2522   \n",
       "23952   ZRI    10006  New York    NY  New York  New York      10753     3505   \n",
       "24257   ZRI    10004  New York    NY  New York  New York      11058     4918   \n",
       "24892   ZRI    10464  New York    NY  New York     Bronx      11693     1903   \n",
       "25197   ZRI    11005  New York    NY  New York    Queens      11998     2150   \n",
       "\n",
       "       2016-09  2016-10  2016-11    2016-12  2017-01  2017-02  2017-03  \\\n",
       "0      1137500  1137700  1152700  1156000.0  1140200  1130000  1131900   \n",
       "8       583600   594800   605200   612100.0   612800   616900   628900   \n",
       "10      934800   946000   949200   950000.0   951800   960100   972200   \n",
       "17     1074900  1076400  1090300  1111300.0  1133100  1154700  1189800   \n",
       "23      120600   121600   121500   121000.0   121100   123200   126200   \n",
       "...        ...      ...      ...        ...      ...      ...      ...   \n",
       "23669     2547     2571     2608     2627.0     2657     2680     2697   \n",
       "23952     3481     3456     3439     3410.0     3386     3361     3331   \n",
       "24257     4871     4836     4806     4774.0     4761     4738     4740   \n",
       "24892     1898     1877     1858     1843.0     1853     1885     1915   \n",
       "25197     2133     2130     2145     2157.0     2160     2155     2157   \n",
       "\n",
       "       2017-04  2017-05  2017-06  \n",
       "0      1149600  1198400  1247000  \n",
       "8       644200   653500   658700  \n",
       "10      986900  1021200  1061200  \n",
       "17     1241000  1288200  1318300  \n",
       "23      128700   131900   135400  \n",
       "...        ...      ...      ...  \n",
       "23669     2697     2707     2739  \n",
       "23952     3301     3284     3289  \n",
       "24257     4749     4781     4770  \n",
       "24892     1927     1938     1946  \n",
       "25197     2163     2160     2145  \n",
       "\n",
       "[271 rows x 18 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning real estate, gets only 2016-08 onwards\n",
    "realcleaned = realestate.filter([\"type\",\"zipcode\",\"city\",\"state\",\"metro\",\"county\",\"size_rank\",\"2016-08\",\"2016-09\",\"2016-10\",\"2016-11\",\"2016-12\",\"2017-01\",\"2017-02\",\"2017-03\",\"2017-04\",\"2017-05\",\"2017-06\"], axis=1)\n",
    "realcleaned = realcleaned[realcleaned.city == 'New York']\n",
    "realcleaned[realcleaned.columns[~realcleaned.isnull().any()]]\n",
    "realcleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>5_years_or_less</th>\n",
       "      <th>5-9_years</th>\n",
       "      <th>10-14_years</th>\n",
       "      <th>15-19_years</th>\n",
       "      <th>20-24_years</th>\n",
       "      <th>25-34_years</th>\n",
       "      <th>35-44_years</th>\n",
       "      <th>45-54_years</th>\n",
       "      <th>55-59_years</th>\n",
       "      <th>...</th>\n",
       "      <th>$10,000-$14,999</th>\n",
       "      <th>$15,000-$24,999</th>\n",
       "      <th>$25,000-$34,999</th>\n",
       "      <th>$35,000-$49,999</th>\n",
       "      <th>$50,000-$64,999</th>\n",
       "      <th>$65,000-$74,999</th>\n",
       "      <th>$75,000-$99,999</th>\n",
       "      <th>$100,000_or_more</th>\n",
       "      <th>median_household_income</th>\n",
       "      <th>mean_household_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601</td>\n",
       "      <td>17982</td>\n",
       "      <td>1006</td>\n",
       "      <td>1080</td>\n",
       "      <td>1342</td>\n",
       "      <td>1352</td>\n",
       "      <td>1321</td>\n",
       "      <td>2253</td>\n",
       "      <td>2149</td>\n",
       "      <td>2434</td>\n",
       "      <td>...</td>\n",
       "      <td>48.1</td>\n",
       "      <td>12</td>\n",
       "      <td>12.8</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>16.3</td>\n",
       "      <td>10816</td>\n",
       "      <td>20349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>40260</td>\n",
       "      <td>2006</td>\n",
       "      <td>2440</td>\n",
       "      <td>2421</td>\n",
       "      <td>2953</td>\n",
       "      <td>2865</td>\n",
       "      <td>5124</td>\n",
       "      <td>5139</td>\n",
       "      <td>5947</td>\n",
       "      <td>...</td>\n",
       "      <td>31.4</td>\n",
       "      <td>16.3</td>\n",
       "      <td>17.9</td>\n",
       "      <td>12.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>16079</td>\n",
       "      <td>23282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>603</td>\n",
       "      <td>52408</td>\n",
       "      <td>2664</td>\n",
       "      <td>3177</td>\n",
       "      <td>3351</td>\n",
       "      <td>3685</td>\n",
       "      <td>3585</td>\n",
       "      <td>6473</td>\n",
       "      <td>6775</td>\n",
       "      <td>6678</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>14.9</td>\n",
       "      <td>17.5</td>\n",
       "      <td>11.7</td>\n",
       "      <td>10.8</td>\n",
       "      <td>8.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>21.9</td>\n",
       "      <td>16804</td>\n",
       "      <td>26820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>606</td>\n",
       "      <td>6331</td>\n",
       "      <td>347</td>\n",
       "      <td>331</td>\n",
       "      <td>461</td>\n",
       "      <td>474</td>\n",
       "      <td>469</td>\n",
       "      <td>707</td>\n",
       "      <td>933</td>\n",
       "      <td>776</td>\n",
       "      <td>...</td>\n",
       "      <td>45.3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>20</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>12512</td>\n",
       "      <td>15730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>610</td>\n",
       "      <td>28328</td>\n",
       "      <td>1438</td>\n",
       "      <td>1490</td>\n",
       "      <td>2044</td>\n",
       "      <td>2122</td>\n",
       "      <td>1985</td>\n",
       "      <td>3358</td>\n",
       "      <td>3778</td>\n",
       "      <td>3858</td>\n",
       "      <td>...</td>\n",
       "      <td>26.9</td>\n",
       "      <td>14.8</td>\n",
       "      <td>23.7</td>\n",
       "      <td>15.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>18.4</td>\n",
       "      <td>17475</td>\n",
       "      <td>23360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33115</th>\n",
       "      <td>99923</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>46.2</td>\n",
       "      <td>53.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33116</th>\n",
       "      <td>99925</td>\n",
       "      <td>826</td>\n",
       "      <td>65</td>\n",
       "      <td>50</td>\n",
       "      <td>47</td>\n",
       "      <td>36</td>\n",
       "      <td>60</td>\n",
       "      <td>86</td>\n",
       "      <td>82</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>7.1</td>\n",
       "      <td>6.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>11.1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>17.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>39.6</td>\n",
       "      <td>38594</td>\n",
       "      <td>52706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33117</th>\n",
       "      <td>99926</td>\n",
       "      <td>1711</td>\n",
       "      <td>161</td>\n",
       "      <td>124</td>\n",
       "      <td>140</td>\n",
       "      <td>113</td>\n",
       "      <td>107</td>\n",
       "      <td>224</td>\n",
       "      <td>182</td>\n",
       "      <td>236</td>\n",
       "      <td>...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>14.1</td>\n",
       "      <td>11.1</td>\n",
       "      <td>16.8</td>\n",
       "      <td>14.5</td>\n",
       "      <td>12</td>\n",
       "      <td>43.3</td>\n",
       "      <td>51071</td>\n",
       "      <td>71580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33118</th>\n",
       "      <td>99927</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>28.2</td>\n",
       "      <td>0</td>\n",
       "      <td>23.1</td>\n",
       "      <td>15.4</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>19861</td>\n",
       "      <td>35617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33119</th>\n",
       "      <td>99929</td>\n",
       "      <td>2365</td>\n",
       "      <td>89</td>\n",
       "      <td>103</td>\n",
       "      <td>133</td>\n",
       "      <td>142</td>\n",
       "      <td>100</td>\n",
       "      <td>223</td>\n",
       "      <td>197</td>\n",
       "      <td>381</td>\n",
       "      <td>...</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>14.2</td>\n",
       "      <td>9.9</td>\n",
       "      <td>15.1</td>\n",
       "      <td>17.6</td>\n",
       "      <td>13.6</td>\n",
       "      <td>46.3</td>\n",
       "      <td>47941</td>\n",
       "      <td>62587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33120 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       zipcode  5_years_or_less  5-9_years  10-14_years  15-19_years  \\\n",
       "0          601            17982       1006         1080         1342   \n",
       "1          602            40260       2006         2440         2421   \n",
       "2          603            52408       2664         3177         3351   \n",
       "3          606             6331        347          331          461   \n",
       "4          610            28328       1438         1490         2044   \n",
       "...        ...              ...        ...          ...          ...   \n",
       "33115    99923               13          0            0            0   \n",
       "33116    99925              826         65           50           47   \n",
       "33117    99926             1711        161          124          140   \n",
       "33118    99927              123          0            0            0   \n",
       "33119    99929             2365         89          103          133   \n",
       "\n",
       "       20-24_years  25-34_years  35-44_years  45-54_years  55-59_years  ...  \\\n",
       "0             1352         1321         2253         2149         2434  ...   \n",
       "1             2953         2865         5124         5139         5947  ...   \n",
       "2             3685         3585         6473         6775         6678  ...   \n",
       "3              474          469          707          933          776  ...   \n",
       "4             2122         1985         3358         3778         3858  ...   \n",
       "...            ...          ...          ...          ...          ...  ...   \n",
       "33115            0            0            0            0            0  ...   \n",
       "33116           36           60           86           82          111  ...   \n",
       "33117          113          107          224          182          236  ...   \n",
       "33118            0            0            0           32           22  ...   \n",
       "33119          142          100          223          197          381  ...   \n",
       "\n",
       "       $10,000-$14,999  $15,000-$24,999  $25,000-$34,999  $35,000-$49,999  \\\n",
       "0                 48.1               12             12.8              8.6   \n",
       "1                 31.4             16.3             17.9             12.2   \n",
       "2                   31             14.9             17.5             11.7   \n",
       "3                 45.3             10.2               20             11.7   \n",
       "4                 26.9             14.8             23.7             15.2   \n",
       "...                ...              ...              ...              ...   \n",
       "33115                0             46.2             53.8                0   \n",
       "33116              7.1              6.2             21.2             11.1   \n",
       "33117              5.5              1.9             14.1             11.1   \n",
       "33118             28.2                0             23.1             15.4   \n",
       "33119              5.1              6.9             14.2              9.9   \n",
       "\n",
       "       $50,000-$64,999  $65,000-$74,999 $75,000-$99,999 $100,000_or_more  \\\n",
       "0                  8.7              6.2             1.4             16.3   \n",
       "1                 10.6              7.7             2.9             21.2   \n",
       "2                 10.8              8.7             2.4             21.9   \n",
       "3                   11              1.8               0             12.8   \n",
       "4                  9.3              7.5             1.6             18.4   \n",
       "...                ...              ...             ...              ...   \n",
       "33115                0                0               0                0   \n",
       "33116             12.6             17.5             9.5             39.6   \n",
       "33117             16.8             14.5              12             43.3   \n",
       "33118             25.6                0               0             25.6   \n",
       "33119             15.1             17.6            13.6             46.3   \n",
       "\n",
       "      median_household_income mean_household_income  \n",
       "0                       10816                 20349  \n",
       "1                       16079                 23282  \n",
       "2                       16804                 26820  \n",
       "3                       12512                 15730  \n",
       "4                       17475                 23360  \n",
       "...                       ...                   ...  \n",
       "33115                       -                     N  \n",
       "33116                   38594                 52706  \n",
       "33117                   51071                 71580  \n",
       "33118                   19861                 35617  \n",
       "33119                   47941                 62587  \n",
       "\n",
       "[33120 rows x 26 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Econ doesnt need cleaning\n",
    "democlean = demo.filter([\"zipcode\",\"20-24_years\",\"25-34_years\",\"35-44_years\",\"35,000\",\"50,000\",\"65,000\",\"75,000\",\"households\",\"$9,999_or_less\",\"$10,000-$14,999\",\"$15,000-$24,999\",\"$25,000-$34,999\",\"$35,000-$49,999\",\"$50,000-$64,999\",\"$65,000-$74,999\",\"$75,000-$99,999\",\"$100,000_or_more\",\"median_household_income\",\"mean_household_income\"], axis=1)\n",
    "democlean = demo.dropna(how='any',axis=0) \n",
    "democlean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Isolating data for NYC\n",
    "\n",
    "Here we will isolate the data in each DF for NYC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10001: ['Manhattan'],\n",
       " 10002: ['Manhattan'],\n",
       " 10003: ['Manhattan'],\n",
       " 10004: ['Manhattan'],\n",
       " 10005: ['Manhattan'],\n",
       " 10006: ['Manhattan'],\n",
       " 10007: ['Manhattan'],\n",
       " 10009: ['Manhattan'],\n",
       " 10010: ['Manhattan'],\n",
       " 10011: ['Manhattan'],\n",
       " 10012: ['Manhattan'],\n",
       " 10013: ['Manhattan'],\n",
       " 10014: ['Manhattan'],\n",
       " 10015: ['Manhattan'],\n",
       " 10016: ['Manhattan'],\n",
       " 10017: ['Manhattan'],\n",
       " 10018: ['Manhattan'],\n",
       " 10019: ['Manhattan'],\n",
       " 10020: ['Manhattan'],\n",
       " 10021: ['Manhattan'],\n",
       " 10022: ['Manhattan'],\n",
       " 10023: ['Manhattan'],\n",
       " 10024: ['Manhattan'],\n",
       " 10025: ['Manhattan'],\n",
       " 10026: ['Manhattan'],\n",
       " 10027: ['Manhattan'],\n",
       " 10028: ['Manhattan'],\n",
       " 10029: ['Manhattan'],\n",
       " 10030: ['Manhattan'],\n",
       " 10031: ['Manhattan'],\n",
       " 10032: ['Manhattan'],\n",
       " 10033: ['Manhattan'],\n",
       " 10034: ['Manhattan'],\n",
       " 10035: ['Manhattan'],\n",
       " 10036: ['Manhattan'],\n",
       " 10037: ['Manhattan'],\n",
       " 10038: ['Manhattan'],\n",
       " 10039: ['Manhattan'],\n",
       " 10040: ['Manhattan'],\n",
       " 10041: ['Manhattan'],\n",
       " 10044: ['Manhattan'],\n",
       " 10045: ['Manhattan'],\n",
       " 10048: ['Manhattan'],\n",
       " 10055: ['Manhattan'],\n",
       " 10060: ['Manhattan'],\n",
       " 10069: ['Manhattan'],\n",
       " 10090: ['Manhattan'],\n",
       " 10095: ['Manhattan'],\n",
       " 10098: ['Manhattan'],\n",
       " 10099: ['Manhattan'],\n",
       " 10103: ['Manhattan'],\n",
       " 10104: ['Manhattan'],\n",
       " 10105: ['Manhattan'],\n",
       " 10106: ['Manhattan'],\n",
       " 10107: ['Manhattan'],\n",
       " 10110: ['Manhattan'],\n",
       " 10111: ['Manhattan'],\n",
       " 10112: ['Manhattan'],\n",
       " 10115: ['Manhattan'],\n",
       " 10118: ['Manhattan'],\n",
       " 10119: ['Manhattan'],\n",
       " 10120: ['Manhattan'],\n",
       " 10121: ['Manhattan'],\n",
       " 10122: ['Manhattan'],\n",
       " 10123: ['Manhattan'],\n",
       " 10128: ['Manhattan'],\n",
       " 10151: ['Manhattan'],\n",
       " 10152: ['Manhattan'],\n",
       " 10153: ['Manhattan'],\n",
       " 10154: ['Manhattan'],\n",
       " 10155: ['Manhattan'],\n",
       " 10158: ['Manhattan'],\n",
       " 10161: ['Manhattan'],\n",
       " 10162: ['Manhattan'],\n",
       " 10165: ['Manhattan'],\n",
       " 10166: ['Manhattan'],\n",
       " 10167: ['Manhattan'],\n",
       " 10168: ['Manhattan'],\n",
       " 10169: ['Manhattan'],\n",
       " 10170: ['Manhattan'],\n",
       " 10171: ['Manhattan'],\n",
       " 10172: ['Manhattan'],\n",
       " 10173: ['Manhattan'],\n",
       " 10174: ['Manhattan'],\n",
       " 10175: ['Manhattan'],\n",
       " 10176: ['Manhattan'],\n",
       " 10177: ['Manhattan'],\n",
       " 10178: ['Manhattan'],\n",
       " 10199: ['Manhattan'],\n",
       " 10270: ['Manhattan'],\n",
       " 10271: ['Manhattan'],\n",
       " 10278: ['Manhattan'],\n",
       " 10279: ['Manhattan'],\n",
       " 10280: ['Manhattan'],\n",
       " 10281: ['Manhattan'],\n",
       " 10282: ['Manhattan'],\n",
       " 10301: ['Staten'],\n",
       " 10302: ['Staten'],\n",
       " 10303: ['Staten'],\n",
       " 10304: ['Staten'],\n",
       " 10305: ['Staten'],\n",
       " 10306: ['Staten'],\n",
       " 10307: ['Staten'],\n",
       " 10308: ['Staten'],\n",
       " 10309: ['Staten'],\n",
       " 10310: ['Staten'],\n",
       " 10311: ['Staten'],\n",
       " 10312: ['Staten'],\n",
       " 10314: ['Staten'],\n",
       " 10451: ['Bronx'],\n",
       " 10452: ['Bronx'],\n",
       " 10453: ['Bronx'],\n",
       " 10454: ['Bronx'],\n",
       " 10455: ['Bronx'],\n",
       " 10456: ['Bronx'],\n",
       " 10457: ['Bronx'],\n",
       " 10458: ['Bronx'],\n",
       " 10459: ['Bronx'],\n",
       " 10460: ['Bronx'],\n",
       " 10461: ['Bronx'],\n",
       " 10462: ['Bronx'],\n",
       " 10463: ['Bronx'],\n",
       " 10464: ['Bronx'],\n",
       " 10465: ['Bronx'],\n",
       " 10466: ['Bronx'],\n",
       " 10467: ['Bronx'],\n",
       " 10468: ['Bronx'],\n",
       " 10469: ['Bronx'],\n",
       " 10470: ['Bronx'],\n",
       " 10471: ['Bronx'],\n",
       " 10472: ['Bronx'],\n",
       " 10473: ['Bronx'],\n",
       " 10474: ['Bronx'],\n",
       " 10475: ['Bronx'],\n",
       " 11004: ['Queens'],\n",
       " 11101: ['Queens'],\n",
       " 11102: ['Queens'],\n",
       " 11103: ['Queens'],\n",
       " 11104: ['Queens'],\n",
       " 11105: ['Queens'],\n",
       " 11106: ['Queens'],\n",
       " 11109: ['Queens'],\n",
       " 11201: ['Brooklyn'],\n",
       " 11203: ['Brooklyn'],\n",
       " 11204: ['Brooklyn'],\n",
       " 11205: ['Brooklyn'],\n",
       " 11206: ['Brooklyn'],\n",
       " 11207: ['Brooklyn'],\n",
       " 11208: ['Brooklyn'],\n",
       " 11209: ['Brooklyn'],\n",
       " 11210: ['Brooklyn'],\n",
       " 11211: ['Brooklyn'],\n",
       " 11212: ['Brooklyn'],\n",
       " 11213: ['Brooklyn'],\n",
       " 11214: ['Brooklyn'],\n",
       " 11215: ['Brooklyn'],\n",
       " 11216: ['Brooklyn'],\n",
       " 11217: ['Brooklyn'],\n",
       " 11218: ['Brooklyn'],\n",
       " 11219: ['Brooklyn'],\n",
       " 11220: ['Brooklyn'],\n",
       " 11221: ['Brooklyn'],\n",
       " 11222: ['Brooklyn'],\n",
       " 11223: ['Brooklyn'],\n",
       " 11224: ['Brooklyn'],\n",
       " 11225: ['Brooklyn'],\n",
       " 11226: ['Brooklyn'],\n",
       " 11228: ['Brooklyn'],\n",
       " 11229: ['Brooklyn'],\n",
       " 11230: ['Brooklyn'],\n",
       " 11231: ['Brooklyn'],\n",
       " 11232: ['Brooklyn'],\n",
       " 11233: ['Brooklyn'],\n",
       " 11234: ['Brooklyn'],\n",
       " 11235: ['Brooklyn'],\n",
       " 11236: ['Brooklyn'],\n",
       " 11237: ['Brooklyn'],\n",
       " 11238: ['Brooklyn'],\n",
       " 11239: ['Brooklyn'],\n",
       " 11241: ['Brooklyn'],\n",
       " 11242: ['Brooklyn'],\n",
       " 11243: ['Brooklyn'],\n",
       " 11249: ['Brooklyn'],\n",
       " 11252: ['Brooklyn'],\n",
       " 11256: ['Brooklyn'],\n",
       " 11351: ['Queens'],\n",
       " 11354: ['Queens'],\n",
       " 11355: ['Queens'],\n",
       " 11356: ['Queens'],\n",
       " 11357: ['Queens'],\n",
       " 11358: ['Queens'],\n",
       " 11359: ['Queens'],\n",
       " 11360: ['Queens'],\n",
       " 11361: ['Queens'],\n",
       " 11362: ['Queens'],\n",
       " 11363: ['Queens'],\n",
       " 11364: ['Queens'],\n",
       " 11365: ['Queens'],\n",
       " 11366: ['Queens'],\n",
       " 11367: ['Queens'],\n",
       " 11368: ['Queens'],\n",
       " 11369: ['Queens'],\n",
       " 11370: ['Queens'],\n",
       " 11371: ['Queens'],\n",
       " 11372: ['Queens'],\n",
       " 11373: ['Queens'],\n",
       " 11374: ['Queens'],\n",
       " 11375: ['Queens'],\n",
       " 11377: ['Queens'],\n",
       " 11378: ['Queens'],\n",
       " 11379: ['Queens'],\n",
       " 11385: ['Queens'],\n",
       " 11411: ['Queens'],\n",
       " 11412: ['Queens'],\n",
       " 11413: ['Queens'],\n",
       " 11414: ['Queens'],\n",
       " 11415: ['Queens'],\n",
       " 11416: ['Queens'],\n",
       " 11417: ['Queens'],\n",
       " 11418: ['Queens'],\n",
       " 11419: ['Queens'],\n",
       " 11420: ['Queens'],\n",
       " 11421: ['Queens'],\n",
       " 11422: ['Queens'],\n",
       " 11423: ['Queens'],\n",
       " 11426: ['Queens'],\n",
       " 11427: ['Queens'],\n",
       " 11428: ['Queens'],\n",
       " 11429: ['Queens'],\n",
       " 11430: ['Queens'],\n",
       " 11432: ['Queens'],\n",
       " 11433: ['Queens'],\n",
       " 11434: ['Queens'],\n",
       " 11435: ['Queens'],\n",
       " 11436: ['Queens'],\n",
       " 11691: ['Queens'],\n",
       " 11692: ['Queens'],\n",
       " 11693: ['Queens'],\n",
       " 11694: ['Queens'],\n",
       " 11697: ['Queens']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipdict = ziplook.set_index('Column1').T.to_dict('list')\n",
    "zipdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>metro</th>\n",
       "      <th>county</th>\n",
       "      <th>size_rank</th>\n",
       "      <th>2016-08</th>\n",
       "      <th>2016-09</th>\n",
       "      <th>2016-10</th>\n",
       "      <th>2016-11</th>\n",
       "      <th>2016-12</th>\n",
       "      <th>2017-01</th>\n",
       "      <th>2017-02</th>\n",
       "      <th>2017-03</th>\n",
       "      <th>2017-04</th>\n",
       "      <th>2017-05</th>\n",
       "      <th>2017-06</th>\n",
       "      <th>borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10025</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>1</td>\n",
       "      <td>1132500</td>\n",
       "      <td>1137500</td>\n",
       "      <td>1137700</td>\n",
       "      <td>1152700</td>\n",
       "      <td>1156000.0</td>\n",
       "      <td>1140200</td>\n",
       "      <td>1130000</td>\n",
       "      <td>1131900</td>\n",
       "      <td>1149600</td>\n",
       "      <td>1198400</td>\n",
       "      <td>1247000</td>\n",
       "      <td>[Manhattan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>11226</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>Kings</td>\n",
       "      <td>9</td>\n",
       "      <td>572800</td>\n",
       "      <td>583600</td>\n",
       "      <td>594800</td>\n",
       "      <td>605200</td>\n",
       "      <td>612100.0</td>\n",
       "      <td>612800</td>\n",
       "      <td>616900</td>\n",
       "      <td>628900</td>\n",
       "      <td>644200</td>\n",
       "      <td>653500</td>\n",
       "      <td>658700</td>\n",
       "      <td>[Brooklyn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10016</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>11</td>\n",
       "      <td>917000</td>\n",
       "      <td>934800</td>\n",
       "      <td>946000</td>\n",
       "      <td>949200</td>\n",
       "      <td>950000.0</td>\n",
       "      <td>951800</td>\n",
       "      <td>960100</td>\n",
       "      <td>972200</td>\n",
       "      <td>986900</td>\n",
       "      <td>1021200</td>\n",
       "      <td>1061200</td>\n",
       "      <td>[Manhattan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10128</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>18</td>\n",
       "      <td>1077000</td>\n",
       "      <td>1074900</td>\n",
       "      <td>1076400</td>\n",
       "      <td>1090300</td>\n",
       "      <td>1111300.0</td>\n",
       "      <td>1133100</td>\n",
       "      <td>1154700</td>\n",
       "      <td>1189800</td>\n",
       "      <td>1241000</td>\n",
       "      <td>1288200</td>\n",
       "      <td>1318300</td>\n",
       "      <td>[Manhattan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10462</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>24</td>\n",
       "      <td>118200</td>\n",
       "      <td>120600</td>\n",
       "      <td>121600</td>\n",
       "      <td>121500</td>\n",
       "      <td>121000.0</td>\n",
       "      <td>121100</td>\n",
       "      <td>123200</td>\n",
       "      <td>126200</td>\n",
       "      <td>128700</td>\n",
       "      <td>131900</td>\n",
       "      <td>135400</td>\n",
       "      <td>[Bronx]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type  zipcode      city state     metro    county  size_rank  2016-08  \\\n",
       "0   ZHVI    10025  New York    NY  New York  New York          1  1132500   \n",
       "8   ZHVI    11226  New York    NY  New York     Kings          9   572800   \n",
       "10  ZHVI    10016  New York    NY  New York  New York         11   917000   \n",
       "17  ZHVI    10128  New York    NY  New York  New York         18  1077000   \n",
       "23  ZHVI    10462  New York    NY  New York     Bronx         24   118200   \n",
       "\n",
       "    2016-09  2016-10  2016-11    2016-12  2017-01  2017-02  2017-03  2017-04  \\\n",
       "0   1137500  1137700  1152700  1156000.0  1140200  1130000  1131900  1149600   \n",
       "8    583600   594800   605200   612100.0   612800   616900   628900   644200   \n",
       "10   934800   946000   949200   950000.0   951800   960100   972200   986900   \n",
       "17  1074900  1076400  1090300  1111300.0  1133100  1154700  1189800  1241000   \n",
       "23   120600   121600   121500   121000.0   121100   123200   126200   128700   \n",
       "\n",
       "    2017-05  2017-06      borough  \n",
       "0   1198400  1247000  [Manhattan]  \n",
       "8    653500   658700   [Brooklyn]  \n",
       "10  1021200  1061200  [Manhattan]  \n",
       "17  1288200  1318300  [Manhattan]  \n",
       "23   131900   135400      [Bronx]  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realcleaned['borough'] = realcleaned['zipcode'].map(zipdict)\n",
    "realcleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>types</th>\n",
       "      <th>ZIPCODE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48476</th>\n",
       "      <td>new york city</td>\n",
       "      <td>5161e3e3db931a8574e44c063f4ed8647375e8a9</td>\n",
       "      <td>40.669737</td>\n",
       "      <td>-73.842051</td>\n",
       "      <td>Boulevard Discount</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['home_goods_store', 'store', 'point_of_intere...</td>\n",
       "      <td>11417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59758</th>\n",
       "      <td>new york city</td>\n",
       "      <td>0bce4e42ae30780b6da1c24edf56a2070a041e7e</td>\n",
       "      <td>40.746251</td>\n",
       "      <td>-73.913639</td>\n",
       "      <td>Discount Variety &amp; Grocery.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['grocery_or_supermarket', 'food', 'store', 'p...</td>\n",
       "      <td>11377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59855</th>\n",
       "      <td>new york city</td>\n",
       "      <td>b113763e103d2d45836b1f6c92b8a4d1d39867fb</td>\n",
       "      <td>40.767347</td>\n",
       "      <td>-73.911814</td>\n",
       "      <td>TANJAWI, Hallal Food Emporium Inc</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['food', 'store', 'point_of_interest', 'establ...</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59847</th>\n",
       "      <td>new york city</td>\n",
       "      <td>5a005af8307213bfcf140bbe3536770010c20679</td>\n",
       "      <td>40.766207</td>\n",
       "      <td>-73.913381</td>\n",
       "      <td>Watany Food Market</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['grocery_or_supermarket', 'food', 'store', 'p...</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59840</th>\n",
       "      <td>new york city</td>\n",
       "      <td>ce4779dd89bb0912874f1f8e7795adb94bc54474</td>\n",
       "      <td>40.766203</td>\n",
       "      <td>-73.913372</td>\n",
       "      <td>Steinway Pharmacy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['pharmacy', 'health', 'store', 'point_of_inte...</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59833</th>\n",
       "      <td>new york city</td>\n",
       "      <td>880649fd667fe93cd574597baafc346eeff51f1f</td>\n",
       "      <td>40.765861</td>\n",
       "      <td>-73.913584</td>\n",
       "      <td>Astoria Family Pharmacy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['pharmacy', 'health', 'store', 'point_of_inte...</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59831</th>\n",
       "      <td>new york city</td>\n",
       "      <td>ad3e713d3707210c8face5f5438c4ac8162ab5d9</td>\n",
       "      <td>40.765253</td>\n",
       "      <td>-73.913657</td>\n",
       "      <td>NXT Sports Nutrition</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['health', 'store', 'point_of_interest', 'esta...</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59823</th>\n",
       "      <td>new york city</td>\n",
       "      <td>fcab3ff847930d63a870199391a9f0a01c5231c1</td>\n",
       "      <td>40.762899</td>\n",
       "      <td>-73.912928</td>\n",
       "      <td>D K M JEWELRY INC</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['jewelry_store', 'store', 'point_of_interest'...</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59814</th>\n",
       "      <td>new york city</td>\n",
       "      <td>53f3280c13005be2e93ab9d17eaf650a26a1c6c0</td>\n",
       "      <td>40.761752</td>\n",
       "      <td>-73.913251</td>\n",
       "      <td>Hip Hop Enterprises Inc</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['store', 'point_of_interest', 'establishment']</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59802</th>\n",
       "      <td>new york city</td>\n",
       "      <td>a6d7d463a5be06f1c3f380d9059cd9449ef2d735</td>\n",
       "      <td>40.762082</td>\n",
       "      <td>-73.910753</td>\n",
       "      <td>Avenue Chemists Pharmacy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['pharmacy', 'health', 'food', 'store', 'point...</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59801</th>\n",
       "      <td>new york city</td>\n",
       "      <td>49778922295499564376067c6c1b1ecd3abf6f1f</td>\n",
       "      <td>40.762072</td>\n",
       "      <td>-73.911451</td>\n",
       "      <td>Sorriso</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['food', 'store', 'point_of_interest', 'establ...</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59776</th>\n",
       "      <td>new york city</td>\n",
       "      <td>4532daa6d2a4a8e7d0d27badbe28cd525e86e736</td>\n",
       "      <td>40.752329</td>\n",
       "      <td>-73.912083</td>\n",
       "      <td>Garden Center at The Home Depot</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['food', 'furniture_store', 'home_goods_store'...</td>\n",
       "      <td>11101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59767</th>\n",
       "      <td>new york city</td>\n",
       "      <td>b5e045fff05d3e3f75df6396ea19fc0d3d807160</td>\n",
       "      <td>40.750076</td>\n",
       "      <td>-73.910896</td>\n",
       "      <td>Paratransit Auto Parts</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['car_repair', 'store', 'point_of_interest', '...</td>\n",
       "      <td>11104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59760</th>\n",
       "      <td>new york city</td>\n",
       "      <td>9db9d9c868e9ea88099010448104ed845647ce48</td>\n",
       "      <td>40.750054</td>\n",
       "      <td>-73.911436</td>\n",
       "      <td>Capitol Glass &amp; Sash Co Inc</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['store', 'point_of_interest', 'establishment']</td>\n",
       "      <td>11104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59759</th>\n",
       "      <td>new york city</td>\n",
       "      <td>eb5dec0a2c0c79d91c5cbaa1581f87dcdff16240</td>\n",
       "      <td>40.746241</td>\n",
       "      <td>-73.913650</td>\n",
       "      <td>Discount Variety &amp; Grocery. ( AATMA ENTERPRISE...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['grocery_or_supermarket', 'book_store', 'food...</td>\n",
       "      <td>11377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59748</th>\n",
       "      <td>new york city</td>\n",
       "      <td>cd190ce8843e931c1d1878eb6ecfa9c9053498ef</td>\n",
       "      <td>40.744752</td>\n",
       "      <td>-73.910873</td>\n",
       "      <td>Raju Restaurant Equipment &amp; Supplies</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['store', 'point_of_interest', 'establishment']</td>\n",
       "      <td>11377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59861</th>\n",
       "      <td>new york city</td>\n",
       "      <td>20f8c78313394413fad17b47cec219cfa5a54d38</td>\n",
       "      <td>40.768934</td>\n",
       "      <td>-73.912868</td>\n",
       "      <td>Uncurtain Shower Doors</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['store', 'point_of_interest', 'establishment']</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59747</th>\n",
       "      <td>new york city</td>\n",
       "      <td>8b00a467541c84ddf76010839c352ff2e943e137</td>\n",
       "      <td>40.744726</td>\n",
       "      <td>-73.911067</td>\n",
       "      <td>MIND &amp; BODY Pharmacy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['pharmacy', 'health', 'store', 'point_of_inte...</td>\n",
       "      <td>11377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59744</th>\n",
       "      <td>new york city</td>\n",
       "      <td>cbdaae3e755b8d84a892bfba1b0b19f4280a6565</td>\n",
       "      <td>40.744474</td>\n",
       "      <td>-73.911798</td>\n",
       "      <td>M J Deli Grocery</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['grocery_or_supermarket', 'food', 'store', 'p...</td>\n",
       "      <td>11377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59734</th>\n",
       "      <td>new york city</td>\n",
       "      <td>dbb0e26954529758765234c943642d5500273548</td>\n",
       "      <td>40.727859</td>\n",
       "      <td>-73.913400</td>\n",
       "      <td>K &amp; L Paper Supply, Inc.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['grocery_or_supermarket', 'food', 'store', 'p...</td>\n",
       "      <td>11378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59733</th>\n",
       "      <td>new york city</td>\n",
       "      <td>a9ef7aa0360bd311ed446e54e9971296a11f246a</td>\n",
       "      <td>40.727859</td>\n",
       "      <td>-73.913400</td>\n",
       "      <td>Rainbow Bag Co</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['store', 'point_of_interest', 'establishment']</td>\n",
       "      <td>11378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59727</th>\n",
       "      <td>new york city</td>\n",
       "      <td>fb6d4a9bc4a7da88b95f7dea115e2782038ae377</td>\n",
       "      <td>40.724362</td>\n",
       "      <td>-73.912770</td>\n",
       "      <td>Five Star Label</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['store', 'point_of_interest', 'establishment']</td>\n",
       "      <td>11378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59720</th>\n",
       "      <td>new york city</td>\n",
       "      <td>381cc27451a896193fccd9e7e247aceaf722abec</td>\n",
       "      <td>40.722724</td>\n",
       "      <td>-73.912649</td>\n",
       "      <td>Delicias Andinas Inc</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['bakery', 'food', 'store', 'point_of_interest...</td>\n",
       "      <td>11378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59719</th>\n",
       "      <td>new york city</td>\n",
       "      <td>9090b459bc1f7944249e96dc6c78d91af26e02b0</td>\n",
       "      <td>40.723926</td>\n",
       "      <td>-73.911267</td>\n",
       "      <td>Quality Installations of New York</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['storage', 'furniture_store', 'home_goods_sto...</td>\n",
       "      <td>11378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59717</th>\n",
       "      <td>new york city</td>\n",
       "      <td>62cbd72e249e8245fa77e61b15a39189138d392b</td>\n",
       "      <td>40.721240</td>\n",
       "      <td>-73.912564</td>\n",
       "      <td>M &amp; A Linens</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['home_goods_store', 'store', 'point_of_intere...</td>\n",
       "      <td>11378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59712</th>\n",
       "      <td>new york city</td>\n",
       "      <td>ed746de69dcdebd3b6b6e995a8b5fd4687894728</td>\n",
       "      <td>40.720612</td>\n",
       "      <td>-73.910622</td>\n",
       "      <td>Capas Headwear Inc</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['clothing_store', 'store', 'point_of_interest...</td>\n",
       "      <td>11378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59705</th>\n",
       "      <td>new york city</td>\n",
       "      <td>64ab4287b7e9af30a34c743ecddbf066d227495e</td>\n",
       "      <td>40.718526</td>\n",
       "      <td>-73.913432</td>\n",
       "      <td>Long Island Pipe Supply</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['store', 'point_of_interest', 'establishment']</td>\n",
       "      <td>11378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59698</th>\n",
       "      <td>new york city</td>\n",
       "      <td>f81c870b5b5b538b724e7ef9eb734cd58d69a4fb</td>\n",
       "      <td>40.717822</td>\n",
       "      <td>-73.911844</td>\n",
       "      <td>On Time Development</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['car_repair', 'store', 'point_of_interest', '...</td>\n",
       "      <td>11378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59693</th>\n",
       "      <td>new york city</td>\n",
       "      <td>63daf00f6282357a77b481d464fa1d8871f34276</td>\n",
       "      <td>40.714802</td>\n",
       "      <td>-73.913587</td>\n",
       "      <td>DMF Electronics Co.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['store', 'point_of_interest', 'establishment']</td>\n",
       "      <td>11378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59686</th>\n",
       "      <td>new york city</td>\n",
       "      <td>4b2b971b3549abe481de8fcbb4c802d78a00a2ff</td>\n",
       "      <td>40.716783</td>\n",
       "      <td>-73.912864</td>\n",
       "      <td>AM PM Supply Corporation</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['plumber', 'store', 'point_of_interest', 'est...</td>\n",
       "      <td>11378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59859</th>\n",
       "      <td>new york city</td>\n",
       "      <td>fdca7aaa1cd3d422b705765d03731d490270033e</td>\n",
       "      <td>40.768851</td>\n",
       "      <td>-73.911826</td>\n",
       "      <td>Modesto Cotrina - Floral Designer</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['florist', 'store', 'point_of_interest', 'est...</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59862</th>\n",
       "      <td>new york city</td>\n",
       "      <td>aff97b4b41650cf159b5db8439137508596d12b5</td>\n",
       "      <td>40.768960</td>\n",
       "      <td>-73.912852</td>\n",
       "      <td>Uncurtain</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['painter', 'general_contractor', 'store', 'po...</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59675</th>\n",
       "      <td>new york city</td>\n",
       "      <td>ad1d5d5450e6358b06aa6519ffd4da1a666d0d8a</td>\n",
       "      <td>40.708565</td>\n",
       "      <td>-73.911502</td>\n",
       "      <td>Zian Farm</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['food', 'store', 'point_of_interest', 'establ...</td>\n",
       "      <td>11385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59955</th>\n",
       "      <td>new york city</td>\n",
       "      <td>5a3d7b13c75132cf23d2bda83637dfd4843fbdd8</td>\n",
       "      <td>40.781829</td>\n",
       "      <td>-73.913479</td>\n",
       "      <td>Fatima Food Mart</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['grocery_or_supermarket', 'food', 'store', 'p...</td>\n",
       "      <td>11105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59997</th>\n",
       "      <td>new york city</td>\n",
       "      <td>23787c637861f4f2a7d86bcf15d681e4bba91140</td>\n",
       "      <td>40.815968</td>\n",
       "      <td>-73.910782</td>\n",
       "      <td>Cauldwell Pharmacy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['pharmacy', 'health', 'store', 'point_of_inte...</td>\n",
       "      <td>10455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59988</th>\n",
       "      <td>new york city</td>\n",
       "      <td>b6149ea8452354dc7d976d1c60ce0693a5f43564</td>\n",
       "      <td>40.805120</td>\n",
       "      <td>-73.913070</td>\n",
       "      <td>Grand Stop Grocery</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['grocery_or_supermarket', 'convenience_store'...</td>\n",
       "      <td>10454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59986</th>\n",
       "      <td>new york city</td>\n",
       "      <td>7ca9e09447d507c644849510a260db5bfdebf831</td>\n",
       "      <td>40.804852</td>\n",
       "      <td>-73.913206</td>\n",
       "      <td>Famous Deli</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['grocery_or_supermarket', 'food', 'store', 'p...</td>\n",
       "      <td>10454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13112</th>\n",
       "      <td>new york city</td>\n",
       "      <td>029ee35963d91418e8bbace248f36117d54e14c1</td>\n",
       "      <td>40.777607</td>\n",
       "      <td>-73.978364</td>\n",
       "      <td>TAP NYC</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "      <td>10025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59981</th>\n",
       "      <td>new york city</td>\n",
       "      <td>a79ebf221a6e39ce27989799de02aadad8d6121a</td>\n",
       "      <td>40.803924</td>\n",
       "      <td>-73.912173</td>\n",
       "      <td>Charles H Beckley Inc</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['furniture_store', 'home_goods_store', 'store...</td>\n",
       "      <td>10454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59973</th>\n",
       "      <td>new york city</td>\n",
       "      <td>8df2b54fc83f10315d4d9b291b7baea619701ba5</td>\n",
       "      <td>40.801788</td>\n",
       "      <td>-73.913808</td>\n",
       "      <td>Empire Safe Co.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['moving_company', 'store', 'point_of_interest...</td>\n",
       "      <td>10454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59971</th>\n",
       "      <td>new york city</td>\n",
       "      <td>60c240366068b4c520a7a8f58f11f59323b3062a</td>\n",
       "      <td>40.802002</td>\n",
       "      <td>-73.913658</td>\n",
       "      <td>Savant Metals LLC</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['general_contractor', 'store', 'point_of_inte...</td>\n",
       "      <td>10454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59970</th>\n",
       "      <td>new york city</td>\n",
       "      <td>f33c27b5645b30afe575b277d538109b42d862bf</td>\n",
       "      <td>40.801991</td>\n",
       "      <td>-73.913304</td>\n",
       "      <td>PLATINUM WIRELESS ACCESSORIES</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['electronics_store', 'store', 'point_of_inter...</td>\n",
       "      <td>10454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59968</th>\n",
       "      <td>new york city</td>\n",
       "      <td>bee3bafa87c05fa6e10d524072cb28f6468f3116</td>\n",
       "      <td>40.802765</td>\n",
       "      <td>-73.912968</td>\n",
       "      <td>GBC UPHOLSTERY</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['furniture_store', 'home_goods_store', 'store...</td>\n",
       "      <td>10454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59967</th>\n",
       "      <td>new york city</td>\n",
       "      <td>e96e013d07d869c22e3aa2e78d3a3a556e2b1f46</td>\n",
       "      <td>40.802789</td>\n",
       "      <td>-73.912387</td>\n",
       "      <td>Hands On Heating Supplies Inc.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['store', 'point_of_interest', 'establishment']</td>\n",
       "      <td>10454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59963</th>\n",
       "      <td>new york city</td>\n",
       "      <td>43fdfeed96a76cd05ac16302bf7560248838ec76</td>\n",
       "      <td>40.802765</td>\n",
       "      <td>-73.912968</td>\n",
       "      <td>US Clothing Company</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['clothing_store', 'store', 'point_of_interest...</td>\n",
       "      <td>10454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59962</th>\n",
       "      <td>new york city</td>\n",
       "      <td>79eec111473face6ceedced0cc517b1c9a3311d3</td>\n",
       "      <td>40.803183</td>\n",
       "      <td>-73.912559</td>\n",
       "      <td>Globus Cork</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['home_goods_store', 'store', 'point_of_intere...</td>\n",
       "      <td>10454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59956</th>\n",
       "      <td>new york city</td>\n",
       "      <td>009b0f4b8c7b78ba9dbfa86815feadcd7505d038</td>\n",
       "      <td>40.798784</td>\n",
       "      <td>-73.910887</td>\n",
       "      <td>Exterminator Westchester Inc</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['home_goods_store', 'store', 'point_of_intere...</td>\n",
       "      <td>10454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59937</th>\n",
       "      <td>new york city</td>\n",
       "      <td>ce5a65e72a15cd5f1b2d00fc6b9f3d7535c0a003</td>\n",
       "      <td>40.774947</td>\n",
       "      <td>-73.913207</td>\n",
       "      <td>Fresh Start</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['store', 'point_of_interest', 'establishment']</td>\n",
       "      <td>11105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59863</th>\n",
       "      <td>new york city</td>\n",
       "      <td>426bcc4f6e4693798675c9609cbf9b0ddd9ad685</td>\n",
       "      <td>40.768960</td>\n",
       "      <td>-73.912852</td>\n",
       "      <td>Optimistic Crafts</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['store', 'point_of_interest', 'establishment']</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59935</th>\n",
       "      <td>new york city</td>\n",
       "      <td>911daf4a9711a89147d78f67a8c4b4eb6685c287</td>\n",
       "      <td>40.775760</td>\n",
       "      <td>-73.910785</td>\n",
       "      <td>Gem Pawnbrokers</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['finance', 'store', 'point_of_interest', 'est...</td>\n",
       "      <td>11105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                city                                        id   latitude  \\\n",
       "48476  new york city  5161e3e3db931a8574e44c063f4ed8647375e8a9  40.669737   \n",
       "59758  new york city  0bce4e42ae30780b6da1c24edf56a2070a041e7e  40.746251   \n",
       "59855  new york city  b113763e103d2d45836b1f6c92b8a4d1d39867fb  40.767347   \n",
       "59847  new york city  5a005af8307213bfcf140bbe3536770010c20679  40.766207   \n",
       "59840  new york city  ce4779dd89bb0912874f1f8e7795adb94bc54474  40.766203   \n",
       "59833  new york city  880649fd667fe93cd574597baafc346eeff51f1f  40.765861   \n",
       "59831  new york city  ad3e713d3707210c8face5f5438c4ac8162ab5d9  40.765253   \n",
       "59823  new york city  fcab3ff847930d63a870199391a9f0a01c5231c1  40.762899   \n",
       "59814  new york city  53f3280c13005be2e93ab9d17eaf650a26a1c6c0  40.761752   \n",
       "59802  new york city  a6d7d463a5be06f1c3f380d9059cd9449ef2d735  40.762082   \n",
       "59801  new york city  49778922295499564376067c6c1b1ecd3abf6f1f  40.762072   \n",
       "59776  new york city  4532daa6d2a4a8e7d0d27badbe28cd525e86e736  40.752329   \n",
       "59767  new york city  b5e045fff05d3e3f75df6396ea19fc0d3d807160  40.750076   \n",
       "59760  new york city  9db9d9c868e9ea88099010448104ed845647ce48  40.750054   \n",
       "59759  new york city  eb5dec0a2c0c79d91c5cbaa1581f87dcdff16240  40.746241   \n",
       "59748  new york city  cd190ce8843e931c1d1878eb6ecfa9c9053498ef  40.744752   \n",
       "59861  new york city  20f8c78313394413fad17b47cec219cfa5a54d38  40.768934   \n",
       "59747  new york city  8b00a467541c84ddf76010839c352ff2e943e137  40.744726   \n",
       "59744  new york city  cbdaae3e755b8d84a892bfba1b0b19f4280a6565  40.744474   \n",
       "59734  new york city  dbb0e26954529758765234c943642d5500273548  40.727859   \n",
       "59733  new york city  a9ef7aa0360bd311ed446e54e9971296a11f246a  40.727859   \n",
       "59727  new york city  fb6d4a9bc4a7da88b95f7dea115e2782038ae377  40.724362   \n",
       "59720  new york city  381cc27451a896193fccd9e7e247aceaf722abec  40.722724   \n",
       "59719  new york city  9090b459bc1f7944249e96dc6c78d91af26e02b0  40.723926   \n",
       "59717  new york city  62cbd72e249e8245fa77e61b15a39189138d392b  40.721240   \n",
       "59712  new york city  ed746de69dcdebd3b6b6e995a8b5fd4687894728  40.720612   \n",
       "59705  new york city  64ab4287b7e9af30a34c743ecddbf066d227495e  40.718526   \n",
       "59698  new york city  f81c870b5b5b538b724e7ef9eb734cd58d69a4fb  40.717822   \n",
       "59693  new york city  63daf00f6282357a77b481d464fa1d8871f34276  40.714802   \n",
       "59686  new york city  4b2b971b3549abe481de8fcbb4c802d78a00a2ff  40.716783   \n",
       "59859  new york city  fdca7aaa1cd3d422b705765d03731d490270033e  40.768851   \n",
       "59862  new york city  aff97b4b41650cf159b5db8439137508596d12b5  40.768960   \n",
       "59675  new york city  ad1d5d5450e6358b06aa6519ffd4da1a666d0d8a  40.708565   \n",
       "59955  new york city  5a3d7b13c75132cf23d2bda83637dfd4843fbdd8  40.781829   \n",
       "59997  new york city  23787c637861f4f2a7d86bcf15d681e4bba91140  40.815968   \n",
       "59988  new york city  b6149ea8452354dc7d976d1c60ce0693a5f43564  40.805120   \n",
       "59986  new york city  7ca9e09447d507c644849510a260db5bfdebf831  40.804852   \n",
       "13112  new york city  029ee35963d91418e8bbace248f36117d54e14c1  40.777607   \n",
       "59981  new york city  a79ebf221a6e39ce27989799de02aadad8d6121a  40.803924   \n",
       "59973  new york city  8df2b54fc83f10315d4d9b291b7baea619701ba5  40.801788   \n",
       "59971  new york city  60c240366068b4c520a7a8f58f11f59323b3062a  40.802002   \n",
       "59970  new york city  f33c27b5645b30afe575b277d538109b42d862bf  40.801991   \n",
       "59968  new york city  bee3bafa87c05fa6e10d524072cb28f6468f3116  40.802765   \n",
       "59967  new york city  e96e013d07d869c22e3aa2e78d3a3a556e2b1f46  40.802789   \n",
       "59963  new york city  43fdfeed96a76cd05ac16302bf7560248838ec76  40.802765   \n",
       "59962  new york city  79eec111473face6ceedced0cc517b1c9a3311d3  40.803183   \n",
       "59956  new york city  009b0f4b8c7b78ba9dbfa86815feadcd7505d038  40.798784   \n",
       "59937  new york city  ce5a65e72a15cd5f1b2d00fc6b9f3d7535c0a003  40.774947   \n",
       "59863  new york city  426bcc4f6e4693798675c9609cbf9b0ddd9ad685  40.768960   \n",
       "59935  new york city  911daf4a9711a89147d78f67a8c4b4eb6685c287  40.775760   \n",
       "\n",
       "       longitude                                               name  rating  \\\n",
       "48476 -73.842051                                 Boulevard Discount     5.0   \n",
       "59758 -73.913639                        Discount Variety & Grocery.     5.0   \n",
       "59855 -73.911814                  TANJAWI, Hallal Food Emporium Inc     5.0   \n",
       "59847 -73.913381                                 Watany Food Market     5.0   \n",
       "59840 -73.913372                                  Steinway Pharmacy     5.0   \n",
       "59833 -73.913584                            Astoria Family Pharmacy     5.0   \n",
       "59831 -73.913657                               NXT Sports Nutrition     5.0   \n",
       "59823 -73.912928                                  D K M JEWELRY INC     5.0   \n",
       "59814 -73.913251                            Hip Hop Enterprises Inc     5.0   \n",
       "59802 -73.910753                           Avenue Chemists Pharmacy     5.0   \n",
       "59801 -73.911451                                            Sorriso     5.0   \n",
       "59776 -73.912083                    Garden Center at The Home Depot     5.0   \n",
       "59767 -73.910896                             Paratransit Auto Parts     5.0   \n",
       "59760 -73.911436                        Capitol Glass & Sash Co Inc     5.0   \n",
       "59759 -73.913650  Discount Variety & Grocery. ( AATMA ENTERPRISE...     5.0   \n",
       "59748 -73.910873               Raju Restaurant Equipment & Supplies     5.0   \n",
       "59861 -73.912868                             Uncurtain Shower Doors     5.0   \n",
       "59747 -73.911067                               MIND & BODY Pharmacy     5.0   \n",
       "59744 -73.911798                                   M J Deli Grocery     5.0   \n",
       "59734 -73.913400                           K & L Paper Supply, Inc.     5.0   \n",
       "59733 -73.913400                                     Rainbow Bag Co     5.0   \n",
       "59727 -73.912770                                    Five Star Label     5.0   \n",
       "59720 -73.912649                               Delicias Andinas Inc     5.0   \n",
       "59719 -73.911267                  Quality Installations of New York     5.0   \n",
       "59717 -73.912564                                       M & A Linens     5.0   \n",
       "59712 -73.910622                                 Capas Headwear Inc     5.0   \n",
       "59705 -73.913432                            Long Island Pipe Supply     5.0   \n",
       "59698 -73.911844                                On Time Development     5.0   \n",
       "59693 -73.913587                                DMF Electronics Co.     5.0   \n",
       "59686 -73.912864                           AM PM Supply Corporation     5.0   \n",
       "59859 -73.911826                  Modesto Cotrina - Floral Designer     5.0   \n",
       "59862 -73.912852                                          Uncurtain     5.0   \n",
       "59675 -73.911502                                          Zian Farm     5.0   \n",
       "59955 -73.913479                                   Fatima Food Mart     5.0   \n",
       "59997 -73.910782                                 Cauldwell Pharmacy     5.0   \n",
       "59988 -73.913070                                 Grand Stop Grocery     5.0   \n",
       "59986 -73.913206                                        Famous Deli     5.0   \n",
       "13112 -73.978364                                            TAP NYC     5.0   \n",
       "59981 -73.912173                              Charles H Beckley Inc     5.0   \n",
       "59973 -73.913808                                    Empire Safe Co.     5.0   \n",
       "59971 -73.913658                                  Savant Metals LLC     5.0   \n",
       "59970 -73.913304                      PLATINUM WIRELESS ACCESSORIES     5.0   \n",
       "59968 -73.912968                                     GBC UPHOLSTERY     5.0   \n",
       "59967 -73.912387                     Hands On Heating Supplies Inc.     5.0   \n",
       "59963 -73.912968                                US Clothing Company     5.0   \n",
       "59962 -73.912559                                        Globus Cork     5.0   \n",
       "59956 -73.910887                       Exterminator Westchester Inc     5.0   \n",
       "59937 -73.913207                                        Fresh Start     5.0   \n",
       "59863 -73.912852                                  Optimistic Crafts     5.0   \n",
       "59935 -73.910785                                    Gem Pawnbrokers     5.0   \n",
       "\n",
       "                                                   types ZIPCODE  \n",
       "48476  ['home_goods_store', 'store', 'point_of_intere...   11417  \n",
       "59758  ['grocery_or_supermarket', 'food', 'store', 'p...   11377  \n",
       "59855  ['food', 'store', 'point_of_interest', 'establ...   11103  \n",
       "59847  ['grocery_or_supermarket', 'food', 'store', 'p...   11103  \n",
       "59840  ['pharmacy', 'health', 'store', 'point_of_inte...   11103  \n",
       "59833  ['pharmacy', 'health', 'store', 'point_of_inte...   11103  \n",
       "59831  ['health', 'store', 'point_of_interest', 'esta...   11103  \n",
       "59823  ['jewelry_store', 'store', 'point_of_interest'...   11103  \n",
       "59814    ['store', 'point_of_interest', 'establishment']   11103  \n",
       "59802  ['pharmacy', 'health', 'food', 'store', 'point...   11103  \n",
       "59801  ['food', 'store', 'point_of_interest', 'establ...   11103  \n",
       "59776  ['food', 'furniture_store', 'home_goods_store'...   11101  \n",
       "59767  ['car_repair', 'store', 'point_of_interest', '...   11104  \n",
       "59760    ['store', 'point_of_interest', 'establishment']   11104  \n",
       "59759  ['grocery_or_supermarket', 'book_store', 'food...   11377  \n",
       "59748    ['store', 'point_of_interest', 'establishment']   11377  \n",
       "59861    ['store', 'point_of_interest', 'establishment']   11103  \n",
       "59747  ['pharmacy', 'health', 'store', 'point_of_inte...   11377  \n",
       "59744  ['grocery_or_supermarket', 'food', 'store', 'p...   11377  \n",
       "59734  ['grocery_or_supermarket', 'food', 'store', 'p...   11378  \n",
       "59733    ['store', 'point_of_interest', 'establishment']   11378  \n",
       "59727    ['store', 'point_of_interest', 'establishment']   11378  \n",
       "59720  ['bakery', 'food', 'store', 'point_of_interest...   11378  \n",
       "59719  ['storage', 'furniture_store', 'home_goods_sto...   11378  \n",
       "59717  ['home_goods_store', 'store', 'point_of_intere...   11378  \n",
       "59712  ['clothing_store', 'store', 'point_of_interest...   11378  \n",
       "59705    ['store', 'point_of_interest', 'establishment']   11378  \n",
       "59698  ['car_repair', 'store', 'point_of_interest', '...   11378  \n",
       "59693    ['store', 'point_of_interest', 'establishment']   11378  \n",
       "59686  ['plumber', 'store', 'point_of_interest', 'est...   11378  \n",
       "59859  ['florist', 'store', 'point_of_interest', 'est...   11103  \n",
       "59862  ['painter', 'general_contractor', 'store', 'po...   11103  \n",
       "59675  ['food', 'store', 'point_of_interest', 'establ...   11385  \n",
       "59955  ['grocery_or_supermarket', 'food', 'store', 'p...   11105  \n",
       "59997  ['pharmacy', 'health', 'store', 'point_of_inte...   10455  \n",
       "59988  ['grocery_or_supermarket', 'convenience_store'...   10454  \n",
       "59986  ['grocery_or_supermarket', 'food', 'store', 'p...   10454  \n",
       "13112  ['restaurant', 'food', 'point_of_interest', 'e...   10025  \n",
       "59981  ['furniture_store', 'home_goods_store', 'store...   10454  \n",
       "59973  ['moving_company', 'store', 'point_of_interest...   10454  \n",
       "59971  ['general_contractor', 'store', 'point_of_inte...   10454  \n",
       "59970  ['electronics_store', 'store', 'point_of_inter...   10454  \n",
       "59968  ['furniture_store', 'home_goods_store', 'store...   10454  \n",
       "59967    ['store', 'point_of_interest', 'establishment']   10454  \n",
       "59963  ['clothing_store', 'store', 'point_of_interest...   10454  \n",
       "59962  ['home_goods_store', 'store', 'point_of_intere...   10454  \n",
       "59956  ['home_goods_store', 'store', 'point_of_intere...   10454  \n",
       "59937    ['store', 'point_of_interest', 'establishment']   11105  \n",
       "59863    ['store', 'point_of_interest', 'establishment']   11103  \n",
       "59935  ['finance', 'store', 'point_of_interest', 'est...   11105  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.point import Point\n",
    "import geopy\n",
    "\n",
    "def get_zip_code(x):\n",
    "    geolocator = geopy.Nominatim(user_agent=\"check_1\")\n",
    "    location = geolocator.reverse(\"{}, {}\".format(x['latitude'],x['longitude']))\n",
    "    return location.raw['address']['postcode']\n",
    "venu = venuescl.sort_values(by=['rating'], ascending=False)\n",
    "venu = venu.head(50)\n",
    "venu['ZIPCODE'] = venu.apply(lambda x: get_zip_code(x), axis = 1)\n",
    "venu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: (5 min)\n",
    "\n",
    "Conduct an exploratory analysis of the sizes of reviews: find the shortest and longest reviews, then plot a histogram showing the distribution of review lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text visualization with word clouds (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like visualization is crucial for standard CSV data, it is also important for text data. But text doesn't lend itself to histograms or scatterplots the way that numerical or even categorical data do. In such cases, **word clouds** are a common and <font color=\"orange\">sometimes</font> useful tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** While wordclouds can be a useful way of quickly gaining high level insights into raw textual data, they are also limited. In some ways, they can be seen as the pie charts of NLP: often used, but also often hated. [Some](https://www.niemanlab.org/2011/10/word-clouds-considered-harmful/) [people](https://towardsdatascience.com/can-we-please-stop-using-word-clouds-eca2bbda7b9d) would prefer if they didn't exist at all. If used in the correct way, however, they definitely deserve their place in a data scientist's toolbelt.\n",
    "\n",
    "The main problem with word clouds is that they are difficult to interpret in a standard way. The layout algorithm has some randomness involved and although more common words are shown more prominently, it's not possible to look at a word cloud and know which words are the most important, or how much more important these are than other words. Colours and rotation are also used randomly, making some words (e.g the ones in bright colours, positioned closer to the centre, with horizontal rotation) seem more important when in fact they are no more important than other words which were randomly assigned a less noticeable combination of color, rotation, and position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: (7 min)\n",
    "\n",
    "Write a function `word_cloud_rating(data, star_value)` that constructs a word cloud from the subset of `data` that exhibit a certain `star_value`. Visualize the results of this function for 1-star reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the resolution for better clarity \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 30, 60\n",
    "\n",
    "def word_cloud_rating(data,star_value):\n",
    "    \n",
    "    data_filtered = data[data.stars == star_value] #filtering according to the star value\n",
    "    Reviews = data_filtered.text\n",
    "\n",
    "    Reviews_text = ' '.join(Reviews.values) #joining all the words together\n",
    "\n",
    "    # Creating a word cloud object\n",
    "    wordcloud = WordCloud(max_font_size=100, max_words=100, background_color=\"white\",\\\n",
    "                          scale = 10,width=800, height=400).generate(Reviews_text)\n",
    "\n",
    "\n",
    "    # Plotting the generated word cloud\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m word_cloud_rating(\u001b[43mdata\u001b[49m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "word_cloud_rating(data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: (5 min)\n",
    "\n",
    "The word \"good\" seems to appear quite frequently in the negative reviews. Investigate why that is and come up with a reasonable explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Answer.** Let's look at the first 5 reviews or so with 1-star ratings to see if there are any discernible patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_containing_good = [each for each in data[data.stars == 1].text if 'good' in each]\n",
    "for review in reviews_containing_good[:20]:\n",
    "    good_index = review.find(\"good\")\n",
    "    print(review[good_index-20:good_index+20].replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading each of the reviews, it is clear that \"good\" is often mentioned in a context like \"no good place to sit\" or \"sound good\". This indicates that in the world of text we cannot go by single words (also called **1-grams**) alone. The context of the sentence or the surrounding words at least are very much necessary to understand the sentiment of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams (25 min)\n",
    "\n",
    "Since 1-grams are insufficient to understand the significance of certain words in our text, it is natural to consider blocks of words, or **n-grams**.\n",
    "\n",
    "The simplest version of the n-gram model, for $n > 1$, is the **bigram** model, which looks at pairs of consecutive words. For example, the sentence \"The quick brown fox jumps over the lazy dog\" would have tokens \"the quick\", \"quick brown\",..., \"lazy dog\". The following image explains this concept:\n",
    "\n",
    "<img src=\"ngrams.png\" alt=\"ngrams\" width=\"500\"/>\n",
    "\n",
    "This has obvious advantages and disadvantages over looking at words individually:\n",
    "\n",
    "1. This retains the structure of the overall document, and\n",
    "2. It paves the way for analyzing words in contex; however,\n",
    "3. The dimension is vastly larger\n",
    "\n",
    "In practice, this last challenge can be truly daunting. As an example, *War and Peace* has 3 million characters, which translates to several hundred thousand 1-grams (words). If you consider that the set of all possible bigrams can be as large as the square of the number of 1-grams, this gets us to a hundred billion possible bigrams! If classical ML techniques are not suitable for training on 3 million characters, how can they possibly deal with a hundred billion dimensions?\n",
    "\n",
    "For this reason, it is often prudent to start by extracting as much value out of 1-grams as possible, before working our way up to more complex structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we also start to look again at our main application: calculating some \"interesting\" features of our corpus of reviews.\n",
    "\n",
    "When thinking about word analysis, the main topic of interest is finding an *efficient* and *low-dimensional* representation in order to facilitate document visualization and larger-scale analyses. We discuss two broad categories of word representations:\n",
    "\n",
    "1. `Count-based representations`: word-word and word-document matrices.\n",
    "2. `Word embeddings`: spectral embedding, UMAP, word2vec, GloVe, and many many more.\n",
    "\n",
    "These are often used to assist with downstream tasks such as clustering, ranking and labeling, which will be briefly discussed in a future case. Word embeddings in particular have become something of a posterchild. These, combined with neural networks (which will also be discussed in a future case!), have led to many of the recent headline improvements in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count-based representations (20 min)\n",
    "\n",
    "n-grams fall under a broader category of techniques otherwise known as [**count-based representations**](https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage). These are techniques to analyze documents by indicating how frequently certain types of structures occur throughout.\n",
    "\n",
    "Let's start with 1-grams (words). The simplest type of information would be whether a particular word occurs in particular documents. This leads to **word-document co-occurrence matrices**, where the $(W, X)$ entry of the word-document matrix is set to 1 if word $W$ occurs in document $X$, and 0 otherwise.\n",
    "\n",
    "There are many variants of this. In lieu of the fact that we are looking for count-based representations of our documents, one natural variable is the following: the $(W, X)$ entry of the word-document matrix equals the number of times that word $W$ occurs in document $X$, rather than merely being a binary variable.\n",
    "\n",
    "Let's create a word-document co-occurrence matrix for our set of reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code creates a word-document matrix.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(AllReviews)\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: (5 min)\n",
    "\n",
    "Find all the high-frequency (top 1%) and low-frequency (bottom 1%) words in the reviews overall. (Hint: import the `Counter()` function from the `collections` class.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_reviews_text = ' '.join(AllReviews)\n",
    "tokenized_words = nltk.word_tokenize(all_reviews_text)\n",
    "word_freq = Counter(tokenized_words)\n",
    "\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Therefore, top 1% is ~463 words\n",
    "word_freq.most_common(463)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similarly, bottom 1% is ~463 words\n",
    "word_freq.most_common()[-463:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for bigrams. Here is the code to get the set of bigrams for the first 5 reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "first_5_revs = data.text[0:5]\n",
    "word_tokens = nltk.word_tokenize(''.join(first_5_revs))\n",
    "list(ngrams(word_tokens, 2)) #ngrams(word_tokens,n) gives the n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: (12 min)\n",
    "\n",
    "Write a function called `top_k_ngrams(word_tokens, n, k)` for printing out the top $k$ n-grams. Use this function to get the top 10 1-grams, 2-grams, and 3-grams from the first 1000 reviews in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def top_k_ngrams(word_tokens,n,k):\n",
    "    \n",
    "    ## Getting them as n-grams\n",
    "    n_gram_list = list(ngrams(word_tokens, n))\n",
    "\n",
    "    ### Getting each n-gram as a separate string\n",
    "    n_gram_strings = [' '.join(each) for each in n_gram_list]\n",
    "    \n",
    "    n_gram_counter = Counter(n_gram_strings)\n",
    "    most_common_k = n_gram_counter.most_common(k)\n",
    "    print(most_common_k)\n",
    "    \n",
    "   # x_pos = [k for k,v in most_common_k]\n",
    "   # y_pos = [v for k,v in most_common_k]\n",
    "    \n",
    "   # plt.bar(x_pos, y_pos,align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting a single string\n",
    "top_1000_reviews = data.text[0:1000]\n",
    "all_reviews_text = ' '.join(top_1000_reviews)\n",
    "\n",
    "## Splitting them into tokens\n",
    "word_tokens = nltk.word_tokenize(all_reviews_text)\n",
    "\n",
    "## Calling the function for top k\n",
    "top_k_ngrams(word_tokens, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_ngrams(word_tokens, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_ngrams(word_tokens, 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words (20 min)\n",
    "\n",
    "You may have noticed a pattern in the types of words that show up in the top 10 1-grams, 2-grams, and 3-grams. In particular, these are common words that appear in every sentence of the English language: pronoums like \"I\", prepositions like \"but\", \"of\", \"and\", articles like \"the\", etc. These very common words are usually uninformative, and their very large occurrence values can distort the results of many NLP algorithms.\n",
    "\n",
    "For this reason, it is common to pre-process text by removing words that you have a reason to believe are uninformative; these words are called [**stop words**](https://en.wikipedia.org/wiki/Stop_words). Usually, it suffices to simply treat extremely common words as stop words. However, for specific types of applications it might make sense to use other stop words; e.g. the word \"burger\" when analyzing reviews of burger chains.\n",
    "\n",
    "(Note that stop words are often removed by default as a cleaning step in all NLP tasks. However, sometimes they can be useful. For example in authorship attribution (automatically detecting who wrote a specific piece of text by their 'writing style'), stop words can be one of the most useful features, as they appear in nearly all texts, and yet each author uses them in slightly different ways.)\n",
    "\n",
    "The `nltk` library has a standard list of stopwords, which you can download by writing `nltk.download(“stopwords”)`. We can then load the stopwords package from the nltk.corpus and use it to load the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words(\"japanese\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a list of all the Spanish stop words as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words(\"spanish\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: (15 min)\n",
    "\n",
    "#### 7.1\n",
    "\n",
    "Filter out all of the stop words in the first review of the Yelp review data and print out your answer. Additionally, print out (separately) the stopwords you found in this review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "without_stop_words = []\n",
    "stopword = []\n",
    "sentence = data.text[0]\n",
    "words = nltk.word_tokenize(sentence)\n",
    "for word in words:\n",
    "    if word in stop_words:\n",
    "        stopword.append(word)\n",
    "    else:\n",
    "        without_stop_words.append(word)\n",
    "\n",
    "print(stopword)\n",
    "print()\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2\n",
    "\n",
    "Modify the function `top_k_ngrams(word_tokens, n, k)` to remove stop words before determining the top n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** Our recommended solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the most basic stop words from the ntlk corpus and including only those\n",
    "# words with character size above 2 so as to remove punctuations\n",
    "# But, this could be extended to remove further high and low frequency stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "### Getting a single string\n",
    "all_reviews_text = ' '.join(top_1000_reviews)\n",
    "\n",
    "## Splitting them into tokens\n",
    "word_tokens = nltk.word_tokenize(all_reviews_text)\n",
    "\n",
    "## Removing the stopwords\n",
    "word_tokens_clean = [each for each in word_tokens if each.lower() not in eng_stopwords and len(each.lower()) > 2]\n",
    "\n",
    "## Calling the function for top k\n",
    "top_k_ngrams(word_tokens_clean, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_ngrams(word_tokens_clean, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some contexts, it is common to remove both very common and very *uncommon* words. The idea is that common words like \"a\" are almost never informative, while uncommon words like \"syzygy\" occur so infrequently in a corpus that many algorithms have a hard time processing them in a meaningful way. We will not deal with uncommon words today, but you should be aware that doing so improves the performance of several NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding important words (30 min)\n",
    "\n",
    "Up to this point, we have focused on techniques for transforming our data. We are now ready to start looking for some answers, so let's take a break from discussing techniques so we can explore our dataset and various ways to summarize it.\n",
    "\n",
    "We begin by looking at the words and n-grams that are most common in positive and negative reviews. Note that in the following code, we don't reuse many of the pre-processing steps discussed at the start of the tutorial. This is because many of them are included as options in existing packages. In a serious project one would often customize this pre-processing to some degree, but we skip this in order to get some displayable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following code grabbed from:\n",
    "# https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n",
    "# we will use it in our context to create some visualizations.\n",
    "def get_top_n_words(corpus, n=1,k=1):\n",
    "    vec = CountVectorizer(ngram_range=(k,k),stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by getting a list of the most common words.\n",
    "\n",
    "common_words = get_top_n_words(data['text'], 20,1)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar',  title='Top 20 words from all reviews')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: (15 min)\n",
    "\n",
    "#### 8.1\n",
    "\n",
    "Divide the data into \"good reviews\" (i.e. `stars` rating was greater than 3) and \"bad reviews\" (i.e. `stars` rating was less than 3) and make a bar plot of the top 20 words in each case. Are these results different from above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We continue by splitting according to good/bad review scores, then grabbing again.\n",
    "\n",
    "GoodInd = data['stars'] >3.1\n",
    "GoodRev = data[GoodInd]\n",
    "BadInd = data['stars'] <2.1\n",
    "BadRev = data[BadInd]\n",
    "\n",
    "common_words = get_top_n_words(GoodRev['text'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 words from good reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(BadRev['text'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 words from bad reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was pretty useless. The \"good\" words are mostly a mix of generic words like \"place\" and overtly positive words like \"good\" itself.\n",
    "\n",
    "The problem here is that we are dealing with single words, which cannot convey much information out of context. The natural solution then is to deal with n-grams, so that we can get context-aware results like \"good burger\" or \"good service\" (in the positive reviews) or, as we saw, \"good 45 minutes\" (in the negative reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2\n",
    "\n",
    "Use the `get_top_n_words()` function to find the top 20 bigrams and trigrams. Do the results seem useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top bigrams and trigrams from bad reviews\n",
    "common_words = get_top_n_words(BadRev['text'], 20,2)\n",
    "\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 bigrams from bad reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top bigrams and trigrams from bad reviews\n",
    "common_words = get_top_n_words(GoodRev['text'], 20,2)\n",
    "\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 bigrams from bad reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(BadRev['text'], 20,3)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 trigrams from bad reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(GoodRev['text'], 20,3)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 bigrams from good reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(GoodRev['text'], 20,3)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 trigrams from good reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are some nonsense entries such as \"http www yelp\" this is starting to be helpful. We can see a few recurring themes among good reviews (e.g. \"staff friendly helpful\"). Nonsense entries are typically difficult to eliminate completely in NLP with user-generated text and smaller corpora. NLP is still useful *despite* the existence of nonsense results, and we should think of the output of NLP algorithms like this as a *screening tool* for finding important phrases rather than a *careful estimate* of the most important phrases. In other words, it's an application of machine intelligence to *conduct exploratory analysis* rather than to *build predictive models*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "Look at the 5 most important bigrams for bad reviews. What *single, specific* problem seems to be the most important driver of bad reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three of the top 5 bigrams were \"20 minutes\", \"15 minutes\", and \"10 minutes.\" These are all times, *strongly* suggesting that *waiting time for service* is a main driver for bad review scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: (10 min)\n",
    "\n",
    "#### 9.1\n",
    "\n",
    "You may have noticed that many of the important \"bad\" bigrams included the words \"like\" or \"just\" but didn't seem very informative (e.g. \"felt like\", \"food just\"). Give some ideas of how to use this sort of observation in future pre-processing of reviews, based on the pre-processing ideas we have already studied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** Two potential answers are (there are many others):\n",
    "\n",
    "1. Having recognized that these words go together in common bigrams, you could modify your algorithm so that it \"clumps\" these bigrams together; i.e. treats them as one word, so that your algorithm will focus on the words following that.\n",
    "2. Having recognized this as a key phrase, we could have a list of the most important words that follow these key phrases (which are presumably informative). This is more time-consuming as it requires human input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2\n",
    "\n",
    "Building on the previous question, we note that most of the most important complaints and compliments can't be *completely* observed by looking at bigrams or trigrams. This can often be fixed by small modifications. Do the following:\n",
    "\n",
    "1. Write down a complaint that is unlikely to be (completely) picked up by bigram analysis. Hint: what might you write if your hamburger was served cold?\n",
    "2. Write down a processing step that would fix this problem. Try to find a solution that would work for several similar problems without additional human input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** There are many good answers, but we focus on a simple one:\n",
    "\n",
    "1. I would probably write \"the burger was cold\" or \"the burger was served cold.\" If this were a common complaint, the most-important bigrams might include \"was cold\" or \"served cold,\" which doesn't tell me *what* was served cold.\n",
    "2. A simple fix, along the lines of the previous question, would be to \"clump\" words like \"was cold\" together. However, I think this is a bad fix, as it focuses too much on the word \"cold\" and would require a great deal of hand tuning. A better idea would be to recognize that words like \"was\" are always going to be a problem in this context, as they separate the important noun describing the subject (\"burger\") from the adjective describing the problem (\"cold\"). This suggests that we should take a *much* more aggressive stance towards removing stop words. This should certainly include conjugations of \"to be,\" and likely many other common but uninformative words (like \"too\").\n",
    "\n",
    "This second response is an important takeaway for NLP – this sort of problem is extremely common, and a great deal of time is often spent tweaking initial pre-processing rules. In the final part of this case, you will learn about a method that can help systematically deal with these uninformative stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions (25 min)\n",
    "\n",
    "Having spent a lot of time on n-grams and how to featurize a document using them, we now take a break from `nltk` tools to introduce the most important text wrangling tool in Python (and many other languages): [**regular expressions**](https://en.wikipedia.org/wiki/Regular_expression).\n",
    "\n",
    "The basic idea here is that you often want to perform some specific transformation (e.g. delete or substitute) every time that some possibly-complicated pattern (e.g. the letter 'A', the word 'hello', any word containing the letters 'a','r' in that order) occurs. Regular expressions are a compact and powerful language for expressing these sorts of patterns. This is super important whenever you are trying to clean a text dataset that contains thematically similar, but not exactly, the same errors. \n",
    "\n",
    "The terse syntax of regular expressions has led to them having a reputation for being [almost magical](https://xkcd.com/208/) in some situations (with only a few characters, you can build complete computer programs) but also for being difficult to create and read, which can [create more problems](https://xkcd.com/1171/) than they solve.\n",
    "\n",
    "In Python, [the `re` module](https://docs.python.org/3/library/re.html) provides regular expression matching operations and common operations. Regular expressions are a deep subject, with some documentation here: https://docs.python.org/3/library/re.html?highlight=regex.\n",
    "\n",
    "As some simple examples, we have:\n",
    "\n",
    "1. `.` matches any character except \\n (newline)\n",
    "2. `\\d` matches any digit (this can also be written as [0-9])\n",
    "3. `\\D` matches any non-digit (this can also be written as [^0-9])\n",
    "4. `\\w` matches any alphanumeric character ([a-zA-Z0-9_])\n",
    "5. `\\W` matches any non-alphanumeric character ([^a-zA-Z0-9_])\n",
    "\n",
    "As some more complex examples, regular expressions also allow you to quantify the number of times matches can occur. For example,\n",
    "\n",
    "1. `[a-d]+` matches any time you get $\\{a,b,c,d\\}$ one or more times in a row\n",
    "2. `[a-d]{3}` matches any time you get them exactly 3 times in a row\n",
    "3. `[a-d]*` matches any time you get them 0 or more times in a row\n",
    "\n",
    "For now, we give a simple application based on the  `re.sub()` function, which substitutes words that match a pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentence = 'That was an \"interesting\" way to cook bread.'\n",
    "pattern = r\"[^\\w]\" # the ^ character denotes 'not', \n",
    "#                   the \\w character denotes a word, and []  means\n",
    "#                    anything that matches anything in the brackets. \n",
    "#                     Together, this refers to any character that is not a word.\n",
    "print(re.sub(pattern, \" \", sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"Natesh loves all the foold and loveds sdaslo\"\n",
    "x   = re.compile('lo')\n",
    "iterator = x.finditer(str)\n",
    "for item in iterator:\n",
    "    print(item.span())\n",
    "    print(item.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: (15 min)\n",
    "\n",
    "#### 10.1\n",
    "\n",
    "1. Use the `re.split()` function to split the first Yelp review into a list of its constituent words.\n",
    "2. Use the `re.findall()` function to search the first 30 reviews for the number of times they contain the word \"food\". Print the maximum number of times the word \"food\" is mentioned in a single review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r'\\s', AllReviews.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_count = []\n",
    "for sentence in AllReviews.values:\n",
    "    temp = len(re.findall('food', sentence))\n",
    "    food_count.append(temp)\n",
    "print(max(food_count[0:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2\n",
    "\n",
    "Using regular expressions, find the percentage of reviews in top 500 reviews that have numbers in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Considering the top 500 reviews for this analysis\n",
    "top_500_reviews = AllReviews.values[:500]\n",
    "reviews_nos_regex = []\n",
    "\n",
    "for each_review in top_500_reviews:\n",
    "    number_list = re.findall('\\d',each_review)\n",
    "    \n",
    "    ## number list returns all the possible digits in a review\n",
    "    ## Look if the number list is empty - if so, the review has no digits in them\n",
    "    if(len(number_list)) > 0:\n",
    "        reviews_nos_regex.append(each_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews_nos_regex)/len(top_500_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is clear from above, regular expressions are very useful for extracting more general properties of text. These properties are not as informative or context-aware as n-grams can be, but they are much simpler to code and therefore can often serve as the first step of an EDA on text data.\n",
    "\n",
    "Although regular expressions usually cannot tell us much about context overall, they *can* be used to find specific instances of words in context. For example, we may be interested in finding the first word following \"good\" or \"bad\" in a review (which can help us distinguish a positive from a negative review). Let's write some code that finds the first word following \"good\" in the sentence \"hello I want a good burger, please.\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"hello I want a good burger, please\"\n",
    "\n",
    "# Find everything after \"good\", including \"good\"\n",
    "\n",
    "post = re.findall(r'good.*', sample)[0]\n",
    "\n",
    "print(post)\n",
    "\n",
    "# Take just the first word after \"good\"\n",
    "\n",
    "first_post = re.split(r'\\s',post)[1]\n",
    "\n",
    "print(first_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.3\n",
    "\n",
    "Using the above as a template, write a generalized function that can extract the first word following \"good\". Don't forget to include a default behavior for when the word doesn't appear in the sentence. Run this function for all reviews and print the first 300 results for reviews that do contain the word \"good\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(sentence):\n",
    "    post = re.findall(r'good.*', sentence)\n",
    "    if (len(post) > 0):\n",
    "        temp = re.split(r'\\s',post[0])\n",
    "        if (len(temp) > 1):\n",
    "            return(temp[1])\n",
    "        else:\n",
    "            return('')\n",
    "    else:\n",
    "        return('')\n",
    "    \n",
    "    \n",
    "print(next_word(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_good = []\n",
    "ind = 0\n",
    "for sentence in AllReviews.values:\n",
    "    temp = next_word(sentence)\n",
    "    post_good.append(temp)\n",
    "    \n",
    "nonempty = [i for i in post_good if i] \n",
    "print(nonempty[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skimming over the results of the previous exercise, a few things stood out:\n",
    "\n",
    "1. People like to talk about good burgers – this appeared 5 times in the first 300 results.\n",
    "2. A large number of the results are useless. One common problem is the occurrence of a sentence boundary; e.g. \"good. The\" near the start of the list. In this case, we should look *before* the word \"good\" rather than after. However, we can't look *immediately* before good – that word will usually be some conjugation of \"to be\", which is also not informative – rather, we need to look for a word before \"good\" that isn't too boring. Other times, there is a word following \"good\" that is uninformative; e.g. \"good for\" – we want to know *what* something was good for! In this case, we should keep on skimming *forward* until we see a word following \"good\" that isn't too boring.\n",
    "\n",
    "In both of these cases, we can't use simple regular expressions by themselves, as regular expressions don't know how to ignore \"boring\" words. Regular expressions can only help us filter for the structure of words, not the content they convey within a context. We *can* use what we learned about stop words to remove these from the reviews before conducting the above analysis, but as we have been, we will still sometimes get not very informative phrases like \"was cold\" or \"served cold\". So we will introduce an alternative method, which can be applied to serve as an even better remover of stop words: **part-of-speech tagging**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part-of-speech (POS) tagging (45 min)\n",
    "\n",
    "In English, there are eight main parts of speech - `nouns`, `pronouns`, `adjectives`, `verbs`, `adverbs`, `prepositions`, `conjunctions` and `interjections`. These are\n",
    "`sustantivos`, `pronombres`, `adjetivos`, `verbos`, `adverbios`, `preposiciones`, `conjunciones` and `interjecciones`, respectively, in Spanish. The purpose of POS tagging is to label each word in a document with its part of speech.\n",
    "\n",
    "Unsurprisingly, [POS tagging](http://www.nltk.org/book/ch05.html) can be very difficult to do by hand. `nltk` has a default function for this, called `nltk.pos_tag()`, which we will use. As a word of warning, this function is far from infallible, especially on informal text (e.g. website reviews, forum posts, text messages, etc), and words in English often exhibit POS drift (e.g. the drift of \"Google\" from noun to verb): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#https://www.nltk.org/book/ch05.html\n",
    "text_word_token = nltk.word_tokenize(\"Jairo is having a good day\")\n",
    "#text_word_token = nltk.word_tokenize(data.text[0])\n",
    "nltk.pos_tag(text_word_token)\n",
    "#https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I prefer to buy burgers\n",
    "prefer -> burger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_word_token = nltk.word_tokenize(\"We are going to Race\") # try \"Race can be both a verb and a noun\"\n",
    "#text_word_token = nltk.word_tokenize(data.text[0])\n",
    "nltk.pos_tag(text_word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.nltk.org/_modules/nltk/tag/perceptron.html\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides documentation for each tag, which can be queried using the tag itself; e.g. `nltk.help.upenn_tagset('RB')`. Since POS is context-sensitive, POS-taggers must usually be trained on an existing corpus that has been tagged by professional linguists (possibly alongside unlabeled data to take advantage of semi-supervised methods). The most popular tag set is called the Penn Treebank set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get more details about any POS tag using the help function of nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('CD$')\n",
    "nltk.help.upenn_tagset('NN$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11: (10 min)\n",
    "\n",
    "#### 11.1\n",
    "\n",
    "Write code to find the percentage of reviews in the first 500 reviews of the dataset that contains a number or a cardinal using POS taggings only. (Hint: POS tag `CD` is the indicator for cardinal or number.) How does this compare to the figure we extracted from using regular expressions only?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinal_review = []\n",
    "\n",
    "for sentence in top_500_reviews:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    cd_len = [k for k,v in nltk.pos_tag(words) if 'CD' == v]\n",
    "    \n",
    "    if len(cd_len) > 0:\n",
    "        cardinal_review.append(sentence)\n",
    "\n",
    "#### Proportion of reviews with a number/cardinal in it        \n",
    "len(cardinal_review)/len(top_500_reviews) \n",
    "## 56.6% of the reviews have a number/cardinal in the top 500 reviews. \n",
    "## You could improve the accuracy of this estimate by looking at more than 500 reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number is considerably higher than the one we got from using regular expressions only! The reason is because POS tagging can extract numbers in text form (e.g. \"one\", \"two\") whereas regular expressions cannot. This is one advantage of using POS tagging over regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2\n",
    "\n",
    "Extract all of the nouns from each review using POS tagging. This may be useful for later analysis. Even though words like \"good\" may be the most prevalent in good reviews, we think nouns like \"service\" or \"burgers\" are likely to be more informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_reviews = []\n",
    "for sentence in AllReviews.values:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    noun_pt = [k for k,v in nltk.pos_tag(words) if 'NN' == v]\n",
    "    noun_reviews.append(noun_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12: (15 min)\n",
    "\n",
    "Use POS tagging to find the first word following \"good\" that has an interesting POS tag. We leave this up to your discretion, but should probably include nouns and proper nouns. Inspecting the above, we think that cardinals are also almost certainly interesting: we recognize that \"good 45\" is probably followed by \"minutes\", definitely an important (though not \"good\") part of a review!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, a function that extracts only the \"interesting\" parts of speech\n",
    "\n",
    "sentence = \"This is a good burger, but I prefer pizza\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "interesting = [k for k,v in nltk.pos_tag(words) if v in ['CD','FW','NN','NNS','NNP','NNPS']]\n",
    "\n",
    "print(interesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_pos = ['CD','FW','NN','NNS','NNP','NNPS']\n",
    "\n",
    "def ExtractInteresting(sentence, good):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    interesting = [k for k,v in nltk.pos_tag(words) if v in good]\n",
    "    return(interesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, a function that extracts the first \"interesting\" word that follows \"good\"\n",
    "\n",
    "sentence = \"This is a good burger, but I prefer pizza\"\n",
    "post = re.findall(r'good.*', sentence)\n",
    "print(post)\n",
    "# Check that post isn't empty here before doing next line\n",
    "temp = ExtractInteresting(post[0],good_pos)\n",
    "print(temp)\n",
    "# Check that temp isn't empty here before doing next line\n",
    "print(post[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word2(sentence):\n",
    "    post = re.findall(r'good.*', sentence)\n",
    "    if (len(post) > 0):\n",
    "        temp = ExtractInteresting(post[0],good_pos)\n",
    "        if (len(temp) > 0):\n",
    "            return(temp[0])\n",
    "        else:\n",
    "            return('')\n",
    "    else:\n",
    "        return('')\n",
    "    \n",
    "    \n",
    "print(next_word2(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, apply this.\n",
    "\n",
    "post_good = []\n",
    "ind = 0\n",
    "for sentence in AllReviews.values:\n",
    "    temp = next_word2(sentence)\n",
    "    post_good.append(temp)\n",
    "    \n",
    "nonempty = [i for i in post_good if i] \n",
    "print(nonempty[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a much more interesting list!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13: (10 min)\n",
    "\n",
    "It is interesting to look specifically at Adjectives (which have a tag name of \"JJ\" in NLTK) when looking at reviews. We can hypothesise that good reviews and bad reviews might use very different adjectives, but that some adjectives might appear often in both good and bad reviews, as we saw with the word \"good\" previously.\n",
    "\n",
    "Use POS tags to extract all adjectives from the first 500 five star reviews and the first 500 one star reviews. Extract the most 30 most commonly used adjectives from each set of reviews and print out both. Make a note of several of these; say if they appear in one or both lists, and whether or not this was expected, and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_star_reviews = data[data['stars']==1]['text'][:500]\n",
    "five_star_reviews = data[data['stars']==5]['text'][:500]\n",
    "\n",
    "\n",
    "def extract_specific_pos(reviews, pos_tag):\n",
    "    results = [] \n",
    "    for review in reviews:\n",
    "        words = nltk.word_tokenize(review)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        keep = [x[0] for x in tagged if x[1] == pos_tag]\n",
    "        results += keep\n",
    "    return results\n",
    "\n",
    "\n",
    "negative_adjectives = extract_specific_pos(one_star_reviews, \"JJ\")\n",
    "positive_adjectives = extract_specific_pos(five_star_reviews, \"JJ\")\n",
    "\n",
    "print(Counter(negative_adjectives).most_common(30))\n",
    "print(Counter(positive_adjectives).most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw before, the word \"good\" appears in both lists, though slightly more often in good reviews. Surprisingly, it is the most common adjective used in *bad* reviews.\n",
    "\n",
    "Words like \"disappointed\", \"awful\", \"horrible\", and \"rude\" only appear frequently in bad reviews, as expected, while words like \"favorite\", \"great\", \"fantastic\", \"amazing\", \"wonderful\", and \"perfect\" appear in good reviews, which is also expected.\n",
    "\n",
    "Words like \"small\" and \"new\" appear frequently in in both sets of reviews. Interestingly, so does \"different\", but slightly more often in good reviews, perhaps indicating that people like variety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions (5 min)\n",
    "\n",
    "In this case, we focused on the basic components of an NLP pipeline, virtually all of which are frequently used *before* building a model for the business question of interest. We saw that every part of the pipeline was highly customizable, and discussed how parameters might vary depending on the specific application in mind. \n",
    "\n",
    "In addition to constructing a basic pipeline, we tried to give initial answers to a business question: \"Which factors are most important for bad reviews?\" The answers we obtained with this out-of-the-box analysis were not perfect, but they did seem to give some genuinely useful information. For example, 3 of the 5 most important phrases for bad reviews were \"20 minutes\", \"10 minutes\", and \"15 minutes\" – strong evidence that long service times were a major driver of bad reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways (7 min)\n",
    "\n",
    "Text pre-processing is more complex than other forms of pre-processing you might be familiar with, as good pre-processing may rely on an enormous number of rules extracted from large corpora of English (Spanish!) text. You shouldn't try to recreate this work by yourself; instead, take advantage of large and powerful libraries such as `nltk` which have built-in corpora when possible, and use regular expressions when necessary to extend or tweak them.\n",
    "\n",
    "Pre-processing is an extremely important and nontrivial part of NLP, and will likely take the bulk of the work for most NLP projects. Most popular parts of the pipeline come with many parameters. Yet they can give surprisingly useful summaries of entire corpora without much adjustment.\n",
    "\n",
    "Overall, NLP can be used in many situations, but it is perhaps most useful in its ability to turn qualitative data into quantitative data. For example, if we have a collection of reviews describing, qualitatively, people’s experiences at restaurants, we can derive quantitative insights such as “X% of people who left bad reviews were unhappy with the waiting time”.\n",
    "\n",
    "After following through this case, you know what NLP is and how it can be useful. You especially know\n",
    "\n",
    "* The challenges of NLP: context specificity and high dimensionality.\n",
    "* How to standardize and pre-process text before carrying out analysis, such as stemming \n",
    "* How to tokenize documents into sentences and words\n",
    "* How to create word clouds to quickly gain high-level insights into text\n",
    "* What n-grams are and how they can be created and used in analysis\n",
    "* Why common (“stop”) words should often be removed before analysis\n",
    "* How to find common words and n-grams\n",
    "* What regular expressions are and how to use them to carry out more custom analysis\n",
    "* What Parts of Speech tagging is and why analysing documents by their grammatical structure can be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk, wordcloud, spacy, nagisa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
