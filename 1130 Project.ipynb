{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project 1130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *AirBnb DataSet Problem*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals (3 min)\n",
    "\n",
    "To understand and work with the AirBnb datasets to solve our business problem \n",
    "\n",
    "## Introduction (5 min)\n",
    "\n",
    "**Business Context.** We are a company looking to expand our ventures based off tourism.\n",
    "\n",
    "**Business Problem.** The main Task is  **wrangle datasets related to AirBnb and create a critera to figure out the areas to build in**.\n",
    "\n",
    "**Analytical Context.** Text data is highly unstructured, and often requires pre-processing before we can gather any business insights from it. We will be leveraging tools from **natural language processing (NLP)** in order to help us process this data and generate new features that can be used for analytics or model building.\n",
    "\n",
    "The case will proceed as follows: \n",
    "1. We will extract the files based off what they are\n",
    "2. We will clean the data \n",
    "3. Create bar graph and determine average age in each major zipcode \n",
    "4. Same as above but with economic demos \n",
    "5. Average Airbnb review per zip code /posyiieve airbnb reviews if they're text\n",
    "6. Average size of Airbnb per zip code \n",
    "7. Average cost of Airbnb per zip code \n",
    "8. Airbnb date availability\n",
    "9. Using all these graphs and our critera we will decide where to build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk # imports the natural language toolkit\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import string\n",
    "import zipfile\n",
    "import plotly\n",
    "import tarfile\n",
    "import gzip\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE DATASETS\n",
    "demo = pd.read_csv('demographics.csv')\n",
    "econ = pd.read_csv('econ_state.csv')\n",
    "listings = pd.read_csv('listings.csv')\n",
    "\n",
    "realestate = pd.read_csv('real_estate.csv.gz', compression='gzip')\n",
    "\n",
    "venues = pd.read_csv('venues.csv.gz', compression='gzip')\n",
    "\n",
    "zf = zipfile.ZipFile('calendar.csv.zip') \n",
    "calendar = pd.read_csv(zf.open('calendar.csv'))\n",
    "ziplook = pd.read_csv('zipcodesForLookup.csv')\n",
    "lists = pd.read_csv('listings.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>5_years_or_less</th>\n",
       "      <th>5-9_years</th>\n",
       "      <th>10-14_years</th>\n",
       "      <th>15-19_years</th>\n",
       "      <th>20-24_years</th>\n",
       "      <th>25-34_years</th>\n",
       "      <th>35-44_years</th>\n",
       "      <th>45-54_years</th>\n",
       "      <th>55-59_years</th>\n",
       "      <th>...</th>\n",
       "      <th>$10,000-$14,999</th>\n",
       "      <th>$15,000-$24,999</th>\n",
       "      <th>$25,000-$34,999</th>\n",
       "      <th>$35,000-$49,999</th>\n",
       "      <th>$50,000-$64,999</th>\n",
       "      <th>$65,000-$74,999</th>\n",
       "      <th>$75,000-$99,999</th>\n",
       "      <th>$100,000_or_more</th>\n",
       "      <th>median_household_income</th>\n",
       "      <th>mean_household_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601</td>\n",
       "      <td>17982</td>\n",
       "      <td>1006</td>\n",
       "      <td>1080</td>\n",
       "      <td>1342</td>\n",
       "      <td>1352</td>\n",
       "      <td>1321</td>\n",
       "      <td>2253</td>\n",
       "      <td>2149</td>\n",
       "      <td>2434</td>\n",
       "      <td>...</td>\n",
       "      <td>48.1</td>\n",
       "      <td>12</td>\n",
       "      <td>12.8</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>16.3</td>\n",
       "      <td>10816</td>\n",
       "      <td>20349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>40260</td>\n",
       "      <td>2006</td>\n",
       "      <td>2440</td>\n",
       "      <td>2421</td>\n",
       "      <td>2953</td>\n",
       "      <td>2865</td>\n",
       "      <td>5124</td>\n",
       "      <td>5139</td>\n",
       "      <td>5947</td>\n",
       "      <td>...</td>\n",
       "      <td>31.4</td>\n",
       "      <td>16.3</td>\n",
       "      <td>17.9</td>\n",
       "      <td>12.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>16079</td>\n",
       "      <td>23282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>603</td>\n",
       "      <td>52408</td>\n",
       "      <td>2664</td>\n",
       "      <td>3177</td>\n",
       "      <td>3351</td>\n",
       "      <td>3685</td>\n",
       "      <td>3585</td>\n",
       "      <td>6473</td>\n",
       "      <td>6775</td>\n",
       "      <td>6678</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>14.9</td>\n",
       "      <td>17.5</td>\n",
       "      <td>11.7</td>\n",
       "      <td>10.8</td>\n",
       "      <td>8.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>21.9</td>\n",
       "      <td>16804</td>\n",
       "      <td>26820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>606</td>\n",
       "      <td>6331</td>\n",
       "      <td>347</td>\n",
       "      <td>331</td>\n",
       "      <td>461</td>\n",
       "      <td>474</td>\n",
       "      <td>469</td>\n",
       "      <td>707</td>\n",
       "      <td>933</td>\n",
       "      <td>776</td>\n",
       "      <td>...</td>\n",
       "      <td>45.3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>20</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>12512</td>\n",
       "      <td>15730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>610</td>\n",
       "      <td>28328</td>\n",
       "      <td>1438</td>\n",
       "      <td>1490</td>\n",
       "      <td>2044</td>\n",
       "      <td>2122</td>\n",
       "      <td>1985</td>\n",
       "      <td>3358</td>\n",
       "      <td>3778</td>\n",
       "      <td>3858</td>\n",
       "      <td>...</td>\n",
       "      <td>26.9</td>\n",
       "      <td>14.8</td>\n",
       "      <td>23.7</td>\n",
       "      <td>15.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>18.4</td>\n",
       "      <td>17475</td>\n",
       "      <td>23360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   zipcode  5_years_or_less  5-9_years  10-14_years  15-19_years  20-24_years  \\\n",
       "0      601            17982       1006         1080         1342         1352   \n",
       "1      602            40260       2006         2440         2421         2953   \n",
       "2      603            52408       2664         3177         3351         3685   \n",
       "3      606             6331        347          331          461          474   \n",
       "4      610            28328       1438         1490         2044         2122   \n",
       "\n",
       "   25-34_years  35-44_years  45-54_years  55-59_years  ...  $10,000-$14,999  \\\n",
       "0         1321         2253         2149         2434  ...             48.1   \n",
       "1         2865         5124         5139         5947  ...             31.4   \n",
       "2         3585         6473         6775         6678  ...               31   \n",
       "3          469          707          933          776  ...             45.3   \n",
       "4         1985         3358         3778         3858  ...             26.9   \n",
       "\n",
       "   $15,000-$24,999  $25,000-$34,999  $35,000-$49,999  $50,000-$64,999  \\\n",
       "0               12             12.8              8.6              8.7   \n",
       "1             16.3             17.9             12.2             10.6   \n",
       "2             14.9             17.5             11.7             10.8   \n",
       "3             10.2               20             11.7               11   \n",
       "4             14.8             23.7             15.2              9.3   \n",
       "\n",
       "   $65,000-$74,999 $75,000-$99,999 $100,000_or_more median_household_income  \\\n",
       "0              6.2             1.4             16.3                   10816   \n",
       "1              7.7             2.9             21.2                   16079   \n",
       "2              8.7             2.4             21.9                   16804   \n",
       "3              1.8               0             12.8                   12512   \n",
       "4              7.5             1.6             18.4                   17475   \n",
       "\n",
       "  mean_household_income  \n",
       "0                 20349  \n",
       "1                 23282  \n",
       "2                 26820  \n",
       "3                 15730  \n",
       "4                 23360  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>2005Q1_gdp</th>\n",
       "      <th>2005Q2_gdp</th>\n",
       "      <th>2005Q3_gdp</th>\n",
       "      <th>2005Q4_gdp</th>\n",
       "      <th>2006Q1_gdp</th>\n",
       "      <th>2006Q2_gdp</th>\n",
       "      <th>2006Q3_gdp</th>\n",
       "      <th>2006Q4_gdp</th>\n",
       "      <th>2007Q1_gdp</th>\n",
       "      <th>...</th>\n",
       "      <th>2016/03_ur</th>\n",
       "      <th>2016/04_ur</th>\n",
       "      <th>2016/05_ur</th>\n",
       "      <th>2016/06_ur</th>\n",
       "      <th>2016/07_ur</th>\n",
       "      <th>2016/08_ur</th>\n",
       "      <th>2016/09_ur</th>\n",
       "      <th>2016/10_ur</th>\n",
       "      <th>2016/11_ur</th>\n",
       "      <th>2016/12_ur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>153332</td>\n",
       "      <td>155940</td>\n",
       "      <td>157437</td>\n",
       "      <td>160293</td>\n",
       "      <td>161934</td>\n",
       "      <td>163974</td>\n",
       "      <td>165470</td>\n",
       "      <td>166495</td>\n",
       "      <td>166821</td>\n",
       "      <td>...</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>37517</td>\n",
       "      <td>38907</td>\n",
       "      <td>40691</td>\n",
       "      <td>43138</td>\n",
       "      <td>42872</td>\n",
       "      <td>44653</td>\n",
       "      <td>45349</td>\n",
       "      <td>45840</td>\n",
       "      <td>46658</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>218206</td>\n",
       "      <td>224496</td>\n",
       "      <td>231629</td>\n",
       "      <td>235099</td>\n",
       "      <td>241787</td>\n",
       "      <td>244659</td>\n",
       "      <td>250886</td>\n",
       "      <td>256505</td>\n",
       "      <td>258078</td>\n",
       "      <td>...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>88446</td>\n",
       "      <td>89264</td>\n",
       "      <td>90515</td>\n",
       "      <td>93050</td>\n",
       "      <td>93413</td>\n",
       "      <td>95259</td>\n",
       "      <td>95481</td>\n",
       "      <td>95203</td>\n",
       "      <td>94289</td>\n",
       "      <td>...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>1722091</td>\n",
       "      <td>1747827</td>\n",
       "      <td>1787427</td>\n",
       "      <td>1809426</td>\n",
       "      <td>1857944</td>\n",
       "      <td>1865835</td>\n",
       "      <td>1886549</td>\n",
       "      <td>1907754</td>\n",
       "      <td>1915172</td>\n",
       "      <td>...</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 519 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  2005Q1_gdp  2005Q2_gdp  2005Q3_gdp  2005Q4_gdp  2006Q1_gdp  \\\n",
       "0    AL      153332      155940      157437      160293      161934   \n",
       "1    AK       37517       38907       40691       43138       42872   \n",
       "2    AZ      218206      224496      231629      235099      241787   \n",
       "3    AR       88446       89264       90515       93050       93413   \n",
       "4    CA     1722091     1747827     1787427     1809426     1857944   \n",
       "\n",
       "   2006Q2_gdp  2006Q3_gdp  2006Q4_gdp  2007Q1_gdp  ...  2016/03_ur  \\\n",
       "0      163974      165470      166495      166821  ...         6.6   \n",
       "1       44653       45349       45840       46658  ...         5.9   \n",
       "2      244659      250886      256505      258078  ...         4.1   \n",
       "3       95259       95481       95203       94289  ...         5.5   \n",
       "4     1865835     1886549     1907754     1915172  ...         5.6   \n",
       "\n",
       "   2016/04_ur  2016/05_ur  2016/06_ur  2016/07_ur  2016/08_ur  2016/09_ur  \\\n",
       "0         6.6         6.6         6.7         6.7         6.7         6.6   \n",
       "1         5.8         5.8         5.8         5.8         5.9         6.0   \n",
       "2         4.1         4.1         4.1         4.0         4.0         4.0   \n",
       "3         5.4         5.3         5.3         5.2         5.1         5.1   \n",
       "4         5.5         5.5         5.5         5.4         5.4         5.3   \n",
       "\n",
       "   2016/10_ur  2016/11_ur  2016/12_ur  \n",
       "0         6.6         6.6         6.6  \n",
       "1         6.1         6.2         6.3  \n",
       "2         4.0         4.0         3.9  \n",
       "3         5.0         5.0         5.0  \n",
       "4         5.3         5.3         5.2  \n",
       "\n",
       "[5 rows x 519 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "econ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>metro</th>\n",
       "      <th>county</th>\n",
       "      <th>size_rank</th>\n",
       "      <th>1996-04</th>\n",
       "      <th>1996-05</th>\n",
       "      <th>1996-06</th>\n",
       "      <th>...</th>\n",
       "      <th>2016-09</th>\n",
       "      <th>2016-10</th>\n",
       "      <th>2016-11</th>\n",
       "      <th>2016-12</th>\n",
       "      <th>2017-01</th>\n",
       "      <th>2017-02</th>\n",
       "      <th>2017-03</th>\n",
       "      <th>2017-04</th>\n",
       "      <th>2017-05</th>\n",
       "      <th>2017-06</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10025</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1137500</td>\n",
       "      <td>1137700</td>\n",
       "      <td>1152700</td>\n",
       "      <td>1156000.0</td>\n",
       "      <td>1140200</td>\n",
       "      <td>1130000</td>\n",
       "      <td>1131900</td>\n",
       "      <td>1149600</td>\n",
       "      <td>1198400</td>\n",
       "      <td>1247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>60657</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>2</td>\n",
       "      <td>146700.0</td>\n",
       "      <td>146500.0</td>\n",
       "      <td>146300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>318200</td>\n",
       "      <td>318100</td>\n",
       "      <td>318800</td>\n",
       "      <td>320200.0</td>\n",
       "      <td>320800</td>\n",
       "      <td>322000</td>\n",
       "      <td>323800</td>\n",
       "      <td>326100</td>\n",
       "      <td>327800</td>\n",
       "      <td>329100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>60614</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>3</td>\n",
       "      <td>198000.0</td>\n",
       "      <td>195500.0</td>\n",
       "      <td>194200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>401900</td>\n",
       "      <td>406000</td>\n",
       "      <td>414100</td>\n",
       "      <td>417800.0</td>\n",
       "      <td>417400</td>\n",
       "      <td>418400</td>\n",
       "      <td>414100</td>\n",
       "      <td>404100</td>\n",
       "      <td>406400</td>\n",
       "      <td>415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>79936</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>TX</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>4</td>\n",
       "      <td>70800.0</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>113800</td>\n",
       "      <td>113900</td>\n",
       "      <td>114100</td>\n",
       "      <td>114500.0</td>\n",
       "      <td>114900</td>\n",
       "      <td>115000</td>\n",
       "      <td>114700</td>\n",
       "      <td>114700</td>\n",
       "      <td>114800</td>\n",
       "      <td>114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>60640</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>5</td>\n",
       "      <td>102300.0</td>\n",
       "      <td>101300.0</td>\n",
       "      <td>100700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>198800</td>\n",
       "      <td>199200</td>\n",
       "      <td>200100</td>\n",
       "      <td>201500.0</td>\n",
       "      <td>203000</td>\n",
       "      <td>205100</td>\n",
       "      <td>206700</td>\n",
       "      <td>206500</td>\n",
       "      <td>206200</td>\n",
       "      <td>206700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 262 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  zipcode      city state     metro    county  size_rank   1996-04  \\\n",
       "0  ZHVI    10025  New York    NY  New York  New York          1       NaN   \n",
       "1  ZHVI    60657   Chicago    IL   Chicago      Cook          2  146700.0   \n",
       "2  ZHVI    60614   Chicago    IL   Chicago      Cook          3  198000.0   \n",
       "3  ZHVI    79936   El Paso    TX   El Paso   El Paso          4   70800.0   \n",
       "4  ZHVI    60640   Chicago    IL   Chicago      Cook          5  102300.0   \n",
       "\n",
       "    1996-05   1996-06  ...  2016-09  2016-10  2016-11    2016-12  2017-01  \\\n",
       "0       NaN       NaN  ...  1137500  1137700  1152700  1156000.0  1140200   \n",
       "1  146500.0  146300.0  ...   318200   318100   318800   320200.0   320800   \n",
       "2  195500.0  194200.0  ...   401900   406000   414100   417800.0   417400   \n",
       "3   71000.0   71000.0  ...   113800   113900   114100   114500.0   114900   \n",
       "4  101300.0  100700.0  ...   198800   199200   200100   201500.0   203000   \n",
       "\n",
       "   2017-02  2017-03  2017-04  2017-05  2017-06  \n",
       "0  1130000  1131900  1149600  1198400  1247000  \n",
       "1   322000   323800   326100   327800   329100  \n",
       "2   418400   414100   404100   406400   415500  \n",
       "3   115000   114700   114700   114800   114700  \n",
       "4   205100   206700   206500   206200   206700  \n",
       "\n",
       "[5 rows x 262 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realestate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "      <th>metro_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-05</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-03</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        date available  price metro_area\n",
       "0        2515  2018-03-05         t   69.0        NYC\n",
       "1        2515  2018-03-04         t   69.0        NYC\n",
       "2        2515  2018-03-03         t   69.0        NYC\n",
       "3        2515  2018-03-02         t   69.0        NYC\n",
       "4        2515  2018-03-01         t   69.0        NYC"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new york city</td>\n",
       "      <td>b1a0d113cb17d1d85f0e12700dd71f36bddedc54</td>\n",
       "      <td>40.601540</td>\n",
       "      <td>-73.729636</td>\n",
       "      <td>A Bacon Yacht Charter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new york city</td>\n",
       "      <td>8799bb8d8ac7c2e1933f92ac1ef0f69a5e0b37c1</td>\n",
       "      <td>40.608921</td>\n",
       "      <td>-73.728256</td>\n",
       "      <td>Mezzanote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new york city</td>\n",
       "      <td>f2390680116af4d62e0da6f0432d33e94b9cb0e6</td>\n",
       "      <td>40.616978</td>\n",
       "      <td>-73.730349</td>\n",
       "      <td>Prime Bistro</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['bar', 'restaurant', 'food', 'point_of_intere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new york city</td>\n",
       "      <td>e9294eb56025e8eb29b937dc0aca29fd3059b9ab</td>\n",
       "      <td>40.617318</td>\n",
       "      <td>-73.730637</td>\n",
       "      <td>Rita's Ice</td>\n",
       "      <td>4.6</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new york city</td>\n",
       "      <td>5c7b72c7bf9e7bc4f94baf09db1252088087f65d</td>\n",
       "      <td>40.618371</td>\n",
       "      <td>-73.728178</td>\n",
       "      <td>Cho-Sen Island</td>\n",
       "      <td>4.4</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            city                                        id   latitude  \\\n",
       "0  new york city  b1a0d113cb17d1d85f0e12700dd71f36bddedc54  40.601540   \n",
       "1  new york city  8799bb8d8ac7c2e1933f92ac1ef0f69a5e0b37c1  40.608921   \n",
       "2  new york city  f2390680116af4d62e0da6f0432d33e94b9cb0e6  40.616978   \n",
       "3  new york city  e9294eb56025e8eb29b937dc0aca29fd3059b9ab  40.617318   \n",
       "4  new york city  5c7b72c7bf9e7bc4f94baf09db1252088087f65d  40.618371   \n",
       "\n",
       "   longitude                   name  rating  \\\n",
       "0 -73.729636  A Bacon Yacht Charter     NaN   \n",
       "1 -73.728256              Mezzanote     NaN   \n",
       "2 -73.730349           Prime Bistro     4.0   \n",
       "3 -73.730637             Rita's Ice     4.6   \n",
       "4 -73.728178         Cho-Sen Island     4.4   \n",
       "\n",
       "                                               types  \n",
       "0  ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "1  ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "2  ['bar', 'restaurant', 'food', 'point_of_intere...  \n",
       "3  ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "4  ['restaurant', 'food', 'point_of_interest', 'e...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>accommodates</th>\n",
       "      <th>amenities</th>\n",
       "      <th>availability_30</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bed_type</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>beds</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>city</th>\n",
       "      <th>has_availability</th>\n",
       "      <th>...</th>\n",
       "      <th>review_scores_checkin</th>\n",
       "      <th>review_scores_cleanliness</th>\n",
       "      <th>review_scores_communication</th>\n",
       "      <th>review_scores_location</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>review_scores_value</th>\n",
       "      <th>room_type</th>\n",
       "      <th>state</th>\n",
       "      <th>weekly_price</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.0</td>\n",
       "      <td>{\"Cable TV\",\"Wireless Internet\",\"Air condition...</td>\n",
       "      <td>24</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>sunnysidebronx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Private room</td>\n",
       "      <td>NY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.0</td>\n",
       "      <td>{TV,Internet,\"Wireless Internet\",\"Air conditio...</td>\n",
       "      <td>30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>flexible</td>\n",
       "      <td>sunnysidebronx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Private room</td>\n",
       "      <td>NY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.0</td>\n",
       "      <td>{TV,\"Cable TV\",Internet,\"Wireless Internet\",\"A...</td>\n",
       "      <td>30</td>\n",
       "      <td>3.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>strict</td>\n",
       "      <td>sunnysidebronx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>NY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.0</td>\n",
       "      <td>{TV,Internet,\"Wireless Internet\",\"Air conditio...</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>strict</td>\n",
       "      <td>long island city</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>NY</td>\n",
       "      <td>775.0</td>\n",
       "      <td>10464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.0</td>\n",
       "      <td>{Internet,\"Wireless Internet\",\"Air conditionin...</td>\n",
       "      <td>17</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Real Bed</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>sunnysidebronx</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>97.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>Private room</td>\n",
       "      <td>NY</td>\n",
       "      <td>350.0</td>\n",
       "      <td>10464</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 29 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   accommodates                                          amenities  \\\n",
       "0           2.0  {\"Cable TV\",\"Wireless Internet\",\"Air condition...   \n",
       "1           4.0  {TV,Internet,\"Wireless Internet\",\"Air conditio...   \n",
       "2           4.0  {TV,\"Cable TV\",Internet,\"Wireless Internet\",\"A...   \n",
       "3           3.0  {TV,Internet,\"Wireless Internet\",\"Air conditio...   \n",
       "4           4.0  {Internet,\"Wireless Internet\",\"Air conditionin...   \n",
       "\n",
       "   availability_30  bathrooms  bed_type  bedrooms  beds cancellation_policy  \\\n",
       "0               24        1.0  Real Bed       1.0   1.0            moderate   \n",
       "1               30        1.0  Real Bed       1.0   1.0            flexible   \n",
       "2               30        3.0  Real Bed       3.0   3.0              strict   \n",
       "3                8        1.0  Real Bed       1.0   1.0              strict   \n",
       "4               17        1.0  Real Bed       1.0   1.0            moderate   \n",
       "\n",
       "               city  has_availability  ...  review_scores_checkin  \\\n",
       "0    sunnysidebronx               NaN  ...                   10.0   \n",
       "1    sunnysidebronx               NaN  ...                    NaN   \n",
       "2    sunnysidebronx               NaN  ...                    NaN   \n",
       "3  long island city               NaN  ...                   10.0   \n",
       "4    sunnysidebronx               NaN  ...                   10.0   \n",
       "\n",
       "   review_scores_cleanliness review_scores_communication  \\\n",
       "0                       10.0                        10.0   \n",
       "1                        NaN                         NaN   \n",
       "2                        NaN                         NaN   \n",
       "3                       10.0                        10.0   \n",
       "4                       10.0                        10.0   \n",
       "\n",
       "   review_scores_location  review_scores_rating review_scores_value  \\\n",
       "0                    10.0                 100.0                10.0   \n",
       "1                     NaN                   NaN                 NaN   \n",
       "2                     NaN                   NaN                 NaN   \n",
       "3                    10.0                  93.0                10.0   \n",
       "4                    10.0                  97.0                10.0   \n",
       "\n",
       "         room_type  state weekly_price  zipcode  \n",
       "0     Private room     NY          NaN    10464  \n",
       "1     Private room     NY          NaN    10464  \n",
       "2  Entire home/apt     NY          NaN    10464  \n",
       "3  Entire home/apt     NY        775.0    10464  \n",
       "4     Private room     NY        350.0    10464  \n",
       "\n",
       "[5 rows x 29 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lists.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: (4 min)\n",
    "\n",
    "Clean the data for NYC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>price</th>\n",
       "      <th>weekly_price</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>avg review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sunnysidebronx</td>\n",
       "      <td>7949480</td>\n",
       "      <td>City Island Sanctuary relaxing BR &amp; Bath w Par...</td>\n",
       "      <td>99.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10464</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sunnysidebronx</td>\n",
       "      <td>16042478</td>\n",
       "      <td>WATERFRONT STUDIO APARTMENT</td>\n",
       "      <td>200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10464</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sunnysidebronx</td>\n",
       "      <td>1886820</td>\n",
       "      <td>Quaint City Island Community.</td>\n",
       "      <td>300.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10464</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>long island city</td>\n",
       "      <td>6627449</td>\n",
       "      <td>Large 1 BDRM in Great location</td>\n",
       "      <td>125.0</td>\n",
       "      <td>775.0</td>\n",
       "      <td>10464</td>\n",
       "      <td>23.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>sunnysidebronx</td>\n",
       "      <td>5557381</td>\n",
       "      <td>Quaint City Island Home</td>\n",
       "      <td>69.0</td>\n",
       "      <td>350.0</td>\n",
       "      <td>10464</td>\n",
       "      <td>24.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40735</th>\n",
       "      <td>yonkers</td>\n",
       "      <td>18197581</td>\n",
       "      <td>Cozy room with parking lot included</td>\n",
       "      <td>70.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10705</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40736</th>\n",
       "      <td>new york</td>\n",
       "      <td>3235285</td>\n",
       "      <td>Duplex Garden apt off Central Park!</td>\n",
       "      <td>949.0</td>\n",
       "      <td>6200.0</td>\n",
       "      <td>10024</td>\n",
       "      <td>25.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40737</th>\n",
       "      <td>new york</td>\n",
       "      <td>18324921</td>\n",
       "      <td>Yacht phish shows Madison sq garden</td>\n",
       "      <td>600.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10004</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40738</th>\n",
       "      <td>new york</td>\n",
       "      <td>18035489</td>\n",
       "      <td>In the heart of tourist attractions</td>\n",
       "      <td>129.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10029</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40739</th>\n",
       "      <td>long island city</td>\n",
       "      <td>17360085</td>\n",
       "      <td>Phish bakers dozen</td>\n",
       "      <td>200.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10001</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40737 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   city        id  \\\n",
       "0        sunnysidebronx   7949480   \n",
       "1        sunnysidebronx  16042478   \n",
       "2        sunnysidebronx   1886820   \n",
       "3      long island city   6627449   \n",
       "4        sunnysidebronx   5557381   \n",
       "...                 ...       ...   \n",
       "40735           yonkers  18197581   \n",
       "40736          new york   3235285   \n",
       "40737          new york  18324921   \n",
       "40738          new york  18035489   \n",
       "40739  long island city  17360085   \n",
       "\n",
       "                                                    name  price  weekly_price  \\\n",
       "0      City Island Sanctuary relaxing BR & Bath w Par...   99.0           NaN   \n",
       "1                            WATERFRONT STUDIO APARTMENT  200.0           NaN   \n",
       "2                         Quaint City Island Community.   300.0           NaN   \n",
       "3                         Large 1 BDRM in Great location  125.0         775.0   \n",
       "4                                Quaint City Island Home   69.0         350.0   \n",
       "...                                                  ...    ...           ...   \n",
       "40735                Cozy room with parking lot included   70.0           NaN   \n",
       "40736                Duplex Garden apt off Central Park!  949.0        6200.0   \n",
       "40737                Yacht phish shows Madison sq garden  600.0           NaN   \n",
       "40738                In the heart of tourist attractions  129.0           NaN   \n",
       "40739                                 Phish bakers dozen  200.0           NaN   \n",
       "\n",
       "      zipcode  avg review  \n",
       "0       10464   25.000000  \n",
       "1       10464         NaN  \n",
       "2       10464         NaN  \n",
       "3       10464   23.833333  \n",
       "4       10464   24.500000  \n",
       "...       ...         ...  \n",
       "40735   10705         NaN  \n",
       "40736   10024   25.000000  \n",
       "40737   10004         NaN  \n",
       "40738   10029         NaN  \n",
       "40739   10001         NaN  \n",
       "\n",
       "[40737 rows x 7 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lists = lists.drop(['accommodates', 'amenities', 'availability_30','bathrooms','bed_type','bedrooms','beds'], axis=1) \n",
    "lists = lists.drop(['cancellation_policy', 'has_availability', 'instant_bookable','latitude','longitude'], axis=1) \n",
    "lists = lists.drop(['property_type', 'room_type'], axis=1) \n",
    "lists['avg review'] = lists[['review_scores_checkin', 'review_scores_cleanliness','review_scores_communication','review_scores_location','review_scores_rating','review_scores_value']].mean(axis=1)\n",
    "lists = lists.drop(['review_scores_checkin', 'review_scores_cleanliness','review_scores_communication','review_scores_location','review_scores_rating','review_scores_value'], axis=1) \n",
    "lists = lists[lists.state == 'NY']\n",
    "lists[lists.columns[~lists.isnull().any()]]\n",
    "lists = lists[lists.metropolitan == 'NYC']\n",
    "lists[lists.columns[~lists.isnull().any()]]\n",
    "lists = lists.drop(['host_id', 'metropolitan','state'], axis=1) \n",
    "lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new york city</td>\n",
       "      <td>f2390680116af4d62e0da6f0432d33e94b9cb0e6</td>\n",
       "      <td>40.616978</td>\n",
       "      <td>-73.730349</td>\n",
       "      <td>Prime Bistro</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['bar', 'restaurant', 'food', 'point_of_intere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new york city</td>\n",
       "      <td>e9294eb56025e8eb29b937dc0aca29fd3059b9ab</td>\n",
       "      <td>40.617318</td>\n",
       "      <td>-73.730637</td>\n",
       "      <td>Rita's Ice</td>\n",
       "      <td>4.6</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new york city</td>\n",
       "      <td>5c7b72c7bf9e7bc4f94baf09db1252088087f65d</td>\n",
       "      <td>40.618371</td>\n",
       "      <td>-73.728178</td>\n",
       "      <td>Cho-Sen Island</td>\n",
       "      <td>4.4</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>new york city</td>\n",
       "      <td>0b99220b44ee0d45d28f44e95d08da112f6e2ca7</td>\n",
       "      <td>40.618126</td>\n",
       "      <td>-73.728679</td>\n",
       "      <td>Sunflower Cafe- Lawrence</td>\n",
       "      <td>4.2</td>\n",
       "      <td>['cafe', 'restaurant', 'food', 'point_of_inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>new york city</td>\n",
       "      <td>97fdbf124eeb91d70d7c3f710ca52c997d618bb9</td>\n",
       "      <td>40.617592</td>\n",
       "      <td>-73.729390</td>\n",
       "      <td>Sushi Tokyo Lawrence</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            city                                        id   latitude  \\\n",
       "2  new york city  f2390680116af4d62e0da6f0432d33e94b9cb0e6  40.616978   \n",
       "3  new york city  e9294eb56025e8eb29b937dc0aca29fd3059b9ab  40.617318   \n",
       "4  new york city  5c7b72c7bf9e7bc4f94baf09db1252088087f65d  40.618371   \n",
       "5  new york city  0b99220b44ee0d45d28f44e95d08da112f6e2ca7  40.618126   \n",
       "6  new york city  97fdbf124eeb91d70d7c3f710ca52c997d618bb9  40.617592   \n",
       "\n",
       "   longitude                      name  rating  \\\n",
       "2 -73.730349              Prime Bistro     4.0   \n",
       "3 -73.730637                Rita's Ice     4.6   \n",
       "4 -73.728178            Cho-Sen Island     4.4   \n",
       "5 -73.728679  Sunflower Cafe- Lawrence     4.2   \n",
       "6 -73.729390      Sushi Tokyo Lawrence     4.0   \n",
       "\n",
       "                                               types  \n",
       "2  ['bar', 'restaurant', 'food', 'point_of_intere...  \n",
       "3  ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "4  ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "5  ['cafe', 'restaurant', 'food', 'point_of_inter...  \n",
       "6  ['restaurant', 'food', 'point_of_interest', 'e...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning venues, only get NYC drops NA values\n",
    "venuescl = venues[venues.city == 'new york city']\n",
    "venuescl[venuescl.columns[~venues.isnull().any()]]\n",
    "venuescl = venuescl.dropna(how='any',axis=0) \n",
    "venuescl.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "      <th>metro_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-05</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-03</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        date available  price metro_area\n",
       "0        2515  2018-03-05         t   69.0        NYC\n",
       "1        2515  2018-03-04         t   69.0        NYC\n",
       "2        2515  2018-03-03         t   69.0        NYC\n",
       "3        2515  2018-03-02         t   69.0        NYC\n",
       "4        2515  2018-03-01         t   69.0        NYC"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning Calender, drops NAS and gets only avaliable airbnbs\n",
    "calendar_aval = calendar[calendar.available == 't']\n",
    "calendar_aval[calendar_aval.columns[~calendar_aval.isnull().any()]]\n",
    "calendar_aval = calendar_aval[calendar_aval.metro_area == 'NYC']\n",
    "calendar_aval[calendar_aval.columns[~calendar_aval.isnull().any()]]\n",
    "calendar_aval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>metro</th>\n",
       "      <th>county</th>\n",
       "      <th>size_rank</th>\n",
       "      <th>2016-08</th>\n",
       "      <th>2016-09</th>\n",
       "      <th>2016-10</th>\n",
       "      <th>2016-11</th>\n",
       "      <th>2016-12</th>\n",
       "      <th>2017-01</th>\n",
       "      <th>2017-02</th>\n",
       "      <th>2017-03</th>\n",
       "      <th>2017-04</th>\n",
       "      <th>2017-05</th>\n",
       "      <th>2017-06</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10025</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>1</td>\n",
       "      <td>1132500</td>\n",
       "      <td>1137500</td>\n",
       "      <td>1137700</td>\n",
       "      <td>1152700</td>\n",
       "      <td>1156000.0</td>\n",
       "      <td>1140200</td>\n",
       "      <td>1130000</td>\n",
       "      <td>1131900</td>\n",
       "      <td>1149600</td>\n",
       "      <td>1198400</td>\n",
       "      <td>1247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>11226</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>Kings</td>\n",
       "      <td>9</td>\n",
       "      <td>572800</td>\n",
       "      <td>583600</td>\n",
       "      <td>594800</td>\n",
       "      <td>605200</td>\n",
       "      <td>612100.0</td>\n",
       "      <td>612800</td>\n",
       "      <td>616900</td>\n",
       "      <td>628900</td>\n",
       "      <td>644200</td>\n",
       "      <td>653500</td>\n",
       "      <td>658700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10016</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>11</td>\n",
       "      <td>917000</td>\n",
       "      <td>934800</td>\n",
       "      <td>946000</td>\n",
       "      <td>949200</td>\n",
       "      <td>950000.0</td>\n",
       "      <td>951800</td>\n",
       "      <td>960100</td>\n",
       "      <td>972200</td>\n",
       "      <td>986900</td>\n",
       "      <td>1021200</td>\n",
       "      <td>1061200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10128</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>18</td>\n",
       "      <td>1077000</td>\n",
       "      <td>1074900</td>\n",
       "      <td>1076400</td>\n",
       "      <td>1090300</td>\n",
       "      <td>1111300.0</td>\n",
       "      <td>1133100</td>\n",
       "      <td>1154700</td>\n",
       "      <td>1189800</td>\n",
       "      <td>1241000</td>\n",
       "      <td>1288200</td>\n",
       "      <td>1318300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10462</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>24</td>\n",
       "      <td>118200</td>\n",
       "      <td>120600</td>\n",
       "      <td>121600</td>\n",
       "      <td>121500</td>\n",
       "      <td>121000.0</td>\n",
       "      <td>121100</td>\n",
       "      <td>123200</td>\n",
       "      <td>126200</td>\n",
       "      <td>128700</td>\n",
       "      <td>131900</td>\n",
       "      <td>135400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type  zipcode      city state     metro    county  size_rank  2016-08  \\\n",
       "0   ZHVI    10025  New York    NY  New York  New York          1  1132500   \n",
       "8   ZHVI    11226  New York    NY  New York     Kings          9   572800   \n",
       "10  ZHVI    10016  New York    NY  New York  New York         11   917000   \n",
       "17  ZHVI    10128  New York    NY  New York  New York         18  1077000   \n",
       "23  ZHVI    10462  New York    NY  New York     Bronx         24   118200   \n",
       "\n",
       "    2016-09  2016-10  2016-11    2016-12  2017-01  2017-02  2017-03  2017-04  \\\n",
       "0   1137500  1137700  1152700  1156000.0  1140200  1130000  1131900  1149600   \n",
       "8    583600   594800   605200   612100.0   612800   616900   628900   644200   \n",
       "10   934800   946000   949200   950000.0   951800   960100   972200   986900   \n",
       "17  1074900  1076400  1090300  1111300.0  1133100  1154700  1189800  1241000   \n",
       "23   120600   121600   121500   121000.0   121100   123200   126200   128700   \n",
       "\n",
       "    2017-05  2017-06  \n",
       "0   1198400  1247000  \n",
       "8    653500   658700  \n",
       "10  1021200  1061200  \n",
       "17  1288200  1318300  \n",
       "23   131900   135400  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning real estate, gets only 2016-08 onwards\n",
    "realcleaned = realestate.filter([\"type\",\"zipcode\",\"city\",\"state\",\"metro\",\"county\",\"size_rank\",\"2016-08\",\"2016-09\",\"2016-10\",\"2016-11\",\"2016-12\",\"2017-01\",\"2017-02\",\"2017-03\",\"2017-04\",\"2017-05\",\"2017-06\"], axis=1)\n",
    "realcleaned = realcleaned[realcleaned.city == 'New York']\n",
    "realcleaned[realcleaned.columns[~realcleaned.isnull().any()]]\n",
    "realcleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>5_years_or_less</th>\n",
       "      <th>5-9_years</th>\n",
       "      <th>10-14_years</th>\n",
       "      <th>15-19_years</th>\n",
       "      <th>20-24_years</th>\n",
       "      <th>25-34_years</th>\n",
       "      <th>35-44_years</th>\n",
       "      <th>45-54_years</th>\n",
       "      <th>55-59_years</th>\n",
       "      <th>...</th>\n",
       "      <th>$10,000-$14,999</th>\n",
       "      <th>$15,000-$24,999</th>\n",
       "      <th>$25,000-$34,999</th>\n",
       "      <th>$35,000-$49,999</th>\n",
       "      <th>$50,000-$64,999</th>\n",
       "      <th>$65,000-$74,999</th>\n",
       "      <th>$75,000-$99,999</th>\n",
       "      <th>$100,000_or_more</th>\n",
       "      <th>median_household_income</th>\n",
       "      <th>mean_household_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601</td>\n",
       "      <td>17982</td>\n",
       "      <td>1006</td>\n",
       "      <td>1080</td>\n",
       "      <td>1342</td>\n",
       "      <td>1352</td>\n",
       "      <td>1321</td>\n",
       "      <td>2253</td>\n",
       "      <td>2149</td>\n",
       "      <td>2434</td>\n",
       "      <td>...</td>\n",
       "      <td>48.1</td>\n",
       "      <td>12</td>\n",
       "      <td>12.8</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>16.3</td>\n",
       "      <td>10816</td>\n",
       "      <td>20349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>40260</td>\n",
       "      <td>2006</td>\n",
       "      <td>2440</td>\n",
       "      <td>2421</td>\n",
       "      <td>2953</td>\n",
       "      <td>2865</td>\n",
       "      <td>5124</td>\n",
       "      <td>5139</td>\n",
       "      <td>5947</td>\n",
       "      <td>...</td>\n",
       "      <td>31.4</td>\n",
       "      <td>16.3</td>\n",
       "      <td>17.9</td>\n",
       "      <td>12.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>16079</td>\n",
       "      <td>23282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>603</td>\n",
       "      <td>52408</td>\n",
       "      <td>2664</td>\n",
       "      <td>3177</td>\n",
       "      <td>3351</td>\n",
       "      <td>3685</td>\n",
       "      <td>3585</td>\n",
       "      <td>6473</td>\n",
       "      <td>6775</td>\n",
       "      <td>6678</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>14.9</td>\n",
       "      <td>17.5</td>\n",
       "      <td>11.7</td>\n",
       "      <td>10.8</td>\n",
       "      <td>8.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>21.9</td>\n",
       "      <td>16804</td>\n",
       "      <td>26820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>606</td>\n",
       "      <td>6331</td>\n",
       "      <td>347</td>\n",
       "      <td>331</td>\n",
       "      <td>461</td>\n",
       "      <td>474</td>\n",
       "      <td>469</td>\n",
       "      <td>707</td>\n",
       "      <td>933</td>\n",
       "      <td>776</td>\n",
       "      <td>...</td>\n",
       "      <td>45.3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>20</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>12512</td>\n",
       "      <td>15730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>610</td>\n",
       "      <td>28328</td>\n",
       "      <td>1438</td>\n",
       "      <td>1490</td>\n",
       "      <td>2044</td>\n",
       "      <td>2122</td>\n",
       "      <td>1985</td>\n",
       "      <td>3358</td>\n",
       "      <td>3778</td>\n",
       "      <td>3858</td>\n",
       "      <td>...</td>\n",
       "      <td>26.9</td>\n",
       "      <td>14.8</td>\n",
       "      <td>23.7</td>\n",
       "      <td>15.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>18.4</td>\n",
       "      <td>17475</td>\n",
       "      <td>23360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   zipcode  5_years_or_less  5-9_years  10-14_years  15-19_years  20-24_years  \\\n",
       "0      601            17982       1006         1080         1342         1352   \n",
       "1      602            40260       2006         2440         2421         2953   \n",
       "2      603            52408       2664         3177         3351         3685   \n",
       "3      606             6331        347          331          461          474   \n",
       "4      610            28328       1438         1490         2044         2122   \n",
       "\n",
       "   25-34_years  35-44_years  45-54_years  55-59_years  ...  $10,000-$14,999  \\\n",
       "0         1321         2253         2149         2434  ...             48.1   \n",
       "1         2865         5124         5139         5947  ...             31.4   \n",
       "2         3585         6473         6775         6678  ...               31   \n",
       "3          469          707          933          776  ...             45.3   \n",
       "4         1985         3358         3778         3858  ...             26.9   \n",
       "\n",
       "   $15,000-$24,999  $25,000-$34,999  $35,000-$49,999  $50,000-$64,999  \\\n",
       "0               12             12.8              8.6              8.7   \n",
       "1             16.3             17.9             12.2             10.6   \n",
       "2             14.9             17.5             11.7             10.8   \n",
       "3             10.2               20             11.7               11   \n",
       "4             14.8             23.7             15.2              9.3   \n",
       "\n",
       "   $65,000-$74,999 $75,000-$99,999 $100,000_or_more median_household_income  \\\n",
       "0              6.2             1.4             16.3                   10816   \n",
       "1              7.7             2.9             21.2                   16079   \n",
       "2              8.7             2.4             21.9                   16804   \n",
       "3              1.8               0             12.8                   12512   \n",
       "4              7.5             1.6             18.4                   17475   \n",
       "\n",
       "  mean_household_income  \n",
       "0                 20349  \n",
       "1                 23282  \n",
       "2                 26820  \n",
       "3                 15730  \n",
       "4                 23360  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Econ doesnt need cleaning\n",
    "democlean = demo.filter([\"zipcode\",\"20-24_years\",\"25-34_years\",\"35-44_years\",\"35,000\",\"50,000\",\"65,000\",\"75,000\",\"households\",\"$9,999_or_less\",\"$10,000-$14,999\",\"$15,000-$24,999\",\"$25,000-$34,999\",\"$35,000-$49,999\",\"$50,000-$64,999\",\"$65,000-$74,999\",\"$75,000-$99,999\",\"$100,000_or_more\",\"median_household_income\",\"mean_household_income\"], axis=1)\n",
    "democlean = demo.dropna(how='any',axis=0) \n",
    "democlean.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data to work with it\n",
    "\n",
    "Here we work on the data to get it so we can analyze it further by adding zipcodes and doing zipcode lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{10001: ['Manhattan'],\n",
       " 10002: ['Manhattan'],\n",
       " 10003: ['Manhattan'],\n",
       " 10004: ['Manhattan'],\n",
       " 10005: ['Manhattan'],\n",
       " 10006: ['Manhattan'],\n",
       " 10007: ['Manhattan'],\n",
       " 10009: ['Manhattan'],\n",
       " 10010: ['Manhattan'],\n",
       " 10011: ['Manhattan'],\n",
       " 10012: ['Manhattan'],\n",
       " 10013: ['Manhattan'],\n",
       " 10014: ['Manhattan'],\n",
       " 10015: ['Manhattan'],\n",
       " 10016: ['Manhattan'],\n",
       " 10017: ['Manhattan'],\n",
       " 10018: ['Manhattan'],\n",
       " 10019: ['Manhattan'],\n",
       " 10020: ['Manhattan'],\n",
       " 10021: ['Manhattan'],\n",
       " 10022: ['Manhattan'],\n",
       " 10023: ['Manhattan'],\n",
       " 10024: ['Manhattan'],\n",
       " 10025: ['Manhattan'],\n",
       " 10026: ['Manhattan'],\n",
       " 10027: ['Manhattan'],\n",
       " 10028: ['Manhattan'],\n",
       " 10029: ['Manhattan'],\n",
       " 10030: ['Manhattan'],\n",
       " 10031: ['Manhattan'],\n",
       " 10032: ['Manhattan'],\n",
       " 10033: ['Manhattan'],\n",
       " 10034: ['Manhattan'],\n",
       " 10035: ['Manhattan'],\n",
       " 10036: ['Manhattan'],\n",
       " 10037: ['Manhattan'],\n",
       " 10038: ['Manhattan'],\n",
       " 10039: ['Manhattan'],\n",
       " 10040: ['Manhattan'],\n",
       " 10041: ['Manhattan'],\n",
       " 10044: ['Manhattan'],\n",
       " 10045: ['Manhattan'],\n",
       " 10048: ['Manhattan'],\n",
       " 10055: ['Manhattan'],\n",
       " 10060: ['Manhattan'],\n",
       " 10069: ['Manhattan'],\n",
       " 10090: ['Manhattan'],\n",
       " 10095: ['Manhattan'],\n",
       " 10098: ['Manhattan'],\n",
       " 10099: ['Manhattan'],\n",
       " 10103: ['Manhattan'],\n",
       " 10104: ['Manhattan'],\n",
       " 10105: ['Manhattan'],\n",
       " 10106: ['Manhattan'],\n",
       " 10107: ['Manhattan'],\n",
       " 10110: ['Manhattan'],\n",
       " 10111: ['Manhattan'],\n",
       " 10112: ['Manhattan'],\n",
       " 10115: ['Manhattan'],\n",
       " 10118: ['Manhattan'],\n",
       " 10119: ['Manhattan'],\n",
       " 10120: ['Manhattan'],\n",
       " 10121: ['Manhattan'],\n",
       " 10122: ['Manhattan'],\n",
       " 10123: ['Manhattan'],\n",
       " 10128: ['Manhattan'],\n",
       " 10151: ['Manhattan'],\n",
       " 10152: ['Manhattan'],\n",
       " 10153: ['Manhattan'],\n",
       " 10154: ['Manhattan'],\n",
       " 10155: ['Manhattan'],\n",
       " 10158: ['Manhattan'],\n",
       " 10161: ['Manhattan'],\n",
       " 10162: ['Manhattan'],\n",
       " 10165: ['Manhattan'],\n",
       " 10166: ['Manhattan'],\n",
       " 10167: ['Manhattan'],\n",
       " 10168: ['Manhattan'],\n",
       " 10169: ['Manhattan'],\n",
       " 10170: ['Manhattan'],\n",
       " 10171: ['Manhattan'],\n",
       " 10172: ['Manhattan'],\n",
       " 10173: ['Manhattan'],\n",
       " 10174: ['Manhattan'],\n",
       " 10175: ['Manhattan'],\n",
       " 10176: ['Manhattan'],\n",
       " 10177: ['Manhattan'],\n",
       " 10178: ['Manhattan'],\n",
       " 10199: ['Manhattan'],\n",
       " 10270: ['Manhattan'],\n",
       " 10271: ['Manhattan'],\n",
       " 10278: ['Manhattan'],\n",
       " 10279: ['Manhattan'],\n",
       " 10280: ['Manhattan'],\n",
       " 10281: ['Manhattan'],\n",
       " 10282: ['Manhattan'],\n",
       " 10301: ['Staten'],\n",
       " 10302: ['Staten'],\n",
       " 10303: ['Staten'],\n",
       " 10304: ['Staten'],\n",
       " 10305: ['Staten'],\n",
       " 10306: ['Staten'],\n",
       " 10307: ['Staten'],\n",
       " 10308: ['Staten'],\n",
       " 10309: ['Staten'],\n",
       " 10310: ['Staten'],\n",
       " 10311: ['Staten'],\n",
       " 10312: ['Staten'],\n",
       " 10314: ['Staten'],\n",
       " 10451: ['Bronx'],\n",
       " 10452: ['Bronx'],\n",
       " 10453: ['Bronx'],\n",
       " 10454: ['Bronx'],\n",
       " 10455: ['Bronx'],\n",
       " 10456: ['Bronx'],\n",
       " 10457: ['Bronx'],\n",
       " 10458: ['Bronx'],\n",
       " 10459: ['Bronx'],\n",
       " 10460: ['Bronx'],\n",
       " 10461: ['Bronx'],\n",
       " 10462: ['Bronx'],\n",
       " 10463: ['Bronx'],\n",
       " 10464: ['Bronx'],\n",
       " 10465: ['Bronx'],\n",
       " 10466: ['Bronx'],\n",
       " 10467: ['Bronx'],\n",
       " 10468: ['Bronx'],\n",
       " 10469: ['Bronx'],\n",
       " 10470: ['Bronx'],\n",
       " 10471: ['Bronx'],\n",
       " 10472: ['Bronx'],\n",
       " 10473: ['Bronx'],\n",
       " 10474: ['Bronx'],\n",
       " 10475: ['Bronx'],\n",
       " 11004: ['Queens'],\n",
       " 11101: ['Queens'],\n",
       " 11102: ['Queens'],\n",
       " 11103: ['Queens'],\n",
       " 11104: ['Queens'],\n",
       " 11105: ['Queens'],\n",
       " 11106: ['Queens'],\n",
       " 11109: ['Queens'],\n",
       " 11201: ['Brooklyn'],\n",
       " 11203: ['Brooklyn'],\n",
       " 11204: ['Brooklyn'],\n",
       " 11205: ['Brooklyn'],\n",
       " 11206: ['Brooklyn'],\n",
       " 11207: ['Brooklyn'],\n",
       " 11208: ['Brooklyn'],\n",
       " 11209: ['Brooklyn'],\n",
       " 11210: ['Brooklyn'],\n",
       " 11211: ['Brooklyn'],\n",
       " 11212: ['Brooklyn'],\n",
       " 11213: ['Brooklyn'],\n",
       " 11214: ['Brooklyn'],\n",
       " 11215: ['Brooklyn'],\n",
       " 11216: ['Brooklyn'],\n",
       " 11217: ['Brooklyn'],\n",
       " 11218: ['Brooklyn'],\n",
       " 11219: ['Brooklyn'],\n",
       " 11220: ['Brooklyn'],\n",
       " 11221: ['Brooklyn'],\n",
       " 11222: ['Brooklyn'],\n",
       " 11223: ['Brooklyn'],\n",
       " 11224: ['Brooklyn'],\n",
       " 11225: ['Brooklyn'],\n",
       " 11226: ['Brooklyn'],\n",
       " 11228: ['Brooklyn'],\n",
       " 11229: ['Brooklyn'],\n",
       " 11230: ['Brooklyn'],\n",
       " 11231: ['Brooklyn'],\n",
       " 11232: ['Brooklyn'],\n",
       " 11233: ['Brooklyn'],\n",
       " 11234: ['Brooklyn'],\n",
       " 11235: ['Brooklyn'],\n",
       " 11236: ['Brooklyn'],\n",
       " 11237: ['Brooklyn'],\n",
       " 11238: ['Brooklyn'],\n",
       " 11239: ['Brooklyn'],\n",
       " 11241: ['Brooklyn'],\n",
       " 11242: ['Brooklyn'],\n",
       " 11243: ['Brooklyn'],\n",
       " 11249: ['Brooklyn'],\n",
       " 11252: ['Brooklyn'],\n",
       " 11256: ['Brooklyn'],\n",
       " 11351: ['Queens'],\n",
       " 11354: ['Queens'],\n",
       " 11355: ['Queens'],\n",
       " 11356: ['Queens'],\n",
       " 11357: ['Queens'],\n",
       " 11358: ['Queens'],\n",
       " 11359: ['Queens'],\n",
       " 11360: ['Queens'],\n",
       " 11361: ['Queens'],\n",
       " 11362: ['Queens'],\n",
       " 11363: ['Queens'],\n",
       " 11364: ['Queens'],\n",
       " 11365: ['Queens'],\n",
       " 11366: ['Queens'],\n",
       " 11367: ['Queens'],\n",
       " 11368: ['Queens'],\n",
       " 11369: ['Queens'],\n",
       " 11370: ['Queens'],\n",
       " 11371: ['Queens'],\n",
       " 11372: ['Queens'],\n",
       " 11373: ['Queens'],\n",
       " 11374: ['Queens'],\n",
       " 11375: ['Queens'],\n",
       " 11377: ['Queens'],\n",
       " 11378: ['Queens'],\n",
       " 11379: ['Queens'],\n",
       " 11385: ['Queens'],\n",
       " 11411: ['Queens'],\n",
       " 11412: ['Queens'],\n",
       " 11413: ['Queens'],\n",
       " 11414: ['Queens'],\n",
       " 11415: ['Queens'],\n",
       " 11416: ['Queens'],\n",
       " 11417: ['Queens'],\n",
       " 11418: ['Queens'],\n",
       " 11419: ['Queens'],\n",
       " 11420: ['Queens'],\n",
       " 11421: ['Queens'],\n",
       " 11422: ['Queens'],\n",
       " 11423: ['Queens'],\n",
       " 11426: ['Queens'],\n",
       " 11427: ['Queens'],\n",
       " 11428: ['Queens'],\n",
       " 11429: ['Queens'],\n",
       " 11430: ['Queens'],\n",
       " 11432: ['Queens'],\n",
       " 11433: ['Queens'],\n",
       " 11434: ['Queens'],\n",
       " 11435: ['Queens'],\n",
       " 11436: ['Queens'],\n",
       " 11691: ['Queens'],\n",
       " 11692: ['Queens'],\n",
       " 11693: ['Queens'],\n",
       " 11694: ['Queens'],\n",
       " 11697: ['Queens']}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipdict = ziplook.set_index('Column1').T.to_dict('list')\n",
    "zipdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>metro</th>\n",
       "      <th>county</th>\n",
       "      <th>size_rank</th>\n",
       "      <th>2016-08</th>\n",
       "      <th>2016-09</th>\n",
       "      <th>2016-10</th>\n",
       "      <th>2016-11</th>\n",
       "      <th>2016-12</th>\n",
       "      <th>2017-01</th>\n",
       "      <th>2017-02</th>\n",
       "      <th>2017-03</th>\n",
       "      <th>2017-04</th>\n",
       "      <th>2017-05</th>\n",
       "      <th>2017-06</th>\n",
       "      <th>borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10025</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>1</td>\n",
       "      <td>1132500</td>\n",
       "      <td>1137500</td>\n",
       "      <td>1137700</td>\n",
       "      <td>1152700</td>\n",
       "      <td>1156000.0</td>\n",
       "      <td>1140200</td>\n",
       "      <td>1130000</td>\n",
       "      <td>1131900</td>\n",
       "      <td>1149600</td>\n",
       "      <td>1198400</td>\n",
       "      <td>1247000</td>\n",
       "      <td>[Manhattan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>11226</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>Kings</td>\n",
       "      <td>9</td>\n",
       "      <td>572800</td>\n",
       "      <td>583600</td>\n",
       "      <td>594800</td>\n",
       "      <td>605200</td>\n",
       "      <td>612100.0</td>\n",
       "      <td>612800</td>\n",
       "      <td>616900</td>\n",
       "      <td>628900</td>\n",
       "      <td>644200</td>\n",
       "      <td>653500</td>\n",
       "      <td>658700</td>\n",
       "      <td>[Brooklyn]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10016</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>11</td>\n",
       "      <td>917000</td>\n",
       "      <td>934800</td>\n",
       "      <td>946000</td>\n",
       "      <td>949200</td>\n",
       "      <td>950000.0</td>\n",
       "      <td>951800</td>\n",
       "      <td>960100</td>\n",
       "      <td>972200</td>\n",
       "      <td>986900</td>\n",
       "      <td>1021200</td>\n",
       "      <td>1061200</td>\n",
       "      <td>[Manhattan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10128</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>18</td>\n",
       "      <td>1077000</td>\n",
       "      <td>1074900</td>\n",
       "      <td>1076400</td>\n",
       "      <td>1090300</td>\n",
       "      <td>1111300.0</td>\n",
       "      <td>1133100</td>\n",
       "      <td>1154700</td>\n",
       "      <td>1189800</td>\n",
       "      <td>1241000</td>\n",
       "      <td>1288200</td>\n",
       "      <td>1318300</td>\n",
       "      <td>[Manhattan]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10462</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>24</td>\n",
       "      <td>118200</td>\n",
       "      <td>120600</td>\n",
       "      <td>121600</td>\n",
       "      <td>121500</td>\n",
       "      <td>121000.0</td>\n",
       "      <td>121100</td>\n",
       "      <td>123200</td>\n",
       "      <td>126200</td>\n",
       "      <td>128700</td>\n",
       "      <td>131900</td>\n",
       "      <td>135400</td>\n",
       "      <td>[Bronx]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type  zipcode      city state     metro    county  size_rank  2016-08  \\\n",
       "0   ZHVI    10025  New York    NY  New York  New York          1  1132500   \n",
       "8   ZHVI    11226  New York    NY  New York     Kings          9   572800   \n",
       "10  ZHVI    10016  New York    NY  New York  New York         11   917000   \n",
       "17  ZHVI    10128  New York    NY  New York  New York         18  1077000   \n",
       "23  ZHVI    10462  New York    NY  New York     Bronx         24   118200   \n",
       "\n",
       "    2016-09  2016-10  2016-11    2016-12  2017-01  2017-02  2017-03  2017-04  \\\n",
       "0   1137500  1137700  1152700  1156000.0  1140200  1130000  1131900  1149600   \n",
       "8    583600   594800   605200   612100.0   612800   616900   628900   644200   \n",
       "10   934800   946000   949200   950000.0   951800   960100   972200   986900   \n",
       "17  1074900  1076400  1090300  1111300.0  1133100  1154700  1189800  1241000   \n",
       "23   120600   121600   121500   121000.0   121100   123200   126200   128700   \n",
       "\n",
       "    2017-05  2017-06      borough  \n",
       "0   1198400  1247000  [Manhattan]  \n",
       "8    653500   658700   [Brooklyn]  \n",
       "10  1021200  1061200  [Manhattan]  \n",
       "17  1288200  1318300  [Manhattan]  \n",
       "23   131900   135400      [Bronx]  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realcleaned['borough'] = realcleaned['zipcode'].map(zipdict)\n",
    "realcleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>2016-08</th>\n",
       "      <th>2016-09</th>\n",
       "      <th>2016-10</th>\n",
       "      <th>2016-11</th>\n",
       "      <th>2016-12</th>\n",
       "      <th>2017-01</th>\n",
       "      <th>2017-02</th>\n",
       "      <th>2017-03</th>\n",
       "      <th>2017-04</th>\n",
       "      <th>2017-05</th>\n",
       "      <th>2017-06</th>\n",
       "      <th>borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10025</td>\n",
       "      <td>1132500</td>\n",
       "      <td>1137500</td>\n",
       "      <td>1137700</td>\n",
       "      <td>1152700</td>\n",
       "      <td>1156000.0</td>\n",
       "      <td>1140200</td>\n",
       "      <td>1130000</td>\n",
       "      <td>1131900</td>\n",
       "      <td>1149600</td>\n",
       "      <td>1198400</td>\n",
       "      <td>1247000</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>11226</td>\n",
       "      <td>572800</td>\n",
       "      <td>583600</td>\n",
       "      <td>594800</td>\n",
       "      <td>605200</td>\n",
       "      <td>612100.0</td>\n",
       "      <td>612800</td>\n",
       "      <td>616900</td>\n",
       "      <td>628900</td>\n",
       "      <td>644200</td>\n",
       "      <td>653500</td>\n",
       "      <td>658700</td>\n",
       "      <td>Brooklyn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10016</td>\n",
       "      <td>917000</td>\n",
       "      <td>934800</td>\n",
       "      <td>946000</td>\n",
       "      <td>949200</td>\n",
       "      <td>950000.0</td>\n",
       "      <td>951800</td>\n",
       "      <td>960100</td>\n",
       "      <td>972200</td>\n",
       "      <td>986900</td>\n",
       "      <td>1021200</td>\n",
       "      <td>1061200</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10128</td>\n",
       "      <td>1077000</td>\n",
       "      <td>1074900</td>\n",
       "      <td>1076400</td>\n",
       "      <td>1090300</td>\n",
       "      <td>1111300.0</td>\n",
       "      <td>1133100</td>\n",
       "      <td>1154700</td>\n",
       "      <td>1189800</td>\n",
       "      <td>1241000</td>\n",
       "      <td>1288200</td>\n",
       "      <td>1318300</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10462</td>\n",
       "      <td>118200</td>\n",
       "      <td>120600</td>\n",
       "      <td>121600</td>\n",
       "      <td>121500</td>\n",
       "      <td>121000.0</td>\n",
       "      <td>121100</td>\n",
       "      <td>123200</td>\n",
       "      <td>126200</td>\n",
       "      <td>128700</td>\n",
       "      <td>131900</td>\n",
       "      <td>135400</td>\n",
       "      <td>Bronx</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    type  zipcode  2016-08  2016-09  2016-10  2016-11    2016-12  2017-01  \\\n",
       "0   ZHVI    10025  1132500  1137500  1137700  1152700  1156000.0  1140200   \n",
       "8   ZHVI    11226   572800   583600   594800   605200   612100.0   612800   \n",
       "10  ZHVI    10016   917000   934800   946000   949200   950000.0   951800   \n",
       "17  ZHVI    10128  1077000  1074900  1076400  1090300  1111300.0  1133100   \n",
       "23  ZHVI    10462   118200   120600   121600   121500   121000.0   121100   \n",
       "\n",
       "    2017-02  2017-03  2017-04  2017-05  2017-06    borough  \n",
       "0   1130000  1131900  1149600  1198400  1247000  Manhattan  \n",
       "8    616900   628900   644200   653500   658700   Brooklyn  \n",
       "10   960100   972200   986900  1021200  1061200  Manhattan  \n",
       "17  1154700  1189800  1241000  1288200  1318300  Manhattan  \n",
       "23   123200   126200   128700   131900   135400      Bronx  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop city, state, metro, county, size_rank columns\n",
    "realcleaned  = realcleaned.drop(['city', 'state', 'metro', 'county', 'size_rank'], axis=1) \n",
    "#remove square brackets from start and end of borough column\n",
    "realcleaned['borough'] = realcleaned['borough'].str[0]\n",
    "realcleaned.head() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>5_years_or_less</th>\n",
       "      <th>5-9_years</th>\n",
       "      <th>10-14_years</th>\n",
       "      <th>15-19_years</th>\n",
       "      <th>20-24_years</th>\n",
       "      <th>25-34_years</th>\n",
       "      <th>35-44_years</th>\n",
       "      <th>45-54_years</th>\n",
       "      <th>55-59_years</th>\n",
       "      <th>...</th>\n",
       "      <th>$15,000-$24,999</th>\n",
       "      <th>$25,000-$34,999</th>\n",
       "      <th>$35,000-$49,999</th>\n",
       "      <th>$50,000-$64,999</th>\n",
       "      <th>$65,000-$74,999</th>\n",
       "      <th>$75,000-$99,999</th>\n",
       "      <th>$100,000_or_more</th>\n",
       "      <th>median_household_income</th>\n",
       "      <th>mean_household_income</th>\n",
       "      <th>borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2558</th>\n",
       "      <td>10001</td>\n",
       "      <td>23537</td>\n",
       "      <td>562</td>\n",
       "      <td>1089</td>\n",
       "      <td>900</td>\n",
       "      <td>1053</td>\n",
       "      <td>2785</td>\n",
       "      <td>4970</td>\n",
       "      <td>3905</td>\n",
       "      <td>2795</td>\n",
       "      <td>...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>9.3</td>\n",
       "      <td>5.1</td>\n",
       "      <td>9.2</td>\n",
       "      <td>8.5</td>\n",
       "      <td>9.8</td>\n",
       "      <td>27.5</td>\n",
       "      <td>86801</td>\n",
       "      <td>158183</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2559</th>\n",
       "      <td>10002</td>\n",
       "      <td>80736</td>\n",
       "      <td>3079</td>\n",
       "      <td>3171</td>\n",
       "      <td>3638</td>\n",
       "      <td>3922</td>\n",
       "      <td>5338</td>\n",
       "      <td>13938</td>\n",
       "      <td>12055</td>\n",
       "      <td>11182</td>\n",
       "      <td>...</td>\n",
       "      <td>12.6</td>\n",
       "      <td>14</td>\n",
       "      <td>8.8</td>\n",
       "      <td>10</td>\n",
       "      <td>14.2</td>\n",
       "      <td>7.1</td>\n",
       "      <td>31.3</td>\n",
       "      <td>33726</td>\n",
       "      <td>61946</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2560</th>\n",
       "      <td>10003</td>\n",
       "      <td>57112</td>\n",
       "      <td>1545</td>\n",
       "      <td>911</td>\n",
       "      <td>764</td>\n",
       "      <td>7308</td>\n",
       "      <td>5714</td>\n",
       "      <td>15565</td>\n",
       "      <td>7843</td>\n",
       "      <td>5718</td>\n",
       "      <td>...</td>\n",
       "      <td>1.9</td>\n",
       "      <td>7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>6.4</td>\n",
       "      <td>13</td>\n",
       "      <td>11.3</td>\n",
       "      <td>30.7</td>\n",
       "      <td>98151</td>\n",
       "      <td>161692</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2561</th>\n",
       "      <td>10004</td>\n",
       "      <td>3221</td>\n",
       "      <td>126</td>\n",
       "      <td>101</td>\n",
       "      <td>161</td>\n",
       "      <td>227</td>\n",
       "      <td>255</td>\n",
       "      <td>857</td>\n",
       "      <td>572</td>\n",
       "      <td>387</td>\n",
       "      <td>...</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>6.5</td>\n",
       "      <td>5.1</td>\n",
       "      <td>7.1</td>\n",
       "      <td>14.8</td>\n",
       "      <td>27</td>\n",
       "      <td>119691</td>\n",
       "      <td>177262</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2562</th>\n",
       "      <td>10005</td>\n",
       "      <td>8131</td>\n",
       "      <td>299</td>\n",
       "      <td>250</td>\n",
       "      <td>140</td>\n",
       "      <td>85</td>\n",
       "      <td>1101</td>\n",
       "      <td>3690</td>\n",
       "      <td>1696</td>\n",
       "      <td>631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.8</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>9</td>\n",
       "      <td>9.9</td>\n",
       "      <td>20.3</td>\n",
       "      <td>124194</td>\n",
       "      <td>176424</td>\n",
       "      <td>Manhattan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      zipcode  5_years_or_less  5-9_years  10-14_years  15-19_years  \\\n",
       "2558    10001            23537        562         1089          900   \n",
       "2559    10002            80736       3079         3171         3638   \n",
       "2560    10003            57112       1545          911          764   \n",
       "2561    10004             3221        126          101          161   \n",
       "2562    10005             8131        299          250          140   \n",
       "\n",
       "      20-24_years  25-34_years  35-44_years  45-54_years  55-59_years  ...  \\\n",
       "2558         1053         2785         4970         3905         2795  ...   \n",
       "2559         3922         5338        13938        12055        11182  ...   \n",
       "2560         7308         5714        15565         7843         5718  ...   \n",
       "2561          227          255          857          572          387  ...   \n",
       "2562           85         1101         3690         1696          631  ...   \n",
       "\n",
       "      $15,000-$24,999  $25,000-$34,999  $35,000-$49,999  $50,000-$64,999  \\\n",
       "2558              4.1              9.3              5.1              9.2   \n",
       "2559             12.6               14              8.8               10   \n",
       "2560              1.9                7              4.9              6.4   \n",
       "2561              0.3              1.7              6.5              5.1   \n",
       "2562              0.8              4.5              1.9              1.4   \n",
       "\n",
       "      $65,000-$74,999  $75,000-$99,999 $100,000_or_more  \\\n",
       "2558              8.5              9.8             27.5   \n",
       "2559             14.2              7.1             31.3   \n",
       "2560               13             11.3             30.7   \n",
       "2561              7.1             14.8               27   \n",
       "2562                9              9.9             20.3   \n",
       "\n",
       "     median_household_income mean_household_income    borough  \n",
       "2558                   86801                158183  Manhattan  \n",
       "2559                   33726                 61946  Manhattan  \n",
       "2560                   98151                161692  Manhattan  \n",
       "2561                  119691                177262  Manhattan  \n",
       "2562                  124194                176424  Manhattan  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "democlean['borough'] = democlean['zipcode'].map(zipdict)\n",
    "democlean = democlean.dropna(how='any',axis=0) \n",
    "democlean['borough'] = democlean['borough'].str[0]\n",
    "\n",
    "democlean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>types</th>\n",
       "      <th>zipcode</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48476</th>\n",
       "      <td>new york city</td>\n",
       "      <td>5161e3e3db931a8574e44c063f4ed8647375e8a9</td>\n",
       "      <td>40.669737</td>\n",
       "      <td>-73.842051</td>\n",
       "      <td>Boulevard Discount</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['home_goods_store', 'store', 'point_of_intere...</td>\n",
       "      <td>11417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59758</th>\n",
       "      <td>new york city</td>\n",
       "      <td>0bce4e42ae30780b6da1c24edf56a2070a041e7e</td>\n",
       "      <td>40.746251</td>\n",
       "      <td>-73.913639</td>\n",
       "      <td>Discount Variety &amp; Grocery.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['grocery_or_supermarket', 'food', 'store', 'p...</td>\n",
       "      <td>11377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59855</th>\n",
       "      <td>new york city</td>\n",
       "      <td>b113763e103d2d45836b1f6c92b8a4d1d39867fb</td>\n",
       "      <td>40.767347</td>\n",
       "      <td>-73.911814</td>\n",
       "      <td>TANJAWI, Hallal Food Emporium Inc</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['food', 'store', 'point_of_interest', 'establ...</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59847</th>\n",
       "      <td>new york city</td>\n",
       "      <td>5a005af8307213bfcf140bbe3536770010c20679</td>\n",
       "      <td>40.766207</td>\n",
       "      <td>-73.913381</td>\n",
       "      <td>Watany Food Market</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['grocery_or_supermarket', 'food', 'store', 'p...</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59840</th>\n",
       "      <td>new york city</td>\n",
       "      <td>ce4779dd89bb0912874f1f8e7795adb94bc54474</td>\n",
       "      <td>40.766203</td>\n",
       "      <td>-73.913372</td>\n",
       "      <td>Steinway Pharmacy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['pharmacy', 'health', 'store', 'point_of_inte...</td>\n",
       "      <td>11103</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                city                                        id   latitude  \\\n",
       "48476  new york city  5161e3e3db931a8574e44c063f4ed8647375e8a9  40.669737   \n",
       "59758  new york city  0bce4e42ae30780b6da1c24edf56a2070a041e7e  40.746251   \n",
       "59855  new york city  b113763e103d2d45836b1f6c92b8a4d1d39867fb  40.767347   \n",
       "59847  new york city  5a005af8307213bfcf140bbe3536770010c20679  40.766207   \n",
       "59840  new york city  ce4779dd89bb0912874f1f8e7795adb94bc54474  40.766203   \n",
       "\n",
       "       longitude                               name  rating  \\\n",
       "48476 -73.842051                 Boulevard Discount     5.0   \n",
       "59758 -73.913639        Discount Variety & Grocery.     5.0   \n",
       "59855 -73.911814  TANJAWI, Hallal Food Emporium Inc     5.0   \n",
       "59847 -73.913381                 Watany Food Market     5.0   \n",
       "59840 -73.913372                  Steinway Pharmacy     5.0   \n",
       "\n",
       "                                                   types zipcode  \n",
       "48476  ['home_goods_store', 'store', 'point_of_intere...   11417  \n",
       "59758  ['grocery_or_supermarket', 'food', 'store', 'p...   11377  \n",
       "59855  ['food', 'store', 'point_of_interest', 'establ...   11103  \n",
       "59847  ['grocery_or_supermarket', 'food', 'store', 'p...   11103  \n",
       "59840  ['pharmacy', 'health', 'store', 'point_of_inte...   11103  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from geopy.geocoders import Nominatim\n",
    "from geopy.point import Point\n",
    "import geopy\n",
    "\n",
    "def get_zip_code(x):\n",
    "    geolocator = geopy.Nominatim(user_agent=\"check_1\")\n",
    "    location = geolocator.reverse(\"{}, {}\".format(x['latitude'],x['longitude']))\n",
    "    return location.raw['address']['postcode']\n",
    "venu = venuescl.sort_values(by=['rating'], ascending=False)\n",
    "venu = venu.head(50)\n",
    "venu['zipcode'] = venu.apply(lambda x: get_zip_code(x), axis = 1)\n",
    "venu.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>types</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48476</th>\n",
       "      <td>new york city</td>\n",
       "      <td>5161e3e3db931a8574e44c063f4ed8647375e8a9</td>\n",
       "      <td>40.669737</td>\n",
       "      <td>-73.842051</td>\n",
       "      <td>Boulevard Discount</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['home_goods_store', 'store', 'point_of_intere...</td>\n",
       "      <td>11417</td>\n",
       "      <td>[Queens]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59758</th>\n",
       "      <td>new york city</td>\n",
       "      <td>0bce4e42ae30780b6da1c24edf56a2070a041e7e</td>\n",
       "      <td>40.746251</td>\n",
       "      <td>-73.913639</td>\n",
       "      <td>Discount Variety &amp; Grocery.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['grocery_or_supermarket', 'food', 'store', 'p...</td>\n",
       "      <td>11377</td>\n",
       "      <td>[Queens]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59855</th>\n",
       "      <td>new york city</td>\n",
       "      <td>b113763e103d2d45836b1f6c92b8a4d1d39867fb</td>\n",
       "      <td>40.767347</td>\n",
       "      <td>-73.911814</td>\n",
       "      <td>TANJAWI, Hallal Food Emporium Inc</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['food', 'store', 'point_of_interest', 'establ...</td>\n",
       "      <td>11103</td>\n",
       "      <td>[Queens]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59847</th>\n",
       "      <td>new york city</td>\n",
       "      <td>5a005af8307213bfcf140bbe3536770010c20679</td>\n",
       "      <td>40.766207</td>\n",
       "      <td>-73.913381</td>\n",
       "      <td>Watany Food Market</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['grocery_or_supermarket', 'food', 'store', 'p...</td>\n",
       "      <td>11103</td>\n",
       "      <td>[Queens]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59840</th>\n",
       "      <td>new york city</td>\n",
       "      <td>ce4779dd89bb0912874f1f8e7795adb94bc54474</td>\n",
       "      <td>40.766203</td>\n",
       "      <td>-73.913372</td>\n",
       "      <td>Steinway Pharmacy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['pharmacy', 'health', 'store', 'point_of_inte...</td>\n",
       "      <td>11103</td>\n",
       "      <td>[Queens]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                city                                        id   latitude  \\\n",
       "48476  new york city  5161e3e3db931a8574e44c063f4ed8647375e8a9  40.669737   \n",
       "59758  new york city  0bce4e42ae30780b6da1c24edf56a2070a041e7e  40.746251   \n",
       "59855  new york city  b113763e103d2d45836b1f6c92b8a4d1d39867fb  40.767347   \n",
       "59847  new york city  5a005af8307213bfcf140bbe3536770010c20679  40.766207   \n",
       "59840  new york city  ce4779dd89bb0912874f1f8e7795adb94bc54474  40.766203   \n",
       "\n",
       "       longitude                               name  rating  \\\n",
       "48476 -73.842051                 Boulevard Discount     5.0   \n",
       "59758 -73.913639        Discount Variety & Grocery.     5.0   \n",
       "59855 -73.911814  TANJAWI, Hallal Food Emporium Inc     5.0   \n",
       "59847 -73.913381                 Watany Food Market     5.0   \n",
       "59840 -73.913372                  Steinway Pharmacy     5.0   \n",
       "\n",
       "                                                   types  zipcode   borough  \n",
       "48476  ['home_goods_store', 'store', 'point_of_intere...    11417  [Queens]  \n",
       "59758  ['grocery_or_supermarket', 'food', 'store', 'p...    11377  [Queens]  \n",
       "59855  ['food', 'store', 'point_of_interest', 'establ...    11103  [Queens]  \n",
       "59847  ['grocery_or_supermarket', 'food', 'store', 'p...    11103  [Queens]  \n",
       "59840  ['pharmacy', 'health', 'store', 'point_of_inte...    11103  [Queens]  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venu['zipcode']  = venu['zipcode'].astype(int)\n",
    "venu['borough'] = venu['zipcode'].map(zipdict)\n",
    "venu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>types</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>48476</th>\n",
       "      <td>new york city</td>\n",
       "      <td>Boulevard Discount</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['home_goods_store', 'store', 'point_of_intere...</td>\n",
       "      <td>11417</td>\n",
       "      <td>Queens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59758</th>\n",
       "      <td>new york city</td>\n",
       "      <td>Discount Variety &amp; Grocery.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['grocery_or_supermarket', 'food', 'store', 'p...</td>\n",
       "      <td>11377</td>\n",
       "      <td>Queens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59855</th>\n",
       "      <td>new york city</td>\n",
       "      <td>TANJAWI, Hallal Food Emporium Inc</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['food', 'store', 'point_of_interest', 'establ...</td>\n",
       "      <td>11103</td>\n",
       "      <td>Queens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59847</th>\n",
       "      <td>new york city</td>\n",
       "      <td>Watany Food Market</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['grocery_or_supermarket', 'food', 'store', 'p...</td>\n",
       "      <td>11103</td>\n",
       "      <td>Queens</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59840</th>\n",
       "      <td>new york city</td>\n",
       "      <td>Steinway Pharmacy</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['pharmacy', 'health', 'store', 'point_of_inte...</td>\n",
       "      <td>11103</td>\n",
       "      <td>Queens</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                city                               name  rating  \\\n",
       "48476  new york city                 Boulevard Discount     5.0   \n",
       "59758  new york city        Discount Variety & Grocery.     5.0   \n",
       "59855  new york city  TANJAWI, Hallal Food Emporium Inc     5.0   \n",
       "59847  new york city                 Watany Food Market     5.0   \n",
       "59840  new york city                  Steinway Pharmacy     5.0   \n",
       "\n",
       "                                                   types  zipcode borough  \n",
       "48476  ['home_goods_store', 'store', 'point_of_intere...    11417  Queens  \n",
       "59758  ['grocery_or_supermarket', 'food', 'store', 'p...    11377  Queens  \n",
       "59855  ['food', 'store', 'point_of_interest', 'establ...    11103  Queens  \n",
       "59847  ['grocery_or_supermarket', 'food', 'store', 'p...    11103  Queens  \n",
       "59840  ['pharmacy', 'health', 'store', 'point_of_inte...    11103  Queens  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop city, state, metro, county, size_rank columns\n",
    "venu  = venu.drop(['id', 'latitude', 'longitude'], axis=1) \n",
    "#remove square brackets from start and end of borough column\n",
    "venu['borough'] = venu['borough'].str[0]\n",
    "venu.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>5_years_or_less</th>\n",
       "      <th>5-9_years</th>\n",
       "      <th>10-14_years</th>\n",
       "      <th>15-19_years</th>\n",
       "      <th>20-24_years</th>\n",
       "      <th>25-34_years</th>\n",
       "      <th>35-44_years</th>\n",
       "      <th>45-54_years</th>\n",
       "      <th>55-59_years</th>\n",
       "      <th>60-64_years</th>\n",
       "      <th>65-74_years</th>\n",
       "      <th>75-84_years</th>\n",
       "      <th>85_years_or_more</th>\n",
       "      <th>households</th>\n",
       "      <th>$9,999_or_less</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean_household_income</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53051.840000</th>\n",
       "      <td>10463.000000</td>\n",
       "      <td>57202.080000</td>\n",
       "      <td>4342.480000</td>\n",
       "      <td>4015.320000</td>\n",
       "      <td>3977.320000</td>\n",
       "      <td>4169.800000</td>\n",
       "      <td>4844.560000</td>\n",
       "      <td>8631.880000</td>\n",
       "      <td>7373.520000</td>\n",
       "      <td>7617.760000</td>\n",
       "      <td>3213.800000</td>\n",
       "      <td>2658.760000</td>\n",
       "      <td>3565.000000</td>\n",
       "      <td>1956.720000</td>\n",
       "      <td>835.160000</td>\n",
       "      <td>19534.760000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72558.297297</th>\n",
       "      <td>11220.297297</td>\n",
       "      <td>70142.135135</td>\n",
       "      <td>5227.270270</td>\n",
       "      <td>4450.918919</td>\n",
       "      <td>4177.945946</td>\n",
       "      <td>4147.702703</td>\n",
       "      <td>5228.918919</td>\n",
       "      <td>12469.972973</td>\n",
       "      <td>9684.486486</td>\n",
       "      <td>8729.729730</td>\n",
       "      <td>4051.729730</td>\n",
       "      <td>3615.000000</td>\n",
       "      <td>4612.540541</td>\n",
       "      <td>2544.918919</td>\n",
       "      <td>1201.000000</td>\n",
       "      <td>25183.405405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80195.033898</th>\n",
       "      <td>11378.111111</td>\n",
       "      <td>36518.000000</td>\n",
       "      <td>2275.555556</td>\n",
       "      <td>2021.396825</td>\n",
       "      <td>1946.206349</td>\n",
       "      <td>2020.714286</td>\n",
       "      <td>2576.222222</td>\n",
       "      <td>5960.587302</td>\n",
       "      <td>5248.365079</td>\n",
       "      <td>5196.968254</td>\n",
       "      <td>2398.412698</td>\n",
       "      <td>2029.904762</td>\n",
       "      <td>2654.444444</td>\n",
       "      <td>1495.698413</td>\n",
       "      <td>693.523810</td>\n",
       "      <td>12332.698413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87952.750000</th>\n",
       "      <td>10307.076923</td>\n",
       "      <td>36344.692308</td>\n",
       "      <td>2142.076923</td>\n",
       "      <td>2244.230769</td>\n",
       "      <td>2339.923077</td>\n",
       "      <td>2328.846154</td>\n",
       "      <td>2498.307692</td>\n",
       "      <td>4678.000000</td>\n",
       "      <td>4823.846154</td>\n",
       "      <td>5400.076923</td>\n",
       "      <td>2549.000000</td>\n",
       "      <td>2233.538462</td>\n",
       "      <td>2945.153846</td>\n",
       "      <td>1486.769231</td>\n",
       "      <td>674.923077</td>\n",
       "      <td>12752.615385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146706.093023</th>\n",
       "      <td>10083.940299</td>\n",
       "      <td>23344.940299</td>\n",
       "      <td>1184.507463</td>\n",
       "      <td>880.238806</td>\n",
       "      <td>845.985075</td>\n",
       "      <td>1051.492537</td>\n",
       "      <td>1822.820896</td>\n",
       "      <td>5286.089552</td>\n",
       "      <td>3429.179104</td>\n",
       "      <td>2968.447761</td>\n",
       "      <td>1336.164179</td>\n",
       "      <td>1298.686567</td>\n",
       "      <td>1781.701493</td>\n",
       "      <td>983.611940</td>\n",
       "      <td>476.014925</td>\n",
       "      <td>10687.537313</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            zipcode  5_years_or_less    5-9_years  \\\n",
       "mean_household_income                                               \n",
       "53051.840000           10463.000000     57202.080000  4342.480000   \n",
       "72558.297297           11220.297297     70142.135135  5227.270270   \n",
       "80195.033898           11378.111111     36518.000000  2275.555556   \n",
       "87952.750000           10307.076923     36344.692308  2142.076923   \n",
       "146706.093023          10083.940299     23344.940299  1184.507463   \n",
       "\n",
       "                       10-14_years  15-19_years  20-24_years  25-34_years  \\\n",
       "mean_household_income                                                       \n",
       "53051.840000           4015.320000  3977.320000  4169.800000  4844.560000   \n",
       "72558.297297           4450.918919  4177.945946  4147.702703  5228.918919   \n",
       "80195.033898           2021.396825  1946.206349  2020.714286  2576.222222   \n",
       "87952.750000           2244.230769  2339.923077  2328.846154  2498.307692   \n",
       "146706.093023           880.238806   845.985075  1051.492537  1822.820896   \n",
       "\n",
       "                        35-44_years  45-54_years  55-59_years  60-64_years  \\\n",
       "mean_household_income                                                        \n",
       "53051.840000            8631.880000  7373.520000  7617.760000  3213.800000   \n",
       "72558.297297           12469.972973  9684.486486  8729.729730  4051.729730   \n",
       "80195.033898            5960.587302  5248.365079  5196.968254  2398.412698   \n",
       "87952.750000            4678.000000  4823.846154  5400.076923  2549.000000   \n",
       "146706.093023           5286.089552  3429.179104  2968.447761  1336.164179   \n",
       "\n",
       "                       65-74_years  75-84_years  85_years_or_more  \\\n",
       "mean_household_income                                               \n",
       "53051.840000           2658.760000  3565.000000       1956.720000   \n",
       "72558.297297           3615.000000  4612.540541       2544.918919   \n",
       "80195.033898           2029.904762  2654.444444       1495.698413   \n",
       "87952.750000           2233.538462  2945.153846       1486.769231   \n",
       "146706.093023          1298.686567  1781.701493        983.611940   \n",
       "\n",
       "                        households  $9,999_or_less  \n",
       "mean_household_income                               \n",
       "53051.840000            835.160000    19534.760000  \n",
       "72558.297297           1201.000000    25183.405405  \n",
       "80195.033898            693.523810    12332.698413  \n",
       "87952.750000            674.923077    12752.615385  \n",
       "146706.093023           476.014925    10687.537313  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "democlean['mean_household_income']  = pd.to_numeric(democlean['mean_household_income'] , errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>households</th>\n",
       "      <th>mean_household_income</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>borough</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Bronx</th>\n",
       "      <td>835.160000</td>\n",
       "      <td>53051.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Brooklyn</th>\n",
       "      <td>1201.000000</td>\n",
       "      <td>72558.297297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Manhattan</th>\n",
       "      <td>476.014925</td>\n",
       "      <td>146706.093023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Queens</th>\n",
       "      <td>693.523810</td>\n",
       "      <td>80195.033898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Staten</th>\n",
       "      <td>674.923077</td>\n",
       "      <td>87952.750000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            households  mean_household_income\n",
       "borough                                      \n",
       "Bronx       835.160000           53051.840000\n",
       "Brooklyn   1201.000000           72558.297297\n",
       "Manhattan   476.014925          146706.093023\n",
       "Queens      693.523810           80195.033898\n",
       "Staten      674.923077           87952.750000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = democlean.groupby(['borough']).mean()\n",
    "d = d.drop(['20-24_years', '5_years_or_less', '5-9_years','10-14_years','15-19_years','25-34_years','35-44_years','45-54_years'], axis=1) \n",
    "d = d.drop(['55-59_years', '60-64_years', '65-74_years','75-84_years','85_years_or_more','zipcode','$9,999_or_less',], axis=1) \n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: (5 min)\n",
    "\n",
    "Conduct an exploratory analysis of the sizes of reviews: find the shortest and longest reviews, then plot a histogram showing the distribution of review lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text visualization with word clouds (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like visualization is crucial for standard CSV data, it is also important for text data. But text doesn't lend itself to histograms or scatterplots the way that numerical or even categorical data do. In such cases, **word clouds** are a common and <font color=\"orange\">sometimes</font> useful tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** While wordclouds can be a useful way of quickly gaining high level insights into raw textual data, they are also limited. In some ways, they can be seen as the pie charts of NLP: often used, but also often hated. [Some](https://www.niemanlab.org/2011/10/word-clouds-considered-harmful/) [people](https://towardsdatascience.com/can-we-please-stop-using-word-clouds-eca2bbda7b9d) would prefer if they didn't exist at all. If used in the correct way, however, they definitely deserve their place in a data scientist's toolbelt.\n",
    "\n",
    "The main problem with word clouds is that they are difficult to interpret in a standard way. The layout algorithm has some randomness involved and although more common words are shown more prominently, it's not possible to look at a word cloud and know which words are the most important, or how much more important these are than other words. Colours and rotation are also used randomly, making some words (e.g the ones in bright colours, positioned closer to the centre, with horizontal rotation) seem more important when in fact they are no more important than other words which were randomly assigned a less noticeable combination of color, rotation, and position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: (7 min)\n",
    "\n",
    "Write a function `word_cloud_rating(data, star_value)` that constructs a word cloud from the subset of `data` that exhibit a certain `star_value`. Visualize the results of this function for 1-star reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the resolution for better clarity \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 30, 60\n",
    "\n",
    "def word_cloud_rating(data,star_value):\n",
    "    \n",
    "    data_filtered = data[data.stars == star_value] #filtering according to the star value\n",
    "    Reviews = data_filtered.text\n",
    "\n",
    "    Reviews_text = ' '.join(Reviews.values) #joining all the words together\n",
    "\n",
    "    # Creating a word cloud object\n",
    "    wordcloud = WordCloud(max_font_size=100, max_words=100, background_color=\"white\",\\\n",
    "                          scale = 10,width=800, height=400).generate(Reviews_text)\n",
    "\n",
    "\n",
    "    # Plotting the generated word cloud\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_rating(data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: (5 min)\n",
    "\n",
    "The word \"good\" seems to appear quite frequently in the negative reviews. Investigate why that is and come up with a reasonable explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Answer.** Let's look at the first 5 reviews or so with 1-star ratings to see if there are any discernible patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_containing_good = [each for each in data[data.stars == 1].text if 'good' in each]\n",
    "for review in reviews_containing_good[:20]:\n",
    "    good_index = review.find(\"good\")\n",
    "    print(review[good_index-20:good_index+20].replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading each of the reviews, it is clear that \"good\" is often mentioned in a context like \"no good place to sit\" or \"sound good\". This indicates that in the world of text we cannot go by single words (also called **1-grams**) alone. The context of the sentence or the surrounding words at least are very much necessary to understand the sentiment of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams (25 min)\n",
    "\n",
    "Since 1-grams are insufficient to understand the significance of certain words in our text, it is natural to consider blocks of words, or **n-grams**.\n",
    "\n",
    "The simplest version of the n-gram model, for $n > 1$, is the **bigram** model, which looks at pairs of consecutive words. For example, the sentence \"The quick brown fox jumps over the lazy dog\" would have tokens \"the quick\", \"quick brown\",..., \"lazy dog\". The following image explains this concept:\n",
    "\n",
    "<img src=\"ngrams.png\" alt=\"ngrams\" width=\"500\"/>\n",
    "\n",
    "This has obvious advantages and disadvantages over looking at words individually:\n",
    "\n",
    "1. This retains the structure of the overall document, and\n",
    "2. It paves the way for analyzing words in contex; however,\n",
    "3. The dimension is vastly larger\n",
    "\n",
    "In practice, this last challenge can be truly daunting. As an example, *War and Peace* has 3 million characters, which translates to several hundred thousand 1-grams (words). If you consider that the set of all possible bigrams can be as large as the square of the number of 1-grams, this gets us to a hundred billion possible bigrams! If classical ML techniques are not suitable for training on 3 million characters, how can they possibly deal with a hundred billion dimensions?\n",
    "\n",
    "For this reason, it is often prudent to start by extracting as much value out of 1-grams as possible, before working our way up to more complex structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we also start to look again at our main application: calculating some \"interesting\" features of our corpus of reviews.\n",
    "\n",
    "When thinking about word analysis, the main topic of interest is finding an *efficient* and *low-dimensional* representation in order to facilitate document visualization and larger-scale analyses. We discuss two broad categories of word representations:\n",
    "\n",
    "1. `Count-based representations`: word-word and word-document matrices.\n",
    "2. `Word embeddings`: spectral embedding, UMAP, word2vec, GloVe, and many many more.\n",
    "\n",
    "These are often used to assist with downstream tasks such as clustering, ranking and labeling, which will be briefly discussed in a future case. Word embeddings in particular have become something of a posterchild. These, combined with neural networks (which will also be discussed in a future case!), have led to many of the recent headline improvements in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count-based representations (20 min)\n",
    "\n",
    "n-grams fall under a broader category of techniques otherwise known as [**count-based representations**](https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage). These are techniques to analyze documents by indicating how frequently certain types of structures occur throughout.\n",
    "\n",
    "Let's start with 1-grams (words). The simplest type of information would be whether a particular word occurs in particular documents. This leads to **word-document co-occurrence matrices**, where the $(W, X)$ entry of the word-document matrix is set to 1 if word $W$ occurs in document $X$, and 0 otherwise.\n",
    "\n",
    "There are many variants of this. In lieu of the fact that we are looking for count-based representations of our documents, one natural variable is the following: the $(W, X)$ entry of the word-document matrix equals the number of times that word $W$ occurs in document $X$, rather than merely being a binary variable.\n",
    "\n",
    "Let's create a word-document co-occurrence matrix for our set of reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code creates a word-document matrix.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(AllReviews)\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: (5 min)\n",
    "\n",
    "Find all the high-frequency (top 1%) and low-frequency (bottom 1%) words in the reviews overall. (Hint: import the `Counter()` function from the `collections` class.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_reviews_text = ' '.join(AllReviews)\n",
    "tokenized_words = nltk.word_tokenize(all_reviews_text)\n",
    "word_freq = Counter(tokenized_words)\n",
    "\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Therefore, top 1% is ~463 words\n",
    "word_freq.most_common(463)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similarly, bottom 1% is ~463 words\n",
    "word_freq.most_common()[-463:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for bigrams. Here is the code to get the set of bigrams for the first 5 reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "first_5_revs = data.text[0:5]\n",
    "word_tokens = nltk.word_tokenize(''.join(first_5_revs))\n",
    "list(ngrams(word_tokens, 2)) #ngrams(word_tokens,n) gives the n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: (12 min)\n",
    "\n",
    "Write a function called `top_k_ngrams(word_tokens, n, k)` for printing out the top $k$ n-grams. Use this function to get the top 10 1-grams, 2-grams, and 3-grams from the first 1000 reviews in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def top_k_ngrams(word_tokens,n,k):\n",
    "    \n",
    "    ## Getting them as n-grams\n",
    "    n_gram_list = list(ngrams(word_tokens, n))\n",
    "\n",
    "    ### Getting each n-gram as a separate string\n",
    "    n_gram_strings = [' '.join(each) for each in n_gram_list]\n",
    "    \n",
    "    n_gram_counter = Counter(n_gram_strings)\n",
    "    most_common_k = n_gram_counter.most_common(k)\n",
    "    print(most_common_k)\n",
    "    \n",
    "   # x_pos = [k for k,v in most_common_k]\n",
    "   # y_pos = [v for k,v in most_common_k]\n",
    "    \n",
    "   # plt.bar(x_pos, y_pos,align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting a single string\n",
    "top_1000_reviews = data.text[0:1000]\n",
    "all_reviews_text = ' '.join(top_1000_reviews)\n",
    "\n",
    "## Splitting them into tokens\n",
    "word_tokens = nltk.word_tokenize(all_reviews_text)\n",
    "\n",
    "## Calling the function for top k\n",
    "top_k_ngrams(word_tokens, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_ngrams(word_tokens, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_ngrams(word_tokens, 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words (20 min)\n",
    "\n",
    "You may have noticed a pattern in the types of words that show up in the top 10 1-grams, 2-grams, and 3-grams. In particular, these are common words that appear in every sentence of the English language: pronoums like \"I\", prepositions like \"but\", \"of\", \"and\", articles like \"the\", etc. These very common words are usually uninformative, and their very large occurrence values can distort the results of many NLP algorithms.\n",
    "\n",
    "For this reason, it is common to pre-process text by removing words that you have a reason to believe are uninformative; these words are called [**stop words**](https://en.wikipedia.org/wiki/Stop_words). Usually, it suffices to simply treat extremely common words as stop words. However, for specific types of applications it might make sense to use other stop words; e.g. the word \"burger\" when analyzing reviews of burger chains.\n",
    "\n",
    "(Note that stop words are often removed by default as a cleaning step in all NLP tasks. However, sometimes they can be useful. For example in authorship attribution (automatically detecting who wrote a specific piece of text by their 'writing style'), stop words can be one of the most useful features, as they appear in nearly all texts, and yet each author uses them in slightly different ways.)\n",
    "\n",
    "The `nltk` library has a standard list of stopwords, which you can download by writing `nltk.download(“stopwords”)`. We can then load the stopwords package from the nltk.corpus and use it to load the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words(\"japanese\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a list of all the Spanish stop words as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words(\"spanish\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: (15 min)\n",
    "\n",
    "#### 7.1\n",
    "\n",
    "Filter out all of the stop words in the first review of the Yelp review data and print out your answer. Additionally, print out (separately) the stopwords you found in this review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "without_stop_words = []\n",
    "stopword = []\n",
    "sentence = data.text[0]\n",
    "words = nltk.word_tokenize(sentence)\n",
    "for word in words:\n",
    "    if word in stop_words:\n",
    "        stopword.append(word)\n",
    "    else:\n",
    "        without_stop_words.append(word)\n",
    "\n",
    "print(stopword)\n",
    "print()\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2\n",
    "\n",
    "Modify the function `top_k_ngrams(word_tokens, n, k)` to remove stop words before determining the top n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** Our recommended solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the most basic stop words from the ntlk corpus and including only those\n",
    "# words with character size above 2 so as to remove punctuations\n",
    "# But, this could be extended to remove further high and low frequency stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "### Getting a single string\n",
    "all_reviews_text = ' '.join(top_1000_reviews)\n",
    "\n",
    "## Splitting them into tokens\n",
    "word_tokens = nltk.word_tokenize(all_reviews_text)\n",
    "\n",
    "## Removing the stopwords\n",
    "word_tokens_clean = [each for each in word_tokens if each.lower() not in eng_stopwords and len(each.lower()) > 2]\n",
    "\n",
    "## Calling the function for top k\n",
    "top_k_ngrams(word_tokens_clean, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_ngrams(word_tokens_clean, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some contexts, it is common to remove both very common and very *uncommon* words. The idea is that common words like \"a\" are almost never informative, while uncommon words like \"syzygy\" occur so infrequently in a corpus that many algorithms have a hard time processing them in a meaningful way. We will not deal with uncommon words today, but you should be aware that doing so improves the performance of several NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding important words (30 min)\n",
    "\n",
    "Up to this point, we have focused on techniques for transforming our data. We are now ready to start looking for some answers, so let's take a break from discussing techniques so we can explore our dataset and various ways to summarize it.\n",
    "\n",
    "We begin by looking at the words and n-grams that are most common in positive and negative reviews. Note that in the following code, we don't reuse many of the pre-processing steps discussed at the start of the tutorial. This is because many of them are included as options in existing packages. In a serious project one would often customize this pre-processing to some degree, but we skip this in order to get some displayable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following code grabbed from:\n",
    "# https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n",
    "# we will use it in our context to create some visualizations.\n",
    "def get_top_n_words(corpus, n=1,k=1):\n",
    "    vec = CountVectorizer(ngram_range=(k,k),stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by getting a list of the most common words.\n",
    "\n",
    "common_words = get_top_n_words(data['text'], 20,1)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar',  title='Top 20 words from all reviews')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: (15 min)\n",
    "\n",
    "#### 8.1\n",
    "\n",
    "Divide the data into \"good reviews\" (i.e. `stars` rating was greater than 3) and \"bad reviews\" (i.e. `stars` rating was less than 3) and make a bar plot of the top 20 words in each case. Are these results different from above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We continue by splitting according to good/bad review scores, then grabbing again.\n",
    "\n",
    "GoodInd = data['stars'] >3.1\n",
    "GoodRev = data[GoodInd]\n",
    "BadInd = data['stars'] <2.1\n",
    "BadRev = data[BadInd]\n",
    "\n",
    "common_words = get_top_n_words(GoodRev['text'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 words from good reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(BadRev['text'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 words from bad reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was pretty useless. The \"good\" words are mostly a mix of generic words like \"place\" and overtly positive words like \"good\" itself.\n",
    "\n",
    "The problem here is that we are dealing with single words, which cannot convey much information out of context. The natural solution then is to deal with n-grams, so that we can get context-aware results like \"good burger\" or \"good service\" (in the positive reviews) or, as we saw, \"good 45 minutes\" (in the negative reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2\n",
    "\n",
    "Use the `get_top_n_words()` function to find the top 20 bigrams and trigrams. Do the results seem useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top bigrams and trigrams from bad reviews\n",
    "common_words = get_top_n_words(BadRev['text'], 20,2)\n",
    "\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 bigrams from bad reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top bigrams and trigrams from bad reviews\n",
    "common_words = get_top_n_words(GoodRev['text'], 20,2)\n",
    "\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 bigrams from bad reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(BadRev['text'], 20,3)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 trigrams from bad reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(GoodRev['text'], 20,3)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 bigrams from good reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(GoodRev['text'], 20,3)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 trigrams from good reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are some nonsense entries such as \"http www yelp\" this is starting to be helpful. We can see a few recurring themes among good reviews (e.g. \"staff friendly helpful\"). Nonsense entries are typically difficult to eliminate completely in NLP with user-generated text and smaller corpora. NLP is still useful *despite* the existence of nonsense results, and we should think of the output of NLP algorithms like this as a *screening tool* for finding important phrases rather than a *careful estimate* of the most important phrases. In other words, it's an application of machine intelligence to *conduct exploratory analysis* rather than to *build predictive models*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "Look at the 5 most important bigrams for bad reviews. What *single, specific* problem seems to be the most important driver of bad reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three of the top 5 bigrams were \"20 minutes\", \"15 minutes\", and \"10 minutes.\" These are all times, *strongly* suggesting that *waiting time for service* is a main driver for bad review scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: (10 min)\n",
    "\n",
    "#### 9.1\n",
    "\n",
    "You may have noticed that many of the important \"bad\" bigrams included the words \"like\" or \"just\" but didn't seem very informative (e.g. \"felt like\", \"food just\"). Give some ideas of how to use this sort of observation in future pre-processing of reviews, based on the pre-processing ideas we have already studied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** Two potential answers are (there are many others):\n",
    "\n",
    "1. Having recognized that these words go together in common bigrams, you could modify your algorithm so that it \"clumps\" these bigrams together; i.e. treats them as one word, so that your algorithm will focus on the words following that.\n",
    "2. Having recognized this as a key phrase, we could have a list of the most important words that follow these key phrases (which are presumably informative). This is more time-consuming as it requires human input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2\n",
    "\n",
    "Building on the previous question, we note that most of the most important complaints and compliments can't be *completely* observed by looking at bigrams or trigrams. This can often be fixed by small modifications. Do the following:\n",
    "\n",
    "1. Write down a complaint that is unlikely to be (completely) picked up by bigram analysis. Hint: what might you write if your hamburger was served cold?\n",
    "2. Write down a processing step that would fix this problem. Try to find a solution that would work for several similar problems without additional human input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** There are many good answers, but we focus on a simple one:\n",
    "\n",
    "1. I would probably write \"the burger was cold\" or \"the burger was served cold.\" If this were a common complaint, the most-important bigrams might include \"was cold\" or \"served cold,\" which doesn't tell me *what* was served cold.\n",
    "2. A simple fix, along the lines of the previous question, would be to \"clump\" words like \"was cold\" together. However, I think this is a bad fix, as it focuses too much on the word \"cold\" and would require a great deal of hand tuning. A better idea would be to recognize that words like \"was\" are always going to be a problem in this context, as they separate the important noun describing the subject (\"burger\") from the adjective describing the problem (\"cold\"). This suggests that we should take a *much* more aggressive stance towards removing stop words. This should certainly include conjugations of \"to be,\" and likely many other common but uninformative words (like \"too\").\n",
    "\n",
    "This second response is an important takeaway for NLP – this sort of problem is extremely common, and a great deal of time is often spent tweaking initial pre-processing rules. In the final part of this case, you will learn about a method that can help systematically deal with these uninformative stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions (25 min)\n",
    "\n",
    "Having spent a lot of time on n-grams and how to featurize a document using them, we now take a break from `nltk` tools to introduce the most important text wrangling tool in Python (and many other languages): [**regular expressions**](https://en.wikipedia.org/wiki/Regular_expression).\n",
    "\n",
    "The basic idea here is that you often want to perform some specific transformation (e.g. delete or substitute) every time that some possibly-complicated pattern (e.g. the letter 'A', the word 'hello', any word containing the letters 'a','r' in that order) occurs. Regular expressions are a compact and powerful language for expressing these sorts of patterns. This is super important whenever you are trying to clean a text dataset that contains thematically similar, but not exactly, the same errors. \n",
    "\n",
    "The terse syntax of regular expressions has led to them having a reputation for being [almost magical](https://xkcd.com/208/) in some situations (with only a few characters, you can build complete computer programs) but also for being difficult to create and read, which can [create more problems](https://xkcd.com/1171/) than they solve.\n",
    "\n",
    "In Python, [the `re` module](https://docs.python.org/3/library/re.html) provides regular expression matching operations and common operations. Regular expressions are a deep subject, with some documentation here: https://docs.python.org/3/library/re.html?highlight=regex.\n",
    "\n",
    "As some simple examples, we have:\n",
    "\n",
    "1. `.` matches any character except \\n (newline)\n",
    "2. `\\d` matches any digit (this can also be written as [0-9])\n",
    "3. `\\D` matches any non-digit (this can also be written as [^0-9])\n",
    "4. `\\w` matches any alphanumeric character ([a-zA-Z0-9_])\n",
    "5. `\\W` matches any non-alphanumeric character ([^a-zA-Z0-9_])\n",
    "\n",
    "As some more complex examples, regular expressions also allow you to quantify the number of times matches can occur. For example,\n",
    "\n",
    "1. `[a-d]+` matches any time you get $\\{a,b,c,d\\}$ one or more times in a row\n",
    "2. `[a-d]{3}` matches any time you get them exactly 3 times in a row\n",
    "3. `[a-d]*` matches any time you get them 0 or more times in a row\n",
    "\n",
    "For now, we give a simple application based on the  `re.sub()` function, which substitutes words that match a pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentence = 'That was an \"interesting\" way to cook bread.'\n",
    "pattern = r\"[^\\w]\" # the ^ character denotes 'not', \n",
    "#                   the \\w character denotes a word, and []  means\n",
    "#                    anything that matches anything in the brackets. \n",
    "#                     Together, this refers to any character that is not a word.\n",
    "print(re.sub(pattern, \" \", sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"Natesh loves all the foold and loveds sdaslo\"\n",
    "x   = re.compile('lo')\n",
    "iterator = x.finditer(str)\n",
    "for item in iterator:\n",
    "    print(item.span())\n",
    "    print(item.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: (15 min)\n",
    "\n",
    "#### 10.1\n",
    "\n",
    "1. Use the `re.split()` function to split the first Yelp review into a list of its constituent words.\n",
    "2. Use the `re.findall()` function to search the first 30 reviews for the number of times they contain the word \"food\". Print the maximum number of times the word \"food\" is mentioned in a single review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r'\\s', AllReviews.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_count = []\n",
    "for sentence in AllReviews.values:\n",
    "    temp = len(re.findall('food', sentence))\n",
    "    food_count.append(temp)\n",
    "print(max(food_count[0:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2\n",
    "\n",
    "Using regular expressions, find the percentage of reviews in top 500 reviews that have numbers in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Considering the top 500 reviews for this analysis\n",
    "top_500_reviews = AllReviews.values[:500]\n",
    "reviews_nos_regex = []\n",
    "\n",
    "for each_review in top_500_reviews:\n",
    "    number_list = re.findall('\\d',each_review)\n",
    "    \n",
    "    ## number list returns all the possible digits in a review\n",
    "    ## Look if the number list is empty - if so, the review has no digits in them\n",
    "    if(len(number_list)) > 0:\n",
    "        reviews_nos_regex.append(each_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews_nos_regex)/len(top_500_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is clear from above, regular expressions are very useful for extracting more general properties of text. These properties are not as informative or context-aware as n-grams can be, but they are much simpler to code and therefore can often serve as the first step of an EDA on text data.\n",
    "\n",
    "Although regular expressions usually cannot tell us much about context overall, they *can* be used to find specific instances of words in context. For example, we may be interested in finding the first word following \"good\" or \"bad\" in a review (which can help us distinguish a positive from a negative review). Let's write some code that finds the first word following \"good\" in the sentence \"hello I want a good burger, please.\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"hello I want a good burger, please\"\n",
    "\n",
    "# Find everything after \"good\", including \"good\"\n",
    "\n",
    "post = re.findall(r'good.*', sample)[0]\n",
    "\n",
    "print(post)\n",
    "\n",
    "# Take just the first word after \"good\"\n",
    "\n",
    "first_post = re.split(r'\\s',post)[1]\n",
    "\n",
    "print(first_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.3\n",
    "\n",
    "Using the above as a template, write a generalized function that can extract the first word following \"good\". Don't forget to include a default behavior for when the word doesn't appear in the sentence. Run this function for all reviews and print the first 300 results for reviews that do contain the word \"good\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(sentence):\n",
    "    post = re.findall(r'good.*', sentence)\n",
    "    if (len(post) > 0):\n",
    "        temp = re.split(r'\\s',post[0])\n",
    "        if (len(temp) > 1):\n",
    "            return(temp[1])\n",
    "        else:\n",
    "            return('')\n",
    "    else:\n",
    "        return('')\n",
    "    \n",
    "    \n",
    "print(next_word(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_good = []\n",
    "ind = 0\n",
    "for sentence in AllReviews.values:\n",
    "    temp = next_word(sentence)\n",
    "    post_good.append(temp)\n",
    "    \n",
    "nonempty = [i for i in post_good if i] \n",
    "print(nonempty[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skimming over the results of the previous exercise, a few things stood out:\n",
    "\n",
    "1. People like to talk about good burgers – this appeared 5 times in the first 300 results.\n",
    "2. A large number of the results are useless. One common problem is the occurrence of a sentence boundary; e.g. \"good. The\" near the start of the list. In this case, we should look *before* the word \"good\" rather than after. However, we can't look *immediately* before good – that word will usually be some conjugation of \"to be\", which is also not informative – rather, we need to look for a word before \"good\" that isn't too boring. Other times, there is a word following \"good\" that is uninformative; e.g. \"good for\" – we want to know *what* something was good for! In this case, we should keep on skimming *forward* until we see a word following \"good\" that isn't too boring.\n",
    "\n",
    "In both of these cases, we can't use simple regular expressions by themselves, as regular expressions don't know how to ignore \"boring\" words. Regular expressions can only help us filter for the structure of words, not the content they convey within a context. We *can* use what we learned about stop words to remove these from the reviews before conducting the above analysis, but as we have been, we will still sometimes get not very informative phrases like \"was cold\" or \"served cold\". So we will introduce an alternative method, which can be applied to serve as an even better remover of stop words: **part-of-speech tagging**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part-of-speech (POS) tagging (45 min)\n",
    "\n",
    "In English, there are eight main parts of speech - `nouns`, `pronouns`, `adjectives`, `verbs`, `adverbs`, `prepositions`, `conjunctions` and `interjections`. These are\n",
    "`sustantivos`, `pronombres`, `adjetivos`, `verbos`, `adverbios`, `preposiciones`, `conjunciones` and `interjecciones`, respectively, in Spanish. The purpose of POS tagging is to label each word in a document with its part of speech.\n",
    "\n",
    "Unsurprisingly, [POS tagging](http://www.nltk.org/book/ch05.html) can be very difficult to do by hand. `nltk` has a default function for this, called `nltk.pos_tag()`, which we will use. As a word of warning, this function is far from infallible, especially on informal text (e.g. website reviews, forum posts, text messages, etc), and words in English often exhibit POS drift (e.g. the drift of \"Google\" from noun to verb): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#https://www.nltk.org/book/ch05.html\n",
    "text_word_token = nltk.word_tokenize(\"Jairo is having a good day\")\n",
    "#text_word_token = nltk.word_tokenize(data.text[0])\n",
    "nltk.pos_tag(text_word_token)\n",
    "#https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I prefer to buy burgers\n",
    "prefer -> burger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_word_token = nltk.word_tokenize(\"We are going to Race\") # try \"Race can be both a verb and a noun\"\n",
    "#text_word_token = nltk.word_tokenize(data.text[0])\n",
    "nltk.pos_tag(text_word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.nltk.org/_modules/nltk/tag/perceptron.html\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides documentation for each tag, which can be queried using the tag itself; e.g. `nltk.help.upenn_tagset('RB')`. Since POS is context-sensitive, POS-taggers must usually be trained on an existing corpus that has been tagged by professional linguists (possibly alongside unlabeled data to take advantage of semi-supervised methods). The most popular tag set is called the Penn Treebank set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get more details about any POS tag using the help function of nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('CD$')\n",
    "nltk.help.upenn_tagset('NN$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11: (10 min)\n",
    "\n",
    "#### 11.1\n",
    "\n",
    "Write code to find the percentage of reviews in the first 500 reviews of the dataset that contains a number or a cardinal using POS taggings only. (Hint: POS tag `CD` is the indicator for cardinal or number.) How does this compare to the figure we extracted from using regular expressions only?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinal_review = []\n",
    "\n",
    "for sentence in top_500_reviews:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    cd_len = [k for k,v in nltk.pos_tag(words) if 'CD' == v]\n",
    "    \n",
    "    if len(cd_len) > 0:\n",
    "        cardinal_review.append(sentence)\n",
    "\n",
    "#### Proportion of reviews with a number/cardinal in it        \n",
    "len(cardinal_review)/len(top_500_reviews) \n",
    "## 56.6% of the reviews have a number/cardinal in the top 500 reviews. \n",
    "## You could improve the accuracy of this estimate by looking at more than 500 reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number is considerably higher than the one we got from using regular expressions only! The reason is because POS tagging can extract numbers in text form (e.g. \"one\", \"two\") whereas regular expressions cannot. This is one advantage of using POS tagging over regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2\n",
    "\n",
    "Extract all of the nouns from each review using POS tagging. This may be useful for later analysis. Even though words like \"good\" may be the most prevalent in good reviews, we think nouns like \"service\" or \"burgers\" are likely to be more informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_reviews = []\n",
    "for sentence in AllReviews.values:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    noun_pt = [k for k,v in nltk.pos_tag(words) if 'NN' == v]\n",
    "    noun_reviews.append(noun_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12: (15 min)\n",
    "\n",
    "Use POS tagging to find the first word following \"good\" that has an interesting POS tag. We leave this up to your discretion, but should probably include nouns and proper nouns. Inspecting the above, we think that cardinals are also almost certainly interesting: we recognize that \"good 45\" is probably followed by \"minutes\", definitely an important (though not \"good\") part of a review!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, a function that extracts only the \"interesting\" parts of speech\n",
    "\n",
    "sentence = \"This is a good burger, but I prefer pizza\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "interesting = [k for k,v in nltk.pos_tag(words) if v in ['CD','FW','NN','NNS','NNP','NNPS']]\n",
    "\n",
    "print(interesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_pos = ['CD','FW','NN','NNS','NNP','NNPS']\n",
    "\n",
    "def ExtractInteresting(sentence, good):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    interesting = [k for k,v in nltk.pos_tag(words) if v in good]\n",
    "    return(interesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, a function that extracts the first \"interesting\" word that follows \"good\"\n",
    "\n",
    "sentence = \"This is a good burger, but I prefer pizza\"\n",
    "post = re.findall(r'good.*', sentence)\n",
    "print(post)\n",
    "# Check that post isn't empty here before doing next line\n",
    "temp = ExtractInteresting(post[0],good_pos)\n",
    "print(temp)\n",
    "# Check that temp isn't empty here before doing next line\n",
    "print(post[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word2(sentence):\n",
    "    post = re.findall(r'good.*', sentence)\n",
    "    if (len(post) > 0):\n",
    "        temp = ExtractInteresting(post[0],good_pos)\n",
    "        if (len(temp) > 0):\n",
    "            return(temp[0])\n",
    "        else:\n",
    "            return('')\n",
    "    else:\n",
    "        return('')\n",
    "    \n",
    "    \n",
    "print(next_word2(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, apply this.\n",
    "\n",
    "post_good = []\n",
    "ind = 0\n",
    "for sentence in AllReviews.values:\n",
    "    temp = next_word2(sentence)\n",
    "    post_good.append(temp)\n",
    "    \n",
    "nonempty = [i for i in post_good if i] \n",
    "print(nonempty[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a much more interesting list!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13: (10 min)\n",
    "\n",
    "It is interesting to look specifically at Adjectives (which have a tag name of \"JJ\" in NLTK) when looking at reviews. We can hypothesise that good reviews and bad reviews might use very different adjectives, but that some adjectives might appear often in both good and bad reviews, as we saw with the word \"good\" previously.\n",
    "\n",
    "Use POS tags to extract all adjectives from the first 500 five star reviews and the first 500 one star reviews. Extract the most 30 most commonly used adjectives from each set of reviews and print out both. Make a note of several of these; say if they appear in one or both lists, and whether or not this was expected, and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_star_reviews = data[data['stars']==1]['text'][:500]\n",
    "five_star_reviews = data[data['stars']==5]['text'][:500]\n",
    "\n",
    "\n",
    "def extract_specific_pos(reviews, pos_tag):\n",
    "    results = [] \n",
    "    for review in reviews:\n",
    "        words = nltk.word_tokenize(review)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        keep = [x[0] for x in tagged if x[1] == pos_tag]\n",
    "        results += keep\n",
    "    return results\n",
    "\n",
    "\n",
    "negative_adjectives = extract_specific_pos(one_star_reviews, \"JJ\")\n",
    "positive_adjectives = extract_specific_pos(five_star_reviews, \"JJ\")\n",
    "\n",
    "print(Counter(negative_adjectives).most_common(30))\n",
    "print(Counter(positive_adjectives).most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw before, the word \"good\" appears in both lists, though slightly more often in good reviews. Surprisingly, it is the most common adjective used in *bad* reviews.\n",
    "\n",
    "Words like \"disappointed\", \"awful\", \"horrible\", and \"rude\" only appear frequently in bad reviews, as expected, while words like \"favorite\", \"great\", \"fantastic\", \"amazing\", \"wonderful\", and \"perfect\" appear in good reviews, which is also expected.\n",
    "\n",
    "Words like \"small\" and \"new\" appear frequently in in both sets of reviews. Interestingly, so does \"different\", but slightly more often in good reviews, perhaps indicating that people like variety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions (5 min)\n",
    "\n",
    "In this case, we focused on the basic components of an NLP pipeline, virtually all of which are frequently used *before* building a model for the business question of interest. We saw that every part of the pipeline was highly customizable, and discussed how parameters might vary depending on the specific application in mind. \n",
    "\n",
    "In addition to constructing a basic pipeline, we tried to give initial answers to a business question: \"Which factors are most important for bad reviews?\" The answers we obtained with this out-of-the-box analysis were not perfect, but they did seem to give some genuinely useful information. For example, 3 of the 5 most important phrases for bad reviews were \"20 minutes\", \"10 minutes\", and \"15 minutes\" – strong evidence that long service times were a major driver of bad reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways (7 min)\n",
    "\n",
    "Text pre-processing is more complex than other forms of pre-processing you might be familiar with, as good pre-processing may rely on an enormous number of rules extracted from large corpora of English (Spanish!) text. You shouldn't try to recreate this work by yourself; instead, take advantage of large and powerful libraries such as `nltk` which have built-in corpora when possible, and use regular expressions when necessary to extend or tweak them.\n",
    "\n",
    "Pre-processing is an extremely important and nontrivial part of NLP, and will likely take the bulk of the work for most NLP projects. Most popular parts of the pipeline come with many parameters. Yet they can give surprisingly useful summaries of entire corpora without much adjustment.\n",
    "\n",
    "Overall, NLP can be used in many situations, but it is perhaps most useful in its ability to turn qualitative data into quantitative data. For example, if we have a collection of reviews describing, qualitatively, people’s experiences at restaurants, we can derive quantitative insights such as “X% of people who left bad reviews were unhappy with the waiting time”.\n",
    "\n",
    "After following through this case, you know what NLP is and how it can be useful. You especially know\n",
    "\n",
    "* The challenges of NLP: context specificity and high dimensionality.\n",
    "* How to standardize and pre-process text before carrying out analysis, such as stemming \n",
    "* How to tokenize documents into sentences and words\n",
    "* How to create word clouds to quickly gain high-level insights into text\n",
    "* What n-grams are and how they can be created and used in analysis\n",
    "* Why common (“stop”) words should often be removed before analysis\n",
    "* How to find common words and n-grams\n",
    "* What regular expressions are and how to use them to carry out more custom analysis\n",
    "* What Parts of Speech tagging is and why analysing documents by their grammatical structure can be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk, wordcloud, spacy, nagisa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
