{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Project 1130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## *AirBnb DataSet Problem*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Goals (3 min)\n",
    "\n",
    "To understand and work with the AirBnb datasets to solve our business problem \n",
    "\n",
    "## Introduction (5 min)\n",
    "\n",
    "**Business Context.** We are a company looking to expand our ventures based off tourism.\n",
    "\n",
    "**Business Problem.** The main Task is  **wrangle datasets related to AirBnb and create a critera to figure out the areas to build in**.\n",
    "\n",
    "**Analytical Context.** Text data is highly unstructured, and often requires pre-processing before we can gather any business insights from it. We will be leveraging tools from **natural language processing (NLP)** in order to help us process this data and generate new features that can be used for analytics or model building.\n",
    "\n",
    "The case will proceed as follows: \n",
    "1. We will extract the files based off what they are\n",
    "2. We will clean the data \n",
    "3. Create bar graph and determine average age in each major zipcode \n",
    "4. Same as above but with economic demos \n",
    "5. Average Airbnb review per zip code /posyiieve airbnb reviews if they're text\n",
    "6. Average size of Airbnb per zip code \n",
    "7. Average cost of Airbnb per zip code \n",
    "8. Airbnb date availability\n",
    "9. Using all these graphs and our critera we will decide where to build"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\abdul\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk # imports the natural language toolkit\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import pandas as pd\n",
    "import numpy  as np\n",
    "import string\n",
    "import zipfile\n",
    "import plotly\n",
    "import tarfile\n",
    "import gzip\n",
    "from nltk.stem import PorterStemmer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOADING THE DATASETS\n",
    "demo = pd.read_csv('demographics.csv')\n",
    "econ = pd.read_csv('econ_state.csv')\n",
    "listings = pd.read_csv('listings.csv')\n",
    "\n",
    "realestate = pd.read_csv('real_estate.csv.gz', compression='gzip')\n",
    "\n",
    "venues = pd.read_csv('venues.csv.gz', compression='gzip')\n",
    "\n",
    "zf = zipfile.ZipFile('calendar.csv.zip') \n",
    "calendar = pd.read_csv(zf.open('calendar.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>5_years_or_less</th>\n",
       "      <th>5-9_years</th>\n",
       "      <th>10-14_years</th>\n",
       "      <th>15-19_years</th>\n",
       "      <th>20-24_years</th>\n",
       "      <th>25-34_years</th>\n",
       "      <th>35-44_years</th>\n",
       "      <th>45-54_years</th>\n",
       "      <th>55-59_years</th>\n",
       "      <th>...</th>\n",
       "      <th>$10,000-$14,999</th>\n",
       "      <th>$15,000-$24,999</th>\n",
       "      <th>$25,000-$34,999</th>\n",
       "      <th>$35,000-$49,999</th>\n",
       "      <th>$50,000-$64,999</th>\n",
       "      <th>$65,000-$74,999</th>\n",
       "      <th>$75,000-$99,999</th>\n",
       "      <th>$100,000_or_more</th>\n",
       "      <th>median_household_income</th>\n",
       "      <th>mean_household_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601</td>\n",
       "      <td>17982</td>\n",
       "      <td>1006</td>\n",
       "      <td>1080</td>\n",
       "      <td>1342</td>\n",
       "      <td>1352</td>\n",
       "      <td>1321</td>\n",
       "      <td>2253</td>\n",
       "      <td>2149</td>\n",
       "      <td>2434</td>\n",
       "      <td>...</td>\n",
       "      <td>48.1</td>\n",
       "      <td>12</td>\n",
       "      <td>12.8</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>16.3</td>\n",
       "      <td>10816</td>\n",
       "      <td>20349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>40260</td>\n",
       "      <td>2006</td>\n",
       "      <td>2440</td>\n",
       "      <td>2421</td>\n",
       "      <td>2953</td>\n",
       "      <td>2865</td>\n",
       "      <td>5124</td>\n",
       "      <td>5139</td>\n",
       "      <td>5947</td>\n",
       "      <td>...</td>\n",
       "      <td>31.4</td>\n",
       "      <td>16.3</td>\n",
       "      <td>17.9</td>\n",
       "      <td>12.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>16079</td>\n",
       "      <td>23282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>603</td>\n",
       "      <td>52408</td>\n",
       "      <td>2664</td>\n",
       "      <td>3177</td>\n",
       "      <td>3351</td>\n",
       "      <td>3685</td>\n",
       "      <td>3585</td>\n",
       "      <td>6473</td>\n",
       "      <td>6775</td>\n",
       "      <td>6678</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>14.9</td>\n",
       "      <td>17.5</td>\n",
       "      <td>11.7</td>\n",
       "      <td>10.8</td>\n",
       "      <td>8.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>21.9</td>\n",
       "      <td>16804</td>\n",
       "      <td>26820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>606</td>\n",
       "      <td>6331</td>\n",
       "      <td>347</td>\n",
       "      <td>331</td>\n",
       "      <td>461</td>\n",
       "      <td>474</td>\n",
       "      <td>469</td>\n",
       "      <td>707</td>\n",
       "      <td>933</td>\n",
       "      <td>776</td>\n",
       "      <td>...</td>\n",
       "      <td>45.3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>20</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>12512</td>\n",
       "      <td>15730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>610</td>\n",
       "      <td>28328</td>\n",
       "      <td>1438</td>\n",
       "      <td>1490</td>\n",
       "      <td>2044</td>\n",
       "      <td>2122</td>\n",
       "      <td>1985</td>\n",
       "      <td>3358</td>\n",
       "      <td>3778</td>\n",
       "      <td>3858</td>\n",
       "      <td>...</td>\n",
       "      <td>26.9</td>\n",
       "      <td>14.8</td>\n",
       "      <td>23.7</td>\n",
       "      <td>15.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>18.4</td>\n",
       "      <td>17475</td>\n",
       "      <td>23360</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   zipcode  5_years_or_less  5-9_years  10-14_years  15-19_years  20-24_years  \\\n",
       "0      601            17982       1006         1080         1342         1352   \n",
       "1      602            40260       2006         2440         2421         2953   \n",
       "2      603            52408       2664         3177         3351         3685   \n",
       "3      606             6331        347          331          461          474   \n",
       "4      610            28328       1438         1490         2044         2122   \n",
       "\n",
       "   25-34_years  35-44_years  45-54_years  55-59_years  ...  $10,000-$14,999  \\\n",
       "0         1321         2253         2149         2434  ...             48.1   \n",
       "1         2865         5124         5139         5947  ...             31.4   \n",
       "2         3585         6473         6775         6678  ...               31   \n",
       "3          469          707          933          776  ...             45.3   \n",
       "4         1985         3358         3778         3858  ...             26.9   \n",
       "\n",
       "   $15,000-$24,999  $25,000-$34,999  $35,000-$49,999  $50,000-$64,999  \\\n",
       "0               12             12.8              8.6              8.7   \n",
       "1             16.3             17.9             12.2             10.6   \n",
       "2             14.9             17.5             11.7             10.8   \n",
       "3             10.2               20             11.7               11   \n",
       "4             14.8             23.7             15.2              9.3   \n",
       "\n",
       "   $65,000-$74,999 $75,000-$99,999 $100,000_or_more median_household_income  \\\n",
       "0              6.2             1.4             16.3                   10816   \n",
       "1              7.7             2.9             21.2                   16079   \n",
       "2              8.7             2.4             21.9                   16804   \n",
       "3              1.8               0             12.8                   12512   \n",
       "4              7.5             1.6             18.4                   17475   \n",
       "\n",
       "  mean_household_income  \n",
       "0                 20349  \n",
       "1                 23282  \n",
       "2                 26820  \n",
       "3                 15730  \n",
       "4                 23360  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "demo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>state</th>\n",
       "      <th>2005Q1_gdp</th>\n",
       "      <th>2005Q2_gdp</th>\n",
       "      <th>2005Q3_gdp</th>\n",
       "      <th>2005Q4_gdp</th>\n",
       "      <th>2006Q1_gdp</th>\n",
       "      <th>2006Q2_gdp</th>\n",
       "      <th>2006Q3_gdp</th>\n",
       "      <th>2006Q4_gdp</th>\n",
       "      <th>2007Q1_gdp</th>\n",
       "      <th>...</th>\n",
       "      <th>2016/03_ur</th>\n",
       "      <th>2016/04_ur</th>\n",
       "      <th>2016/05_ur</th>\n",
       "      <th>2016/06_ur</th>\n",
       "      <th>2016/07_ur</th>\n",
       "      <th>2016/08_ur</th>\n",
       "      <th>2016/09_ur</th>\n",
       "      <th>2016/10_ur</th>\n",
       "      <th>2016/11_ur</th>\n",
       "      <th>2016/12_ur</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>153332</td>\n",
       "      <td>155940</td>\n",
       "      <td>157437</td>\n",
       "      <td>160293</td>\n",
       "      <td>161934</td>\n",
       "      <td>163974</td>\n",
       "      <td>165470</td>\n",
       "      <td>166495</td>\n",
       "      <td>166821</td>\n",
       "      <td>...</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.7</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "      <td>6.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AK</td>\n",
       "      <td>37517</td>\n",
       "      <td>38907</td>\n",
       "      <td>40691</td>\n",
       "      <td>43138</td>\n",
       "      <td>42872</td>\n",
       "      <td>44653</td>\n",
       "      <td>45349</td>\n",
       "      <td>45840</td>\n",
       "      <td>46658</td>\n",
       "      <td>...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.8</td>\n",
       "      <td>5.9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>6.2</td>\n",
       "      <td>6.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AZ</td>\n",
       "      <td>218206</td>\n",
       "      <td>224496</td>\n",
       "      <td>231629</td>\n",
       "      <td>235099</td>\n",
       "      <td>241787</td>\n",
       "      <td>244659</td>\n",
       "      <td>250886</td>\n",
       "      <td>256505</td>\n",
       "      <td>258078</td>\n",
       "      <td>...</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AR</td>\n",
       "      <td>88446</td>\n",
       "      <td>89264</td>\n",
       "      <td>90515</td>\n",
       "      <td>93050</td>\n",
       "      <td>93413</td>\n",
       "      <td>95259</td>\n",
       "      <td>95481</td>\n",
       "      <td>95203</td>\n",
       "      <td>94289</td>\n",
       "      <td>...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.2</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CA</td>\n",
       "      <td>1722091</td>\n",
       "      <td>1747827</td>\n",
       "      <td>1787427</td>\n",
       "      <td>1809426</td>\n",
       "      <td>1857944</td>\n",
       "      <td>1865835</td>\n",
       "      <td>1886549</td>\n",
       "      <td>1907754</td>\n",
       "      <td>1915172</td>\n",
       "      <td>...</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.5</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.3</td>\n",
       "      <td>5.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 519 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  state  2005Q1_gdp  2005Q2_gdp  2005Q3_gdp  2005Q4_gdp  2006Q1_gdp  \\\n",
       "0    AL      153332      155940      157437      160293      161934   \n",
       "1    AK       37517       38907       40691       43138       42872   \n",
       "2    AZ      218206      224496      231629      235099      241787   \n",
       "3    AR       88446       89264       90515       93050       93413   \n",
       "4    CA     1722091     1747827     1787427     1809426     1857944   \n",
       "\n",
       "   2006Q2_gdp  2006Q3_gdp  2006Q4_gdp  2007Q1_gdp  ...  2016/03_ur  \\\n",
       "0      163974      165470      166495      166821  ...         6.6   \n",
       "1       44653       45349       45840       46658  ...         5.9   \n",
       "2      244659      250886      256505      258078  ...         4.1   \n",
       "3       95259       95481       95203       94289  ...         5.5   \n",
       "4     1865835     1886549     1907754     1915172  ...         5.6   \n",
       "\n",
       "   2016/04_ur  2016/05_ur  2016/06_ur  2016/07_ur  2016/08_ur  2016/09_ur  \\\n",
       "0         6.6         6.6         6.7         6.7         6.7         6.6   \n",
       "1         5.8         5.8         5.8         5.8         5.9         6.0   \n",
       "2         4.1         4.1         4.1         4.0         4.0         4.0   \n",
       "3         5.4         5.3         5.3         5.2         5.1         5.1   \n",
       "4         5.5         5.5         5.5         5.4         5.4         5.3   \n",
       "\n",
       "   2016/10_ur  2016/11_ur  2016/12_ur  \n",
       "0         6.6         6.6         6.6  \n",
       "1         6.1         6.2         6.3  \n",
       "2         4.0         4.0         3.9  \n",
       "3         5.0         5.0         5.0  \n",
       "4         5.3         5.3         5.2  \n",
       "\n",
       "[5 rows x 519 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "econ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>metro</th>\n",
       "      <th>county</th>\n",
       "      <th>size_rank</th>\n",
       "      <th>1996-04</th>\n",
       "      <th>1996-05</th>\n",
       "      <th>1996-06</th>\n",
       "      <th>...</th>\n",
       "      <th>2016-09</th>\n",
       "      <th>2016-10</th>\n",
       "      <th>2016-11</th>\n",
       "      <th>2016-12</th>\n",
       "      <th>2017-01</th>\n",
       "      <th>2017-02</th>\n",
       "      <th>2017-03</th>\n",
       "      <th>2017-04</th>\n",
       "      <th>2017-05</th>\n",
       "      <th>2017-06</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10025</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1137500</td>\n",
       "      <td>1137700</td>\n",
       "      <td>1152700</td>\n",
       "      <td>1156000.0</td>\n",
       "      <td>1140200</td>\n",
       "      <td>1130000</td>\n",
       "      <td>1131900</td>\n",
       "      <td>1149600</td>\n",
       "      <td>1198400</td>\n",
       "      <td>1247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>60657</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>2</td>\n",
       "      <td>146700.0</td>\n",
       "      <td>146500.0</td>\n",
       "      <td>146300.0</td>\n",
       "      <td>...</td>\n",
       "      <td>318200</td>\n",
       "      <td>318100</td>\n",
       "      <td>318800</td>\n",
       "      <td>320200.0</td>\n",
       "      <td>320800</td>\n",
       "      <td>322000</td>\n",
       "      <td>323800</td>\n",
       "      <td>326100</td>\n",
       "      <td>327800</td>\n",
       "      <td>329100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>60614</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>3</td>\n",
       "      <td>198000.0</td>\n",
       "      <td>195500.0</td>\n",
       "      <td>194200.0</td>\n",
       "      <td>...</td>\n",
       "      <td>401900</td>\n",
       "      <td>406000</td>\n",
       "      <td>414100</td>\n",
       "      <td>417800.0</td>\n",
       "      <td>417400</td>\n",
       "      <td>418400</td>\n",
       "      <td>414100</td>\n",
       "      <td>404100</td>\n",
       "      <td>406400</td>\n",
       "      <td>415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>79936</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>TX</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>4</td>\n",
       "      <td>70800.0</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>71000.0</td>\n",
       "      <td>...</td>\n",
       "      <td>113800</td>\n",
       "      <td>113900</td>\n",
       "      <td>114100</td>\n",
       "      <td>114500.0</td>\n",
       "      <td>114900</td>\n",
       "      <td>115000</td>\n",
       "      <td>114700</td>\n",
       "      <td>114700</td>\n",
       "      <td>114800</td>\n",
       "      <td>114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>60640</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>5</td>\n",
       "      <td>102300.0</td>\n",
       "      <td>101300.0</td>\n",
       "      <td>100700.0</td>\n",
       "      <td>...</td>\n",
       "      <td>198800</td>\n",
       "      <td>199200</td>\n",
       "      <td>200100</td>\n",
       "      <td>201500.0</td>\n",
       "      <td>203000</td>\n",
       "      <td>205100</td>\n",
       "      <td>206700</td>\n",
       "      <td>206500</td>\n",
       "      <td>206200</td>\n",
       "      <td>206700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 262 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   type  zipcode      city state     metro    county  size_rank   1996-04  \\\n",
       "0  ZHVI    10025  New York    NY  New York  New York          1       NaN   \n",
       "1  ZHVI    60657   Chicago    IL   Chicago      Cook          2  146700.0   \n",
       "2  ZHVI    60614   Chicago    IL   Chicago      Cook          3  198000.0   \n",
       "3  ZHVI    79936   El Paso    TX   El Paso   El Paso          4   70800.0   \n",
       "4  ZHVI    60640   Chicago    IL   Chicago      Cook          5  102300.0   \n",
       "\n",
       "    1996-05   1996-06  ...  2016-09  2016-10  2016-11    2016-12  2017-01  \\\n",
       "0       NaN       NaN  ...  1137500  1137700  1152700  1156000.0  1140200   \n",
       "1  146500.0  146300.0  ...   318200   318100   318800   320200.0   320800   \n",
       "2  195500.0  194200.0  ...   401900   406000   414100   417800.0   417400   \n",
       "3   71000.0   71000.0  ...   113800   113900   114100   114500.0   114900   \n",
       "4  101300.0  100700.0  ...   198800   199200   200100   201500.0   203000   \n",
       "\n",
       "   2017-02  2017-03  2017-04  2017-05  2017-06  \n",
       "0  1130000  1131900  1149600  1198400  1247000  \n",
       "1   322000   323800   326100   327800   329100  \n",
       "2   418400   414100   404100   406400   415500  \n",
       "3   115000   114700   114700   114800   114700  \n",
       "4   205100   206700   206500   206200   206700  \n",
       "\n",
       "[5 rows x 262 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "realestate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "      <th>metro_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-05</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-03</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   listing_id        date available  price metro_area\n",
       "0        2515  2018-03-05         t   69.0        NYC\n",
       "1        2515  2018-03-04         t   69.0        NYC\n",
       "2        2515  2018-03-03         t   69.0        NYC\n",
       "3        2515  2018-03-02         t   69.0        NYC\n",
       "4        2515  2018-03-01         t   69.0        NYC"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "calendar.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>new york city</td>\n",
       "      <td>b1a0d113cb17d1d85f0e12700dd71f36bddedc54</td>\n",
       "      <td>40.601540</td>\n",
       "      <td>-73.729636</td>\n",
       "      <td>A Bacon Yacht Charter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new york city</td>\n",
       "      <td>8799bb8d8ac7c2e1933f92ac1ef0f69a5e0b37c1</td>\n",
       "      <td>40.608921</td>\n",
       "      <td>-73.728256</td>\n",
       "      <td>Mezzanote</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new york city</td>\n",
       "      <td>f2390680116af4d62e0da6f0432d33e94b9cb0e6</td>\n",
       "      <td>40.616978</td>\n",
       "      <td>-73.730349</td>\n",
       "      <td>Prime Bistro</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['bar', 'restaurant', 'food', 'point_of_intere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new york city</td>\n",
       "      <td>e9294eb56025e8eb29b937dc0aca29fd3059b9ab</td>\n",
       "      <td>40.617318</td>\n",
       "      <td>-73.730637</td>\n",
       "      <td>Rita's Ice</td>\n",
       "      <td>4.6</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new york city</td>\n",
       "      <td>5c7b72c7bf9e7bc4f94baf09db1252088087f65d</td>\n",
       "      <td>40.618371</td>\n",
       "      <td>-73.728178</td>\n",
       "      <td>Cho-Sen Island</td>\n",
       "      <td>4.4</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            city                                        id   latitude  \\\n",
       "0  new york city  b1a0d113cb17d1d85f0e12700dd71f36bddedc54  40.601540   \n",
       "1  new york city  8799bb8d8ac7c2e1933f92ac1ef0f69a5e0b37c1  40.608921   \n",
       "2  new york city  f2390680116af4d62e0da6f0432d33e94b9cb0e6  40.616978   \n",
       "3  new york city  e9294eb56025e8eb29b937dc0aca29fd3059b9ab  40.617318   \n",
       "4  new york city  5c7b72c7bf9e7bc4f94baf09db1252088087f65d  40.618371   \n",
       "\n",
       "   longitude                   name  rating  \\\n",
       "0 -73.729636  A Bacon Yacht Charter     NaN   \n",
       "1 -73.728256              Mezzanote     NaN   \n",
       "2 -73.730349           Prime Bistro     4.0   \n",
       "3 -73.730637             Rita's Ice     4.6   \n",
       "4 -73.728178         Cho-Sen Island     4.4   \n",
       "\n",
       "                                               types  \n",
       "0  ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "1  ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "2  ['bar', 'restaurant', 'food', 'point_of_intere...  \n",
       "3  ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "4  ['restaurant', 'food', 'point_of_interest', 'e...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "venues.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: (4 min)\n",
    "\n",
    "Clean the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>listing_id</th>\n",
       "      <th>date</th>\n",
       "      <th>available</th>\n",
       "      <th>price</th>\n",
       "      <th>metro_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-05</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-04</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-03</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-02</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2515</td>\n",
       "      <td>2018-03-01</td>\n",
       "      <td>t</td>\n",
       "      <td>69.0</td>\n",
       "      <td>NYC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21841230</th>\n",
       "      <td>18364166</td>\n",
       "      <td>2017-05-14</td>\n",
       "      <td>t</td>\n",
       "      <td>49.0</td>\n",
       "      <td>dc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21841231</th>\n",
       "      <td>18364166</td>\n",
       "      <td>2017-05-13</td>\n",
       "      <td>t</td>\n",
       "      <td>49.0</td>\n",
       "      <td>dc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21841232</th>\n",
       "      <td>18364166</td>\n",
       "      <td>2017-05-12</td>\n",
       "      <td>t</td>\n",
       "      <td>49.0</td>\n",
       "      <td>dc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21841233</th>\n",
       "      <td>18364166</td>\n",
       "      <td>2017-05-11</td>\n",
       "      <td>t</td>\n",
       "      <td>49.0</td>\n",
       "      <td>dc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21841234</th>\n",
       "      <td>18364166</td>\n",
       "      <td>2017-05-10</td>\n",
       "      <td>t</td>\n",
       "      <td>49.0</td>\n",
       "      <td>dc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8618452 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          listing_id        date available  price metro_area\n",
       "0               2515  2018-03-05         t   69.0        NYC\n",
       "1               2515  2018-03-04         t   69.0        NYC\n",
       "2               2515  2018-03-03         t   69.0        NYC\n",
       "3               2515  2018-03-02         t   69.0        NYC\n",
       "4               2515  2018-03-01         t   69.0        NYC\n",
       "...              ...         ...       ...    ...        ...\n",
       "21841230    18364166  2017-05-14         t   49.0         dc\n",
       "21841231    18364166  2017-05-13         t   49.0         dc\n",
       "21841232    18364166  2017-05-12         t   49.0         dc\n",
       "21841233    18364166  2017-05-11         t   49.0         dc\n",
       "21841234    18364166  2017-05-10         t   49.0         dc\n",
       "\n",
       "[8618452 rows x 5 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning Calender, drops NAS and gets only avaliable airbnbs\n",
    "calendar_aval = calendar[calendar.available == 't']\n",
    "calendar_aval[calendar_aval.columns[~calendar_aval.isnull().any()]]\n",
    "calendar_aval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city</th>\n",
       "      <th>id</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>name</th>\n",
       "      <th>rating</th>\n",
       "      <th>types</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>new york city</td>\n",
       "      <td>f2390680116af4d62e0da6f0432d33e94b9cb0e6</td>\n",
       "      <td>40.616978</td>\n",
       "      <td>-73.730349</td>\n",
       "      <td>Prime Bistro</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['bar', 'restaurant', 'food', 'point_of_intere...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new york city</td>\n",
       "      <td>e9294eb56025e8eb29b937dc0aca29fd3059b9ab</td>\n",
       "      <td>40.617318</td>\n",
       "      <td>-73.730637</td>\n",
       "      <td>Rita's Ice</td>\n",
       "      <td>4.6</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>new york city</td>\n",
       "      <td>5c7b72c7bf9e7bc4f94baf09db1252088087f65d</td>\n",
       "      <td>40.618371</td>\n",
       "      <td>-73.728178</td>\n",
       "      <td>Cho-Sen Island</td>\n",
       "      <td>4.4</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>new york city</td>\n",
       "      <td>0b99220b44ee0d45d28f44e95d08da112f6e2ca7</td>\n",
       "      <td>40.618126</td>\n",
       "      <td>-73.728679</td>\n",
       "      <td>Sunflower Cafe- Lawrence</td>\n",
       "      <td>4.2</td>\n",
       "      <td>['cafe', 'restaurant', 'food', 'point_of_inter...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>new york city</td>\n",
       "      <td>97fdbf124eeb91d70d7c3f710ca52c997d618bb9</td>\n",
       "      <td>40.617592</td>\n",
       "      <td>-73.729390</td>\n",
       "      <td>Sushi Tokyo Lawrence</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['restaurant', 'food', 'point_of_interest', 'e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267950</th>\n",
       "      <td>washington dc</td>\n",
       "      <td>ca10a0af0955722fbc5360ce68d6576615316c7e</td>\n",
       "      <td>38.929941</td>\n",
       "      <td>-77.048213</td>\n",
       "      <td>Great Ape House</td>\n",
       "      <td>4.0</td>\n",
       "      <td>['zoo', 'point_of_interest', 'establishment']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267951</th>\n",
       "      <td>washington dc</td>\n",
       "      <td>33aec9e6edd3bb54120d22550f7c34d1d5d936d5</td>\n",
       "      <td>38.930459</td>\n",
       "      <td>-77.048756</td>\n",
       "      <td>Small Mammal House</td>\n",
       "      <td>3.7</td>\n",
       "      <td>['zoo', 'point_of_interest', 'establishment']</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267954</th>\n",
       "      <td>washington dc</td>\n",
       "      <td>9fa01cff0b2b2bba8269bf3932f889d41d2fb1d2</td>\n",
       "      <td>38.952258</td>\n",
       "      <td>-76.952236</td>\n",
       "      <td>Hamilton Pool</td>\n",
       "      <td>3.9</td>\n",
       "      <td>['amusement_park', 'point_of_interest', 'estab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267955</th>\n",
       "      <td>washington dc</td>\n",
       "      <td>49cccff61018d81869b015c2096e8da3319def09</td>\n",
       "      <td>38.888630</td>\n",
       "      <td>-76.973022</td>\n",
       "      <td>Scream City Haunted House</td>\n",
       "      <td>4.3</td>\n",
       "      <td>['amusement_park', 'point_of_interest', 'estab...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267957</th>\n",
       "      <td>washington dc</td>\n",
       "      <td>78e0304412c65a0100ebbcc1c8c6403fb5c535d1</td>\n",
       "      <td>38.914844</td>\n",
       "      <td>-77.024677</td>\n",
       "      <td>Westminster Park</td>\n",
       "      <td>4.5</td>\n",
       "      <td>['amusement_park', 'point_of_interest', 'estab...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>126742 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 city                                        id   latitude  \\\n",
       "2       new york city  f2390680116af4d62e0da6f0432d33e94b9cb0e6  40.616978   \n",
       "3       new york city  e9294eb56025e8eb29b937dc0aca29fd3059b9ab  40.617318   \n",
       "4       new york city  5c7b72c7bf9e7bc4f94baf09db1252088087f65d  40.618371   \n",
       "5       new york city  0b99220b44ee0d45d28f44e95d08da112f6e2ca7  40.618126   \n",
       "6       new york city  97fdbf124eeb91d70d7c3f710ca52c997d618bb9  40.617592   \n",
       "...               ...                                       ...        ...   \n",
       "267950  washington dc  ca10a0af0955722fbc5360ce68d6576615316c7e  38.929941   \n",
       "267951  washington dc  33aec9e6edd3bb54120d22550f7c34d1d5d936d5  38.930459   \n",
       "267954  washington dc  9fa01cff0b2b2bba8269bf3932f889d41d2fb1d2  38.952258   \n",
       "267955  washington dc  49cccff61018d81869b015c2096e8da3319def09  38.888630   \n",
       "267957  washington dc  78e0304412c65a0100ebbcc1c8c6403fb5c535d1  38.914844   \n",
       "\n",
       "        longitude                       name  rating  \\\n",
       "2      -73.730349               Prime Bistro     4.0   \n",
       "3      -73.730637                 Rita's Ice     4.6   \n",
       "4      -73.728178             Cho-Sen Island     4.4   \n",
       "5      -73.728679   Sunflower Cafe- Lawrence     4.2   \n",
       "6      -73.729390       Sushi Tokyo Lawrence     4.0   \n",
       "...           ...                        ...     ...   \n",
       "267950 -77.048213            Great Ape House     4.0   \n",
       "267951 -77.048756         Small Mammal House     3.7   \n",
       "267954 -76.952236              Hamilton Pool     3.9   \n",
       "267955 -76.973022  Scream City Haunted House     4.3   \n",
       "267957 -77.024677           Westminster Park     4.5   \n",
       "\n",
       "                                                    types  \n",
       "2       ['bar', 'restaurant', 'food', 'point_of_intere...  \n",
       "3       ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "4       ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "5       ['cafe', 'restaurant', 'food', 'point_of_inter...  \n",
       "6       ['restaurant', 'food', 'point_of_interest', 'e...  \n",
       "...                                                   ...  \n",
       "267950      ['zoo', 'point_of_interest', 'establishment']  \n",
       "267951      ['zoo', 'point_of_interest', 'establishment']  \n",
       "267954  ['amusement_park', 'point_of_interest', 'estab...  \n",
       "267955  ['amusement_park', 'point_of_interest', 'estab...  \n",
       "267957  ['amusement_park', 'point_of_interest', 'estab...  \n",
       "\n",
       "[126742 rows x 7 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning venues, drops NA values\n",
    "venues_clean = venues.dropna(how='any',axis=0) \n",
    "venues_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>type</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>metro</th>\n",
       "      <th>county</th>\n",
       "      <th>size_rank</th>\n",
       "      <th>2016-08</th>\n",
       "      <th>2016-09</th>\n",
       "      <th>2016-10</th>\n",
       "      <th>2016-11</th>\n",
       "      <th>2016-12</th>\n",
       "      <th>2017-01</th>\n",
       "      <th>2017-02</th>\n",
       "      <th>2017-03</th>\n",
       "      <th>2017-04</th>\n",
       "      <th>2017-05</th>\n",
       "      <th>2017-06</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>10025</td>\n",
       "      <td>New York</td>\n",
       "      <td>NY</td>\n",
       "      <td>New York</td>\n",
       "      <td>New York</td>\n",
       "      <td>1</td>\n",
       "      <td>1132500</td>\n",
       "      <td>1137500</td>\n",
       "      <td>1137700</td>\n",
       "      <td>1152700</td>\n",
       "      <td>1156000.0</td>\n",
       "      <td>1140200</td>\n",
       "      <td>1130000</td>\n",
       "      <td>1131900</td>\n",
       "      <td>1149600</td>\n",
       "      <td>1198400</td>\n",
       "      <td>1247000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>60657</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>2</td>\n",
       "      <td>318600</td>\n",
       "      <td>318200</td>\n",
       "      <td>318100</td>\n",
       "      <td>318800</td>\n",
       "      <td>320200.0</td>\n",
       "      <td>320800</td>\n",
       "      <td>322000</td>\n",
       "      <td>323800</td>\n",
       "      <td>326100</td>\n",
       "      <td>327800</td>\n",
       "      <td>329100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>60614</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>3</td>\n",
       "      <td>400700</td>\n",
       "      <td>401900</td>\n",
       "      <td>406000</td>\n",
       "      <td>414100</td>\n",
       "      <td>417800.0</td>\n",
       "      <td>417400</td>\n",
       "      <td>418400</td>\n",
       "      <td>414100</td>\n",
       "      <td>404100</td>\n",
       "      <td>406400</td>\n",
       "      <td>415500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>79936</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>TX</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>El Paso</td>\n",
       "      <td>4</td>\n",
       "      <td>113700</td>\n",
       "      <td>113800</td>\n",
       "      <td>113900</td>\n",
       "      <td>114100</td>\n",
       "      <td>114500.0</td>\n",
       "      <td>114900</td>\n",
       "      <td>115000</td>\n",
       "      <td>114700</td>\n",
       "      <td>114700</td>\n",
       "      <td>114800</td>\n",
       "      <td>114700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ZHVI</td>\n",
       "      <td>60640</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>Cook</td>\n",
       "      <td>5</td>\n",
       "      <td>199900</td>\n",
       "      <td>198800</td>\n",
       "      <td>199200</td>\n",
       "      <td>200100</td>\n",
       "      <td>201500.0</td>\n",
       "      <td>203000</td>\n",
       "      <td>205100</td>\n",
       "      <td>206700</td>\n",
       "      <td>206500</td>\n",
       "      <td>206200</td>\n",
       "      <td>206700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29106</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>12720</td>\n",
       "      <td>Bethel</td>\n",
       "      <td>NY</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Sullivan</td>\n",
       "      <td>15907</td>\n",
       "      <td>1191</td>\n",
       "      <td>1185</td>\n",
       "      <td>1181</td>\n",
       "      <td>1174</td>\n",
       "      <td>1167.0</td>\n",
       "      <td>1156</td>\n",
       "      <td>1162</td>\n",
       "      <td>1175</td>\n",
       "      <td>1194</td>\n",
       "      <td>1202</td>\n",
       "      <td>1206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29107</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>1338</td>\n",
       "      <td>Shelburne Falls</td>\n",
       "      <td>MA</td>\n",
       "      <td>Greenfield Town</td>\n",
       "      <td>Franklin</td>\n",
       "      <td>15908</td>\n",
       "      <td>1774</td>\n",
       "      <td>1769</td>\n",
       "      <td>1772</td>\n",
       "      <td>1775</td>\n",
       "      <td>1780.0</td>\n",
       "      <td>1786</td>\n",
       "      <td>1790</td>\n",
       "      <td>1797</td>\n",
       "      <td>1798</td>\n",
       "      <td>1796</td>\n",
       "      <td>1795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29108</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>21405</td>\n",
       "      <td>Annapolis</td>\n",
       "      <td>MD</td>\n",
       "      <td>Baltimore</td>\n",
       "      <td>Anne Arundel</td>\n",
       "      <td>15909</td>\n",
       "      <td>4061</td>\n",
       "      <td>4048</td>\n",
       "      <td>4005</td>\n",
       "      <td>3888</td>\n",
       "      <td>3729.0</td>\n",
       "      <td>3563</td>\n",
       "      <td>3493</td>\n",
       "      <td>3474</td>\n",
       "      <td>3496</td>\n",
       "      <td>3488</td>\n",
       "      <td>3471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29109</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>85220</td>\n",
       "      <td>Apache Junction</td>\n",
       "      <td>AZ</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Pinal</td>\n",
       "      <td>15910</td>\n",
       "      <td>998</td>\n",
       "      <td>1007</td>\n",
       "      <td>1018</td>\n",
       "      <td>1035</td>\n",
       "      <td>1052.0</td>\n",
       "      <td>1061</td>\n",
       "      <td>1060</td>\n",
       "      <td>1058</td>\n",
       "      <td>1065</td>\n",
       "      <td>1074</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29110</th>\n",
       "      <td>ZRI</td>\n",
       "      <td>89595</td>\n",
       "      <td>Reno</td>\n",
       "      <td>NV</td>\n",
       "      <td>Reno</td>\n",
       "      <td>Washoe</td>\n",
       "      <td>15911</td>\n",
       "      <td>870</td>\n",
       "      <td>872</td>\n",
       "      <td>877</td>\n",
       "      <td>871</td>\n",
       "      <td>863.0</td>\n",
       "      <td>843</td>\n",
       "      <td>826</td>\n",
       "      <td>816</td>\n",
       "      <td>820</td>\n",
       "      <td>838</td>\n",
       "      <td>848</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>29111 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       type  zipcode             city state            metro        county  \\\n",
       "0      ZHVI    10025         New York    NY         New York      New York   \n",
       "1      ZHVI    60657          Chicago    IL          Chicago          Cook   \n",
       "2      ZHVI    60614          Chicago    IL          Chicago          Cook   \n",
       "3      ZHVI    79936          El Paso    TX          El Paso       El Paso   \n",
       "4      ZHVI    60640          Chicago    IL          Chicago          Cook   \n",
       "...     ...      ...              ...   ...              ...           ...   \n",
       "29106   ZRI    12720           Bethel    NY              NaN      Sullivan   \n",
       "29107   ZRI     1338  Shelburne Falls    MA  Greenfield Town      Franklin   \n",
       "29108   ZRI    21405        Annapolis    MD        Baltimore  Anne Arundel   \n",
       "29109   ZRI    85220  Apache Junction    AZ          Phoenix         Pinal   \n",
       "29110   ZRI    89595             Reno    NV             Reno        Washoe   \n",
       "\n",
       "       size_rank  2016-08  2016-09  2016-10  2016-11    2016-12  2017-01  \\\n",
       "0              1  1132500  1137500  1137700  1152700  1156000.0  1140200   \n",
       "1              2   318600   318200   318100   318800   320200.0   320800   \n",
       "2              3   400700   401900   406000   414100   417800.0   417400   \n",
       "3              4   113700   113800   113900   114100   114500.0   114900   \n",
       "4              5   199900   198800   199200   200100   201500.0   203000   \n",
       "...          ...      ...      ...      ...      ...        ...      ...   \n",
       "29106      15907     1191     1185     1181     1174     1167.0     1156   \n",
       "29107      15908     1774     1769     1772     1775     1780.0     1786   \n",
       "29108      15909     4061     4048     4005     3888     3729.0     3563   \n",
       "29109      15910      998     1007     1018     1035     1052.0     1061   \n",
       "29110      15911      870      872      877      871      863.0      843   \n",
       "\n",
       "       2017-02  2017-03  2017-04  2017-05  2017-06  \n",
       "0      1130000  1131900  1149600  1198400  1247000  \n",
       "1       322000   323800   326100   327800   329100  \n",
       "2       418400   414100   404100   406400   415500  \n",
       "3       115000   114700   114700   114800   114700  \n",
       "4       205100   206700   206500   206200   206700  \n",
       "...        ...      ...      ...      ...      ...  \n",
       "29106     1162     1175     1194     1202     1206  \n",
       "29107     1790     1797     1798     1796     1795  \n",
       "29108     3493     3474     3496     3488     3471  \n",
       "29109     1060     1058     1065     1074     1080  \n",
       "29110      826      816      820      838      848  \n",
       "\n",
       "[29111 rows x 18 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning real estate, gets only 2016-08 onwards\n",
    "realcleaned = realestate.filter([\"type\",\"zipcode\",\"city\",\"state\",\"metro\",\"county\",\"size_rank\",\"2016-08\",\"2016-09\",\"2016-10\",\"2016-11\",\"2016-12\",\"2017-01\",\"2017-02\",\"2017-03\",\"2017-04\",\"2017-05\",\"2017-06\"], axis=1)\n",
    "realcleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>zipcode</th>\n",
       "      <th>5_years_or_less</th>\n",
       "      <th>5-9_years</th>\n",
       "      <th>10-14_years</th>\n",
       "      <th>15-19_years</th>\n",
       "      <th>20-24_years</th>\n",
       "      <th>25-34_years</th>\n",
       "      <th>35-44_years</th>\n",
       "      <th>45-54_years</th>\n",
       "      <th>55-59_years</th>\n",
       "      <th>...</th>\n",
       "      <th>$10,000-$14,999</th>\n",
       "      <th>$15,000-$24,999</th>\n",
       "      <th>$25,000-$34,999</th>\n",
       "      <th>$35,000-$49,999</th>\n",
       "      <th>$50,000-$64,999</th>\n",
       "      <th>$65,000-$74,999</th>\n",
       "      <th>$75,000-$99,999</th>\n",
       "      <th>$100,000_or_more</th>\n",
       "      <th>median_household_income</th>\n",
       "      <th>mean_household_income</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>601</td>\n",
       "      <td>17982</td>\n",
       "      <td>1006</td>\n",
       "      <td>1080</td>\n",
       "      <td>1342</td>\n",
       "      <td>1352</td>\n",
       "      <td>1321</td>\n",
       "      <td>2253</td>\n",
       "      <td>2149</td>\n",
       "      <td>2434</td>\n",
       "      <td>...</td>\n",
       "      <td>48.1</td>\n",
       "      <td>12</td>\n",
       "      <td>12.8</td>\n",
       "      <td>8.6</td>\n",
       "      <td>8.7</td>\n",
       "      <td>6.2</td>\n",
       "      <td>1.4</td>\n",
       "      <td>16.3</td>\n",
       "      <td>10816</td>\n",
       "      <td>20349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>602</td>\n",
       "      <td>40260</td>\n",
       "      <td>2006</td>\n",
       "      <td>2440</td>\n",
       "      <td>2421</td>\n",
       "      <td>2953</td>\n",
       "      <td>2865</td>\n",
       "      <td>5124</td>\n",
       "      <td>5139</td>\n",
       "      <td>5947</td>\n",
       "      <td>...</td>\n",
       "      <td>31.4</td>\n",
       "      <td>16.3</td>\n",
       "      <td>17.9</td>\n",
       "      <td>12.2</td>\n",
       "      <td>10.6</td>\n",
       "      <td>7.7</td>\n",
       "      <td>2.9</td>\n",
       "      <td>21.2</td>\n",
       "      <td>16079</td>\n",
       "      <td>23282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>603</td>\n",
       "      <td>52408</td>\n",
       "      <td>2664</td>\n",
       "      <td>3177</td>\n",
       "      <td>3351</td>\n",
       "      <td>3685</td>\n",
       "      <td>3585</td>\n",
       "      <td>6473</td>\n",
       "      <td>6775</td>\n",
       "      <td>6678</td>\n",
       "      <td>...</td>\n",
       "      <td>31</td>\n",
       "      <td>14.9</td>\n",
       "      <td>17.5</td>\n",
       "      <td>11.7</td>\n",
       "      <td>10.8</td>\n",
       "      <td>8.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>21.9</td>\n",
       "      <td>16804</td>\n",
       "      <td>26820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>606</td>\n",
       "      <td>6331</td>\n",
       "      <td>347</td>\n",
       "      <td>331</td>\n",
       "      <td>461</td>\n",
       "      <td>474</td>\n",
       "      <td>469</td>\n",
       "      <td>707</td>\n",
       "      <td>933</td>\n",
       "      <td>776</td>\n",
       "      <td>...</td>\n",
       "      <td>45.3</td>\n",
       "      <td>10.2</td>\n",
       "      <td>20</td>\n",
       "      <td>11.7</td>\n",
       "      <td>11</td>\n",
       "      <td>1.8</td>\n",
       "      <td>0</td>\n",
       "      <td>12.8</td>\n",
       "      <td>12512</td>\n",
       "      <td>15730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>610</td>\n",
       "      <td>28328</td>\n",
       "      <td>1438</td>\n",
       "      <td>1490</td>\n",
       "      <td>2044</td>\n",
       "      <td>2122</td>\n",
       "      <td>1985</td>\n",
       "      <td>3358</td>\n",
       "      <td>3778</td>\n",
       "      <td>3858</td>\n",
       "      <td>...</td>\n",
       "      <td>26.9</td>\n",
       "      <td>14.8</td>\n",
       "      <td>23.7</td>\n",
       "      <td>15.2</td>\n",
       "      <td>9.3</td>\n",
       "      <td>7.5</td>\n",
       "      <td>1.6</td>\n",
       "      <td>18.4</td>\n",
       "      <td>17475</td>\n",
       "      <td>23360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33115</th>\n",
       "      <td>99923</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>46.2</td>\n",
       "      <td>53.8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33116</th>\n",
       "      <td>99925</td>\n",
       "      <td>826</td>\n",
       "      <td>65</td>\n",
       "      <td>50</td>\n",
       "      <td>47</td>\n",
       "      <td>36</td>\n",
       "      <td>60</td>\n",
       "      <td>86</td>\n",
       "      <td>82</td>\n",
       "      <td>111</td>\n",
       "      <td>...</td>\n",
       "      <td>7.1</td>\n",
       "      <td>6.2</td>\n",
       "      <td>21.2</td>\n",
       "      <td>11.1</td>\n",
       "      <td>12.6</td>\n",
       "      <td>17.5</td>\n",
       "      <td>9.5</td>\n",
       "      <td>39.6</td>\n",
       "      <td>38594</td>\n",
       "      <td>52706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33117</th>\n",
       "      <td>99926</td>\n",
       "      <td>1711</td>\n",
       "      <td>161</td>\n",
       "      <td>124</td>\n",
       "      <td>140</td>\n",
       "      <td>113</td>\n",
       "      <td>107</td>\n",
       "      <td>224</td>\n",
       "      <td>182</td>\n",
       "      <td>236</td>\n",
       "      <td>...</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.9</td>\n",
       "      <td>14.1</td>\n",
       "      <td>11.1</td>\n",
       "      <td>16.8</td>\n",
       "      <td>14.5</td>\n",
       "      <td>12</td>\n",
       "      <td>43.3</td>\n",
       "      <td>51071</td>\n",
       "      <td>71580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33118</th>\n",
       "      <td>99927</td>\n",
       "      <td>123</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>32</td>\n",
       "      <td>22</td>\n",
       "      <td>...</td>\n",
       "      <td>28.2</td>\n",
       "      <td>0</td>\n",
       "      <td>23.1</td>\n",
       "      <td>15.4</td>\n",
       "      <td>25.6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.6</td>\n",
       "      <td>19861</td>\n",
       "      <td>35617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33119</th>\n",
       "      <td>99929</td>\n",
       "      <td>2365</td>\n",
       "      <td>89</td>\n",
       "      <td>103</td>\n",
       "      <td>133</td>\n",
       "      <td>142</td>\n",
       "      <td>100</td>\n",
       "      <td>223</td>\n",
       "      <td>197</td>\n",
       "      <td>381</td>\n",
       "      <td>...</td>\n",
       "      <td>5.1</td>\n",
       "      <td>6.9</td>\n",
       "      <td>14.2</td>\n",
       "      <td>9.9</td>\n",
       "      <td>15.1</td>\n",
       "      <td>17.6</td>\n",
       "      <td>13.6</td>\n",
       "      <td>46.3</td>\n",
       "      <td>47941</td>\n",
       "      <td>62587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33120 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       zipcode  5_years_or_less  5-9_years  10-14_years  15-19_years  \\\n",
       "0          601            17982       1006         1080         1342   \n",
       "1          602            40260       2006         2440         2421   \n",
       "2          603            52408       2664         3177         3351   \n",
       "3          606             6331        347          331          461   \n",
       "4          610            28328       1438         1490         2044   \n",
       "...        ...              ...        ...          ...          ...   \n",
       "33115    99923               13          0            0            0   \n",
       "33116    99925              826         65           50           47   \n",
       "33117    99926             1711        161          124          140   \n",
       "33118    99927              123          0            0            0   \n",
       "33119    99929             2365         89          103          133   \n",
       "\n",
       "       20-24_years  25-34_years  35-44_years  45-54_years  55-59_years  ...  \\\n",
       "0             1352         1321         2253         2149         2434  ...   \n",
       "1             2953         2865         5124         5139         5947  ...   \n",
       "2             3685         3585         6473         6775         6678  ...   \n",
       "3              474          469          707          933          776  ...   \n",
       "4             2122         1985         3358         3778         3858  ...   \n",
       "...            ...          ...          ...          ...          ...  ...   \n",
       "33115            0            0            0            0            0  ...   \n",
       "33116           36           60           86           82          111  ...   \n",
       "33117          113          107          224          182          236  ...   \n",
       "33118            0            0            0           32           22  ...   \n",
       "33119          142          100          223          197          381  ...   \n",
       "\n",
       "       $10,000-$14,999  $15,000-$24,999  $25,000-$34,999  $35,000-$49,999  \\\n",
       "0                 48.1               12             12.8              8.6   \n",
       "1                 31.4             16.3             17.9             12.2   \n",
       "2                   31             14.9             17.5             11.7   \n",
       "3                 45.3             10.2               20             11.7   \n",
       "4                 26.9             14.8             23.7             15.2   \n",
       "...                ...              ...              ...              ...   \n",
       "33115                0             46.2             53.8                0   \n",
       "33116              7.1              6.2             21.2             11.1   \n",
       "33117              5.5              1.9             14.1             11.1   \n",
       "33118             28.2                0             23.1             15.4   \n",
       "33119              5.1              6.9             14.2              9.9   \n",
       "\n",
       "       $50,000-$64,999  $65,000-$74,999 $75,000-$99,999 $100,000_or_more  \\\n",
       "0                  8.7              6.2             1.4             16.3   \n",
       "1                 10.6              7.7             2.9             21.2   \n",
       "2                 10.8              8.7             2.4             21.9   \n",
       "3                   11              1.8               0             12.8   \n",
       "4                  9.3              7.5             1.6             18.4   \n",
       "...                ...              ...             ...              ...   \n",
       "33115                0                0               0                0   \n",
       "33116             12.6             17.5             9.5             39.6   \n",
       "33117             16.8             14.5              12             43.3   \n",
       "33118             25.6                0               0             25.6   \n",
       "33119             15.1             17.6            13.6             46.3   \n",
       "\n",
       "      median_household_income mean_household_income  \n",
       "0                       10816                 20349  \n",
       "1                       16079                 23282  \n",
       "2                       16804                 26820  \n",
       "3                       12512                 15730  \n",
       "4                       17475                 23360  \n",
       "...                       ...                   ...  \n",
       "33115                       -                     N  \n",
       "33116                   38594                 52706  \n",
       "33117                   51071                 71580  \n",
       "33118                   19861                 35617  \n",
       "33119                   47941                 62587  \n",
       "\n",
       "[33120 rows x 26 columns]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Econ doesnt need cleaning\n",
    "democlean = demo.filter([\"zipcode\",\"10-14_years\",\"20-24_years\",\"25-34_years\",\"35-44_years\",\"10,000\",\"15,000\",\"25,000\",\"35,000\",\"50,000\",\"65,000\",\"75,000\",\"households\",\"$9,999_or_less\",\"$10,000-$14,999\",\"$15,000-$24,999\",\"$25,000-$34,999\",\"$35,000-$49,999\",\"$50,000-$64,999\",\"$65,000-$74,999\",\"$75,000-$99,999\",\"$100,000_or_more\",\"median_household_income\",\"mean_household_income\"], axis=1)\n",
    "democlean = demo.dropna(how='any',axis=0) \n",
    "democlean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenizing words (7 min)\n",
    "\n",
    "Having split documents into sentences, we now split sentences into individual words. As with sentence tokenization, there is (i) a pretty good heuristic (split on spaces), (ii) a number of weird exceptions (e.g. compound words), and (iii) an existing package that does the job fairly well.\n",
    "\n",
    "We use the `nltk.word_tokenize()` function from `nltk`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(data['text'][1])\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(sentence)\n",
    "    print(words)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: (5 min)\n",
    "\n",
    "Conduct an exploratory analysis of the sizes of reviews: find the shortest and longest reviews, then plot a histogram showing the distribution of review lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_words_lengths = AllReviews.apply(lambda x: len(nltk.word_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min(review_words_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This review has been written in a different language \n",
    "AllReviews[review_words_lengths[review_words_lengths == 2].index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(review_words_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AllReviews[review_words_lengths[review_words_lengths == max(review_words_lengths)].index]\n",
    "print(AllReviews[9349])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the resolution for better clarity \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 20, 6\n",
    "review_words_lengths.hist(bins = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text visualization with word clouds (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like visualization is crucial for standard CSV data, it is also important for text data. But text doesn't lend itself to histograms or scatterplots the way that numerical or even categorical data do. In such cases, **word clouds** are a common and <font color=\"orange\">sometimes</font> useful tool."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the required parameter for plotting \n",
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "word_cloud_text = ''.join(data.text)\n",
    "wordcloud = WordCloud(max_font_size=100, max_words=100, background_color=\"white\",\\\n",
    "                          scale = 10,width=1000, height=800).generate(word_cloud_text)\n",
    "plt.figure(figsize=(20,10))\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** While wordclouds can be a useful way of quickly gaining high level insights into raw textual data, they are also limited. In some ways, they can be seen as the pie charts of NLP: often used, but also often hated. [Some](https://www.niemanlab.org/2011/10/word-clouds-considered-harmful/) [people](https://towardsdatascience.com/can-we-please-stop-using-word-clouds-eca2bbda7b9d) would prefer if they didn't exist at all. If used in the correct way, however, they definitely deserve their place in a data scientist's toolbelt.\n",
    "\n",
    "The main problem with word clouds is that they are difficult to interpret in a standard way. The layout algorithm has some randomness involved and although more common words are shown more prominently, it's not possible to look at a word cloud and know which words are the most important, or how much more important these are than other words. Colours and rotation are also used randomly, making some words (e.g the ones in bright colours, positioned closer to the centre, with horizontal rotation) seem more important when in fact they are no more important than other words which were randomly assigned a less noticeable combination of color, rotation, and position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: (7 min)\n",
    "\n",
    "Write a function `word_cloud_rating(data, star_value)` that constructs a word cloud from the subset of `data` that exhibit a certain `star_value`. Visualize the results of this function for 1-star reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting the resolution for better clarity \n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 30, 60\n",
    "\n",
    "def word_cloud_rating(data,star_value):\n",
    "    \n",
    "    data_filtered = data[data.stars == star_value] #filtering according to the star value\n",
    "    Reviews = data_filtered.text\n",
    "\n",
    "    Reviews_text = ' '.join(Reviews.values) #joining all the words together\n",
    "\n",
    "    # Creating a word cloud object\n",
    "    wordcloud = WordCloud(max_font_size=100, max_words=100, background_color=\"white\",\\\n",
    "                          scale = 10,width=800, height=400).generate(Reviews_text)\n",
    "\n",
    "\n",
    "    # Plotting the generated word cloud\n",
    "    plt.figure()\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud_rating(data, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4: (5 min)\n",
    "\n",
    "The word \"good\" seems to appear quite frequently in the negative reviews. Investigate why that is and come up with a reasonable explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " **Answer.** Let's look at the first 5 reviews or so with 1-star ratings to see if there are any discernible patterns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_containing_good = [each for each in data[data.stars == 1].text if 'good' in each]\n",
    "for review in reviews_containing_good[:20]:\n",
    "    good_index = review.find(\"good\")\n",
    "    print(review[good_index-20:good_index+20].replace(\"\\n\", \"\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading each of the reviews, it is clear that \"good\" is often mentioned in a context like \"no good place to sit\" or \"sound good\". This indicates that in the world of text we cannot go by single words (also called **1-grams**) alone. The context of the sentence or the surrounding words at least are very much necessary to understand the sentiment of a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams (25 min)\n",
    "\n",
    "Since 1-grams are insufficient to understand the significance of certain words in our text, it is natural to consider blocks of words, or **n-grams**.\n",
    "\n",
    "The simplest version of the n-gram model, for $n > 1$, is the **bigram** model, which looks at pairs of consecutive words. For example, the sentence \"The quick brown fox jumps over the lazy dog\" would have tokens \"the quick\", \"quick brown\",..., \"lazy dog\". The following image explains this concept:\n",
    "\n",
    "<img src=\"ngrams.png\" alt=\"ngrams\" width=\"500\"/>\n",
    "\n",
    "This has obvious advantages and disadvantages over looking at words individually:\n",
    "\n",
    "1. This retains the structure of the overall document, and\n",
    "2. It paves the way for analyzing words in contex; however,\n",
    "3. The dimension is vastly larger\n",
    "\n",
    "In practice, this last challenge can be truly daunting. As an example, *War and Peace* has 3 million characters, which translates to several hundred thousand 1-grams (words). If you consider that the set of all possible bigrams can be as large as the square of the number of 1-grams, this gets us to a hundred billion possible bigrams! If classical ML techniques are not suitable for training on 3 million characters, how can they possibly deal with a hundred billion dimensions?\n",
    "\n",
    "For this reason, it is often prudent to start by extracting as much value out of 1-grams as possible, before working our way up to more complex structures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we also start to look again at our main application: calculating some \"interesting\" features of our corpus of reviews.\n",
    "\n",
    "When thinking about word analysis, the main topic of interest is finding an *efficient* and *low-dimensional* representation in order to facilitate document visualization and larger-scale analyses. We discuss two broad categories of word representations:\n",
    "\n",
    "1. `Count-based representations`: word-word and word-document matrices.\n",
    "2. `Word embeddings`: spectral embedding, UMAP, word2vec, GloVe, and many many more.\n",
    "\n",
    "These are often used to assist with downstream tasks such as clustering, ranking and labeling, which will be briefly discussed in a future case. Word embeddings in particular have become something of a posterchild. These, combined with neural networks (which will also be discussed in a future case!), have led to many of the recent headline improvements in NLP tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count-based representations (20 min)\n",
    "\n",
    "n-grams fall under a broader category of techniques otherwise known as [**count-based representations**](https://scikit-learn.org/stable/modules/feature_extraction.html#common-vectorizer-usage). These are techniques to analyze documents by indicating how frequently certain types of structures occur throughout.\n",
    "\n",
    "Let's start with 1-grams (words). The simplest type of information would be whether a particular word occurs in particular documents. This leads to **word-document co-occurrence matrices**, where the $(W, X)$ entry of the word-document matrix is set to 1 if word $W$ occurs in document $X$, and 0 otherwise.\n",
    "\n",
    "There are many variants of this. In lieu of the fact that we are looking for count-based representations of our documents, one natural variable is the following: the $(W, X)$ entry of the word-document matrix equals the number of times that word $W$ occurs in document $X$, rather than merely being a binary variable.\n",
    "\n",
    "Let's create a word-document co-occurrence matrix for our set of reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following code creates a word-document matrix.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vec = CountVectorizer()\n",
    "X = vec.fit_transform(AllReviews)\n",
    "df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5: (5 min)\n",
    "\n",
    "Find all the high-frequency (top 1%) and low-frequency (bottom 1%) words in the reviews overall. (Hint: import the `Counter()` function from the `collections` class.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "all_reviews_text = ' '.join(AllReviews)\n",
    "tokenized_words = nltk.word_tokenize(all_reviews_text)\n",
    "word_freq = Counter(tokenized_words)\n",
    "\n",
    "len(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Therefore, top 1% is ~463 words\n",
    "word_freq.most_common(463)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Similarly, bottom 1% is ~463 words\n",
    "word_freq.most_common()[-463:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same for bigrams. Here is the code to get the set of bigrams for the first 5 reviews:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "first_5_revs = data.text[0:5]\n",
    "word_tokens = nltk.word_tokenize(''.join(first_5_revs))\n",
    "list(ngrams(word_tokens, 2)) #ngrams(word_tokens,n) gives the n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6: (12 min)\n",
    "\n",
    "Write a function called `top_k_ngrams(word_tokens, n, k)` for printing out the top $k$ n-grams. Use this function to get the top 10 1-grams, 2-grams, and 3-grams from the first 1000 reviews in our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams\n",
    "\n",
    "def top_k_ngrams(word_tokens,n,k):\n",
    "    \n",
    "    ## Getting them as n-grams\n",
    "    n_gram_list = list(ngrams(word_tokens, n))\n",
    "\n",
    "    ### Getting each n-gram as a separate string\n",
    "    n_gram_strings = [' '.join(each) for each in n_gram_list]\n",
    "    \n",
    "    n_gram_counter = Counter(n_gram_strings)\n",
    "    most_common_k = n_gram_counter.most_common(k)\n",
    "    print(most_common_k)\n",
    "    \n",
    "   # x_pos = [k for k,v in most_common_k]\n",
    "   # y_pos = [v for k,v in most_common_k]\n",
    "    \n",
    "   # plt.bar(x_pos, y_pos,align='center')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Getting a single string\n",
    "top_1000_reviews = data.text[0:1000]\n",
    "all_reviews_text = ' '.join(top_1000_reviews)\n",
    "\n",
    "## Splitting them into tokens\n",
    "word_tokens = nltk.word_tokenize(all_reviews_text)\n",
    "\n",
    "## Calling the function for top k\n",
    "top_k_ngrams(word_tokens, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_ngrams(word_tokens, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_ngrams(word_tokens, 3, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop words (20 min)\n",
    "\n",
    "You may have noticed a pattern in the types of words that show up in the top 10 1-grams, 2-grams, and 3-grams. In particular, these are common words that appear in every sentence of the English language: pronoums like \"I\", prepositions like \"but\", \"of\", \"and\", articles like \"the\", etc. These very common words are usually uninformative, and their very large occurrence values can distort the results of many NLP algorithms.\n",
    "\n",
    "For this reason, it is common to pre-process text by removing words that you have a reason to believe are uninformative; these words are called [**stop words**](https://en.wikipedia.org/wiki/Stop_words). Usually, it suffices to simply treat extremely common words as stop words. However, for specific types of applications it might make sense to use other stop words; e.g. the word \"burger\" when analyzing reviews of burger chains.\n",
    "\n",
    "(Note that stop words are often removed by default as a cleaning step in all NLP tasks. However, sometimes they can be useful. For example in authorship attribution (automatically detecting who wrote a specific piece of text by their 'writing style'), stop words can be one of the most useful features, as they appear in nearly all texts, and yet each author uses them in slightly different ways.)\n",
    "\n",
    "The `nltk` library has a standard list of stopwords, which you can download by writing `nltk.download(“stopwords”)`. We can then load the stopwords package from the nltk.corpus and use it to load the stop words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words(\"japanese\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "print(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can get a list of all the Spanish stop words as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stopwords.words(\"spanish\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7: (15 min)\n",
    "\n",
    "#### 7.1\n",
    "\n",
    "Filter out all of the stop words in the first review of the Yelp review data and print out your answer. Additionally, print out (separately) the stopwords you found in this review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))\n",
    "without_stop_words = []\n",
    "stopword = []\n",
    "sentence = data.text[0]\n",
    "words = nltk.word_tokenize(sentence)\n",
    "for word in words:\n",
    "    if word in stop_words:\n",
    "        stopword.append(word)\n",
    "    else:\n",
    "        without_stop_words.append(word)\n",
    "\n",
    "print(stopword)\n",
    "print()\n",
    "print(without_stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2\n",
    "\n",
    "Modify the function `top_k_ngrams(word_tokens, n, k)` to remove stop words before determining the top n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** Our recommended solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing the most basic stop words from the ntlk corpus and including only those\n",
    "# words with character size above 2 so as to remove punctuations\n",
    "# But, this could be extended to remove further high and low frequency stop words\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "eng_stopwords = stopwords.words('english')\n",
    "\n",
    "### Getting a single string\n",
    "all_reviews_text = ' '.join(top_1000_reviews)\n",
    "\n",
    "## Splitting them into tokens\n",
    "word_tokens = nltk.word_tokenize(all_reviews_text)\n",
    "\n",
    "## Removing the stopwords\n",
    "word_tokens_clean = [each for each in word_tokens if each.lower() not in eng_stopwords and len(each.lower()) > 2]\n",
    "\n",
    "## Calling the function for top k\n",
    "top_k_ngrams(word_tokens_clean, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_ngrams(word_tokens_clean, 2, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some contexts, it is common to remove both very common and very *uncommon* words. The idea is that common words like \"a\" are almost never informative, while uncommon words like \"syzygy\" occur so infrequently in a corpus that many algorithms have a hard time processing them in a meaningful way. We will not deal with uncommon words today, but you should be aware that doing so improves the performance of several NLP techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding important words (30 min)\n",
    "\n",
    "Up to this point, we have focused on techniques for transforming our data. We are now ready to start looking for some answers, so let's take a break from discussing techniques so we can explore our dataset and various ways to summarize it.\n",
    "\n",
    "We begin by looking at the words and n-grams that are most common in positive and negative reviews. Note that in the following code, we don't reuse many of the pre-processing steps discussed at the start of the tutorial. This is because many of them are included as options in existing packages. In a serious project one would often customize this pre-processing to some degree, but we skip this in order to get some displayable results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Following code grabbed from:\n",
    "# https://towardsdatascience.com/a-complete-exploratory-data-analysis-and-visualization-for-text-data-29fb1b96fb6a\n",
    "# we will use it in our context to create some visualizations.\n",
    "def get_top_n_words(corpus, n=1,k=1):\n",
    "    vec = CountVectorizer(ngram_range=(k,k),stop_words = 'english').fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by getting a list of the most common words.\n",
    "\n",
    "common_words = get_top_n_words(data['text'], 20,1)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar',  title='Top 20 words from all reviews')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8: (15 min)\n",
    "\n",
    "#### 8.1\n",
    "\n",
    "Divide the data into \"good reviews\" (i.e. `stars` rating was greater than 3) and \"bad reviews\" (i.e. `stars` rating was less than 3) and make a bar plot of the top 20 words in each case. Are these results different from above?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We continue by splitting according to good/bad review scores, then grabbing again.\n",
    "\n",
    "GoodInd = data['stars'] >3.1\n",
    "GoodRev = data[GoodInd]\n",
    "BadInd = data['stars'] <2.1\n",
    "BadRev = data[BadInd]\n",
    "\n",
    "common_words = get_top_n_words(GoodRev['text'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 words from good reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(BadRev['text'], 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 words from bad reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was pretty useless. The \"good\" words are mostly a mix of generic words like \"place\" and overtly positive words like \"good\" itself.\n",
    "\n",
    "The problem here is that we are dealing with single words, which cannot convey much information out of context. The natural solution then is to deal with n-grams, so that we can get context-aware results like \"good burger\" or \"good service\" (in the positive reviews) or, as we saw, \"good 45 minutes\" (in the negative reviews)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.2\n",
    "\n",
    "Use the `get_top_n_words()` function to find the top 20 bigrams and trigrams. Do the results seem useful?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top bigrams and trigrams from bad reviews\n",
    "common_words = get_top_n_words(BadRev['text'], 20,2)\n",
    "\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 bigrams from bad reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get top bigrams and trigrams from bad reviews\n",
    "common_words = get_top_n_words(GoodRev['text'], 20,2)\n",
    "\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 bigrams from bad reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(BadRev['text'], 20,3)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 trigrams from bad reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(GoodRev['text'], 20,3)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 bigrams from good reviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_words = get_top_n_words(GoodRev['text'], 20,3)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "df = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\n",
    "df.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\n",
    "    kind='bar', title='Top 20 trigrams from good reviews')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although there are some nonsense entries such as \"http www yelp\" this is starting to be helpful. We can see a few recurring themes among good reviews (e.g. \"staff friendly helpful\"). Nonsense entries are typically difficult to eliminate completely in NLP with user-generated text and smaller corpora. NLP is still useful *despite* the existence of nonsense results, and we should think of the output of NLP algorithms like this as a *screening tool* for finding important phrases rather than a *careful estimate* of the most important phrases. In other words, it's an application of machine intelligence to *conduct exploratory analysis* rather than to *build predictive models*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question:\n",
    "\n",
    "Look at the 5 most important bigrams for bad reviews. What *single, specific* problem seems to be the most important driver of bad reviews?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three of the top 5 bigrams were \"20 minutes\", \"15 minutes\", and \"10 minutes.\" These are all times, *strongly* suggesting that *waiting time for service* is a main driver for bad review scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9: (10 min)\n",
    "\n",
    "#### 9.1\n",
    "\n",
    "You may have noticed that many of the important \"bad\" bigrams included the words \"like\" or \"just\" but didn't seem very informative (e.g. \"felt like\", \"food just\"). Give some ideas of how to use this sort of observation in future pre-processing of reviews, based on the pre-processing ideas we have already studied."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** Two potential answers are (there are many others):\n",
    "\n",
    "1. Having recognized that these words go together in common bigrams, you could modify your algorithm so that it \"clumps\" these bigrams together; i.e. treats them as one word, so that your algorithm will focus on the words following that.\n",
    "2. Having recognized this as a key phrase, we could have a list of the most important words that follow these key phrases (which are presumably informative). This is more time-consuming as it requires human input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9.2\n",
    "\n",
    "Building on the previous question, we note that most of the most important complaints and compliments can't be *completely* observed by looking at bigrams or trigrams. This can often be fixed by small modifications. Do the following:\n",
    "\n",
    "1. Write down a complaint that is unlikely to be (completely) picked up by bigram analysis. Hint: what might you write if your hamburger was served cold?\n",
    "2. Write down a processing step that would fix this problem. Try to find a solution that would work for several similar problems without additional human input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** There are many good answers, but we focus on a simple one:\n",
    "\n",
    "1. I would probably write \"the burger was cold\" or \"the burger was served cold.\" If this were a common complaint, the most-important bigrams might include \"was cold\" or \"served cold,\" which doesn't tell me *what* was served cold.\n",
    "2. A simple fix, along the lines of the previous question, would be to \"clump\" words like \"was cold\" together. However, I think this is a bad fix, as it focuses too much on the word \"cold\" and would require a great deal of hand tuning. A better idea would be to recognize that words like \"was\" are always going to be a problem in this context, as they separate the important noun describing the subject (\"burger\") from the adjective describing the problem (\"cold\"). This suggests that we should take a *much* more aggressive stance towards removing stop words. This should certainly include conjugations of \"to be,\" and likely many other common but uninformative words (like \"too\").\n",
    "\n",
    "This second response is an important takeaway for NLP – this sort of problem is extremely common, and a great deal of time is often spent tweaking initial pre-processing rules. In the final part of this case, you will learn about a method that can help systematically deal with these uninformative stop words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular Expressions (25 min)\n",
    "\n",
    "Having spent a lot of time on n-grams and how to featurize a document using them, we now take a break from `nltk` tools to introduce the most important text wrangling tool in Python (and many other languages): [**regular expressions**](https://en.wikipedia.org/wiki/Regular_expression).\n",
    "\n",
    "The basic idea here is that you often want to perform some specific transformation (e.g. delete or substitute) every time that some possibly-complicated pattern (e.g. the letter 'A', the word 'hello', any word containing the letters 'a','r' in that order) occurs. Regular expressions are a compact and powerful language for expressing these sorts of patterns. This is super important whenever you are trying to clean a text dataset that contains thematically similar, but not exactly, the same errors. \n",
    "\n",
    "The terse syntax of regular expressions has led to them having a reputation for being [almost magical](https://xkcd.com/208/) in some situations (with only a few characters, you can build complete computer programs) but also for being difficult to create and read, which can [create more problems](https://xkcd.com/1171/) than they solve.\n",
    "\n",
    "In Python, [the `re` module](https://docs.python.org/3/library/re.html) provides regular expression matching operations and common operations. Regular expressions are a deep subject, with some documentation here: https://docs.python.org/3/library/re.html?highlight=regex.\n",
    "\n",
    "As some simple examples, we have:\n",
    "\n",
    "1. `.` matches any character except \\n (newline)\n",
    "2. `\\d` matches any digit (this can also be written as [0-9])\n",
    "3. `\\D` matches any non-digit (this can also be written as [^0-9])\n",
    "4. `\\w` matches any alphanumeric character ([a-zA-Z0-9_])\n",
    "5. `\\W` matches any non-alphanumeric character ([^a-zA-Z0-9_])\n",
    "\n",
    "As some more complex examples, regular expressions also allow you to quantify the number of times matches can occur. For example,\n",
    "\n",
    "1. `[a-d]+` matches any time you get $\\{a,b,c,d\\}$ one or more times in a row\n",
    "2. `[a-d]{3}` matches any time you get them exactly 3 times in a row\n",
    "3. `[a-d]*` matches any time you get them 0 or more times in a row\n",
    "\n",
    "For now, we give a simple application based on the  `re.sub()` function, which substitutes words that match a pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "sentence = 'That was an \"interesting\" way to cook bread.'\n",
    "pattern = r\"[^\\w]\" # the ^ character denotes 'not', \n",
    "#                   the \\w character denotes a word, and []  means\n",
    "#                    anything that matches anything in the brackets. \n",
    "#                     Together, this refers to any character that is not a word.\n",
    "print(re.sub(pattern, \" \", sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = \"Natesh loves all the foold and loveds sdaslo\"\n",
    "x   = re.compile('lo')\n",
    "iterator = x.finditer(str)\n",
    "for item in iterator:\n",
    "    print(item.span())\n",
    "    print(item.group())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10: (15 min)\n",
    "\n",
    "#### 10.1\n",
    "\n",
    "1. Use the `re.split()` function to split the first Yelp review into a list of its constituent words.\n",
    "2. Use the `re.findall()` function to search the first 30 reviews for the number of times they contain the word \"food\". Print the maximum number of times the word \"food\" is mentioned in a single review."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re.split(r'\\s', AllReviews.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "food_count = []\n",
    "for sentence in AllReviews.values:\n",
    "    temp = len(re.findall('food', sentence))\n",
    "    food_count.append(temp)\n",
    "print(max(food_count[0:30]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.2\n",
    "\n",
    "Using regular expressions, find the percentage of reviews in top 500 reviews that have numbers in them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Considering the top 500 reviews for this analysis\n",
    "top_500_reviews = AllReviews.values[:500]\n",
    "reviews_nos_regex = []\n",
    "\n",
    "for each_review in top_500_reviews:\n",
    "    number_list = re.findall('\\d',each_review)\n",
    "    \n",
    "    ## number list returns all the possible digits in a review\n",
    "    ## Look if the number list is empty - if so, the review has no digits in them\n",
    "    if(len(number_list)) > 0:\n",
    "        reviews_nos_regex.append(each_review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(reviews_nos_regex)/len(top_500_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is clear from above, regular expressions are very useful for extracting more general properties of text. These properties are not as informative or context-aware as n-grams can be, but they are much simpler to code and therefore can often serve as the first step of an EDA on text data.\n",
    "\n",
    "Although regular expressions usually cannot tell us much about context overall, they *can* be used to find specific instances of words in context. For example, we may be interested in finding the first word following \"good\" or \"bad\" in a review (which can help us distinguish a positive from a negative review). Let's write some code that finds the first word following \"good\" in the sentence \"hello I want a good burger, please.\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = \"hello I want a good burger, please\"\n",
    "\n",
    "# Find everything after \"good\", including \"good\"\n",
    "\n",
    "post = re.findall(r'good.*', sample)[0]\n",
    "\n",
    "print(post)\n",
    "\n",
    "# Take just the first word after \"good\"\n",
    "\n",
    "first_post = re.split(r'\\s',post)[1]\n",
    "\n",
    "print(first_post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.3\n",
    "\n",
    "Using the above as a template, write a generalized function that can extract the first word following \"good\". Don't forget to include a default behavior for when the word doesn't appear in the sentence. Run this function for all reviews and print the first 300 results for reviews that do contain the word \"good\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word(sentence):\n",
    "    post = re.findall(r'good.*', sentence)\n",
    "    if (len(post) > 0):\n",
    "        temp = re.split(r'\\s',post[0])\n",
    "        if (len(temp) > 1):\n",
    "            return(temp[1])\n",
    "        else:\n",
    "            return('')\n",
    "    else:\n",
    "        return('')\n",
    "    \n",
    "    \n",
    "print(next_word(sample))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_good = []\n",
    "ind = 0\n",
    "for sentence in AllReviews.values:\n",
    "    temp = next_word(sentence)\n",
    "    post_good.append(temp)\n",
    "    \n",
    "nonempty = [i for i in post_good if i] \n",
    "print(nonempty[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skimming over the results of the previous exercise, a few things stood out:\n",
    "\n",
    "1. People like to talk about good burgers – this appeared 5 times in the first 300 results.\n",
    "2. A large number of the results are useless. One common problem is the occurrence of a sentence boundary; e.g. \"good. The\" near the start of the list. In this case, we should look *before* the word \"good\" rather than after. However, we can't look *immediately* before good – that word will usually be some conjugation of \"to be\", which is also not informative – rather, we need to look for a word before \"good\" that isn't too boring. Other times, there is a word following \"good\" that is uninformative; e.g. \"good for\" – we want to know *what* something was good for! In this case, we should keep on skimming *forward* until we see a word following \"good\" that isn't too boring.\n",
    "\n",
    "In both of these cases, we can't use simple regular expressions by themselves, as regular expressions don't know how to ignore \"boring\" words. Regular expressions can only help us filter for the structure of words, not the content they convey within a context. We *can* use what we learned about stop words to remove these from the reviews before conducting the above analysis, but as we have been, we will still sometimes get not very informative phrases like \"was cold\" or \"served cold\". So we will introduce an alternative method, which can be applied to serve as an even better remover of stop words: **part-of-speech tagging**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Part-of-speech (POS) tagging (45 min)\n",
    "\n",
    "In English, there are eight main parts of speech - `nouns`, `pronouns`, `adjectives`, `verbs`, `adverbs`, `prepositions`, `conjunctions` and `interjections`. These are\n",
    "`sustantivos`, `pronombres`, `adjetivos`, `verbos`, `adverbios`, `preposiciones`, `conjunciones` and `interjecciones`, respectively, in Spanish. The purpose of POS tagging is to label each word in a document with its part of speech.\n",
    "\n",
    "Unsurprisingly, [POS tagging](http://www.nltk.org/book/ch05.html) can be very difficult to do by hand. `nltk` has a default function for this, called `nltk.pos_tag()`, which we will use. As a word of warning, this function is far from infallible, especially on informal text (e.g. website reviews, forum posts, text messages, etc), and words in English often exhibit POS drift (e.g. the drift of \"Google\" from noun to verb): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "#https://www.nltk.org/book/ch05.html\n",
    "text_word_token = nltk.word_tokenize(\"Jairo is having a good day\")\n",
    "#text_word_token = nltk.word_tokenize(data.text[0])\n",
    "nltk.pos_tag(text_word_token)\n",
    "#https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I prefer to buy burgers\n",
    "prefer -> burger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_word_token = nltk.word_tokenize(\"We are going to Race\") # try \"Race can be both a verb and a noun\"\n",
    "#text_word_token = nltk.word_tokenize(data.text[0])\n",
    "nltk.pos_tag(text_word_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.nltk.org/_modules/nltk/tag/perceptron.html\n",
    "nltk.pos_tag(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLTK provides documentation for each tag, which can be queried using the tag itself; e.g. `nltk.help.upenn_tagset('RB')`. Since POS is context-sensitive, POS-taggers must usually be trained on an existing corpus that has been tagged by professional linguists (possibly alongside unlabeled data to take advantage of semi-supervised methods). The most popular tag set is called the Penn Treebank set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can get more details about any POS tag using the help function of nltk\n",
    "nltk.download('tagsets')\n",
    "nltk.help.upenn_tagset('CD$')\n",
    "nltk.help.upenn_tagset('NN$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 11: (10 min)\n",
    "\n",
    "#### 11.1\n",
    "\n",
    "Write code to find the percentage of reviews in the first 500 reviews of the dataset that contains a number or a cardinal using POS taggings only. (Hint: POS tag `CD` is the indicator for cardinal or number.) How does this compare to the figure we extracted from using regular expressions only?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cardinal_review = []\n",
    "\n",
    "for sentence in top_500_reviews:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    cd_len = [k for k,v in nltk.pos_tag(words) if 'CD' == v]\n",
    "    \n",
    "    if len(cd_len) > 0:\n",
    "        cardinal_review.append(sentence)\n",
    "\n",
    "#### Proportion of reviews with a number/cardinal in it        \n",
    "len(cardinal_review)/len(top_500_reviews) \n",
    "## 56.6% of the reviews have a number/cardinal in the top 500 reviews. \n",
    "## You could improve the accuracy of this estimate by looking at more than 500 reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This number is considerably higher than the one we got from using regular expressions only! The reason is because POS tagging can extract numbers in text form (e.g. \"one\", \"two\") whereas regular expressions cannot. This is one advantage of using POS tagging over regular expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 11.2\n",
    "\n",
    "Extract all of the nouns from each review using POS tagging. This may be useful for later analysis. Even though words like \"good\" may be the most prevalent in good reviews, we think nouns like \"service\" or \"burgers\" are likely to be more informative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is given below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "noun_reviews = []\n",
    "for sentence in AllReviews.values:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    noun_pt = [k for k,v in nltk.pos_tag(words) if 'NN' == v]\n",
    "    noun_reviews.append(noun_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12: (15 min)\n",
    "\n",
    "Use POS tagging to find the first word following \"good\" that has an interesting POS tag. We leave this up to your discretion, but should probably include nouns and proper nouns. Inspecting the above, we think that cardinals are also almost certainly interesting: we recognize that \"good 45\" is probably followed by \"minutes\", definitely an important (though not \"good\") part of a review!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, a function that extracts only the \"interesting\" parts of speech\n",
    "\n",
    "sentence = \"This is a good burger, but I prefer pizza\"\n",
    "words = nltk.word_tokenize(sentence)\n",
    "interesting = [k for k,v in nltk.pos_tag(words) if v in ['CD','FW','NN','NNS','NNP','NNPS']]\n",
    "\n",
    "print(interesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_pos = ['CD','FW','NN','NNS','NNP','NNPS']\n",
    "\n",
    "def ExtractInteresting(sentence, good):\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    interesting = [k for k,v in nltk.pos_tag(words) if v in good]\n",
    "    return(interesting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, a function that extracts the first \"interesting\" word that follows \"good\"\n",
    "\n",
    "sentence = \"This is a good burger, but I prefer pizza\"\n",
    "post = re.findall(r'good.*', sentence)\n",
    "print(post)\n",
    "# Check that post isn't empty here before doing next line\n",
    "temp = ExtractInteresting(post[0],good_pos)\n",
    "print(temp)\n",
    "# Check that temp isn't empty here before doing next line\n",
    "print(post[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_word2(sentence):\n",
    "    post = re.findall(r'good.*', sentence)\n",
    "    if (len(post) > 0):\n",
    "        temp = ExtractInteresting(post[0],good_pos)\n",
    "        if (len(temp) > 0):\n",
    "            return(temp[0])\n",
    "        else:\n",
    "            return('')\n",
    "    else:\n",
    "        return('')\n",
    "    \n",
    "    \n",
    "print(next_word2(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, apply this.\n",
    "\n",
    "post_good = []\n",
    "ind = 0\n",
    "for sentence in AllReviews.values:\n",
    "    temp = next_word2(sentence)\n",
    "    post_good.append(temp)\n",
    "    \n",
    "nonempty = [i for i in post_good if i] \n",
    "print(nonempty[0:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems like a much more interesting list!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13: (10 min)\n",
    "\n",
    "It is interesting to look specifically at Adjectives (which have a tag name of \"JJ\" in NLTK) when looking at reviews. We can hypothesise that good reviews and bad reviews might use very different adjectives, but that some adjectives might appear often in both good and bad reviews, as we saw with the word \"good\" previously.\n",
    "\n",
    "Use POS tags to extract all adjectives from the first 500 five star reviews and the first 500 one star reviews. Extract the most 30 most commonly used adjectives from each set of reviews and print out both. Make a note of several of these; say if they appear in one or both lists, and whether or not this was expected, and why."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Answer.** One possible solution shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_star_reviews = data[data['stars']==1]['text'][:500]\n",
    "five_star_reviews = data[data['stars']==5]['text'][:500]\n",
    "\n",
    "\n",
    "def extract_specific_pos(reviews, pos_tag):\n",
    "    results = [] \n",
    "    for review in reviews:\n",
    "        words = nltk.word_tokenize(review)\n",
    "        tagged = nltk.pos_tag(words)\n",
    "        keep = [x[0] for x in tagged if x[1] == pos_tag]\n",
    "        results += keep\n",
    "    return results\n",
    "\n",
    "\n",
    "negative_adjectives = extract_specific_pos(one_star_reviews, \"JJ\")\n",
    "positive_adjectives = extract_specific_pos(five_star_reviews, \"JJ\")\n",
    "\n",
    "print(Counter(negative_adjectives).most_common(30))\n",
    "print(Counter(positive_adjectives).most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw before, the word \"good\" appears in both lists, though slightly more often in good reviews. Surprisingly, it is the most common adjective used in *bad* reviews.\n",
    "\n",
    "Words like \"disappointed\", \"awful\", \"horrible\", and \"rude\" only appear frequently in bad reviews, as expected, while words like \"favorite\", \"great\", \"fantastic\", \"amazing\", \"wonderful\", and \"perfect\" appear in good reviews, which is also expected.\n",
    "\n",
    "Words like \"small\" and \"new\" appear frequently in in both sets of reviews. Interestingly, so does \"different\", but slightly more often in good reviews, perhaps indicating that people like variety."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions (5 min)\n",
    "\n",
    "In this case, we focused on the basic components of an NLP pipeline, virtually all of which are frequently used *before* building a model for the business question of interest. We saw that every part of the pipeline was highly customizable, and discussed how parameters might vary depending on the specific application in mind. \n",
    "\n",
    "In addition to constructing a basic pipeline, we tried to give initial answers to a business question: \"Which factors are most important for bad reviews?\" The answers we obtained with this out-of-the-box analysis were not perfect, but they did seem to give some genuinely useful information. For example, 3 of the 5 most important phrases for bad reviews were \"20 minutes\", \"10 minutes\", and \"15 minutes\" – strong evidence that long service times were a major driver of bad reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways (7 min)\n",
    "\n",
    "Text pre-processing is more complex than other forms of pre-processing you might be familiar with, as good pre-processing may rely on an enormous number of rules extracted from large corpora of English (Spanish!) text. You shouldn't try to recreate this work by yourself; instead, take advantage of large and powerful libraries such as `nltk` which have built-in corpora when possible, and use regular expressions when necessary to extend or tweak them.\n",
    "\n",
    "Pre-processing is an extremely important and nontrivial part of NLP, and will likely take the bulk of the work for most NLP projects. Most popular parts of the pipeline come with many parameters. Yet they can give surprisingly useful summaries of entire corpora without much adjustment.\n",
    "\n",
    "Overall, NLP can be used in many situations, but it is perhaps most useful in its ability to turn qualitative data into quantitative data. For example, if we have a collection of reviews describing, qualitatively, people’s experiences at restaurants, we can derive quantitative insights such as “X% of people who left bad reviews were unhappy with the waiting time”.\n",
    "\n",
    "After following through this case, you know what NLP is and how it can be useful. You especially know\n",
    "\n",
    "* The challenges of NLP: context specificity and high dimensionality.\n",
    "* How to standardize and pre-process text before carrying out analysis, such as stemming \n",
    "* How to tokenize documents into sentences and words\n",
    "* How to create word clouds to quickly gain high-level insights into text\n",
    "* What n-grams are and how they can be created and used in analysis\n",
    "* Why common (“stop”) words should often be removed before analysis\n",
    "* How to find common words and n-grams\n",
    "* What regular expressions are and how to use them to carry out more custom analysis\n",
    "* What Parts of Speech tagging is and why analysing documents by their grammatical structure can be useful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk, wordcloud, spacy, nagisa"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
